<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;jiangzhiqi4551.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Pisces&quot;,&quot;version&quot;:&quot;8.5.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:false,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;搜索...&quot;,&quot;empty&quot;:&quot;没有找到任何搜索结果：${query}&quot;,&quot;hits_time&quot;:&quot;找到 ${hits} 个搜索结果（用时 ${time} 毫秒）&quot;,&quot;hits&quot;:&quot;找到 ${hits} 个搜索结果&quot;}}</script><script src="/js/config.js"></script>
<meta property="og:type" content="website">
<meta property="og:title" content="Jiang&#39;s blog">
<meta property="og:url" content="https://jiangzhiqi4551.github.io/page/2/index.html">
<meta property="og:site_name" content="Jiang&#39;s blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Master Jiang">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://jiangzhiqi4551.github.io/page/2/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:true,&quot;isPost&quot;:false,&quot;lang&quot;:&quot;zh-CN&quot;,&quot;comments&quot;:&quot;&quot;,&quot;permalink&quot;:&quot;&quot;,&quot;path&quot;:&quot;page&#x2F;2&#x2F;index.html&quot;,&quot;title&quot;:&quot;&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>Jiang's blog</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Jiang's blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Master Jiang"
      src="/images/logo1.jpeg">
  <p class="site-author-name" itemprop="name">Master Jiang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">24</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Jiangzhiqi4551" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Jiangzhiqi4551" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:Jiangzhiqi4551@outlook.com" title="E-Mail → mailto:Jiangzhiqi4551@outlook.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/Jiangzhiqi4551?spm=1000.2115.3001.5343" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;Jiangzhiqi4551?spm&#x3D;1000.2115.3001.5343" rel="noopener" target="_blank"><i class="fas fa-blog fa-fw"></i>CSDN</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiangzhiqi4551.github.io/2021/06/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6%E4%B8%AD%E7%9A%84%E5%B0%8F%E6%96%87%E4%BB%B6%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo1.jpeg">
      <meta itemprop="name" content="Master Jiang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiang's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6%E4%B8%AD%E7%9A%84%E5%B0%8F%E6%96%87%E4%BB%B6%E9%97%AE%E9%A2%98/" class="post-title-link" itemprop="url">大数据框架中的小文件问题</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-08 18:34:38" itemprop="dateCreated datePublished" datetime="2021-06-08T18:34:38+08:00">2021-06-08</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-10-10 18:16:20" itemprop="dateModified" datetime="2021-10-10T18:16:20+08:00">2021-10-10</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/" itemprop="url" rel="index"><span itemprop="name">常见问题</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <span id="more"></span>

<p><a target="_blank" rel="noopener" href="http://xcx1024.com/ArtInfo/997661.html">原文链接</a></p>
<h1 id="Hadoop里面的小文件问题"><a href="#Hadoop里面的小文件问题" class="headerlink" title="Hadoop里面的小文件问题"></a>Hadoop里面的小文件问题</h1><p>小文件指的是那些size比HDFS的block size(默认64M)小的多的文件。如果在HDFS中存储小文件，那么在HDFS中肯定会含有许许多多这样的小文件(不然就不会用hadoop了)。而HDFS的问题在于无法很有效的处理大量小文件。</p>
<p>任何一个文件，目录和block，在HDFS中都会被表示为一个object存储在namenode的内存中，没一个object占用150 bytes的内存空间。所以，如果有10million个文件，没一个文件对应一个block，那么就将要消耗namenode 3G的内存来保存这些block的信息。如果规模再大一些，那么将会超出现阶段计算机硬件所能满足的极限。</p>
<p>不仅如此，HDFS并不是为了有效的处理大量小文件而存在的。它主要是为了流式的访问大文件而设计的。对小文件的读取通常会造成大量从datanode到datanode的seeks和hopping来retrieve文件，而这样是非常的低效的一种访问方式。</p>
<h3 id="大量小文件在mapreduce中的问题"><a href="#大量小文件在mapreduce中的问题" class="headerlink" title="大量小文件在mapreduce中的问题"></a>大量小文件在mapreduce中的问题</h3><p>Map tasks通常是每次处理一个block的input(默认使用FileInputFormat)。如果文件非常的小，并且拥有大量的这种小文件，那么每一个map task都仅仅处理了非常小的input数据，并且会产生大量的map tasks，每一个map task都会消耗一定量的bookkeeping的资源。比较一个1GB的文件，默认block size为64M，和1Gb的文件，没一个文件100KB，那么后者没一个小文件使用一个map task，那么job的时间将会十倍甚至百倍慢于前者。</p>
<p>hadoop中有一些特性可以用来减轻这种问题：可以在一个JVM中允许task reuse，以支持在一个JVM中运行多个map task，以此来减少一些JVM的启动消耗(通过设置mapred.job.reuse.jvm.num.tasks属性，默认为1，－1为无限制)。另一种方法为使用MultiFileInputSplit，它可以使得一个map中能够处理多个split。</p>
<h5 id="为什么会产生大量的小文件？"><a href="#为什么会产生大量的小文件？" class="headerlink" title="为什么会产生大量的小文件？"></a>为什么会产生大量的小文件？</h5><p>至少有两种情况下会产生大量的小文件</p>
<p>1.这些小文件都是一个大的逻辑文件的pieces。由于HDFS仅仅在不久前才刚刚支持对文件的append，因此以前用来向unbounde files(例如log文件)添加内容的方式都是通过将这些数据用许多chunks的方式写入HDFS中。</p>
<p>2.文件本身就是很小。例如许许多多的小图片文件。每一个图片都是一个独立的文件。并且没有一种很有效的方法来将这些文件合并为一个大的文件<br>这两种情况需要有不同的解决方式。对于第一种情况，文件是由许许多多的records组成的，那么可以通过件邪行的调用HDFS的sync()方法(和append方法结合使用)来解决。或者，可以通过些一个程序来专门合并这些小文件(see Nathan Marz’s post about a tool called the Consolidator which does exactly this)。<br>对于第二种情况，就需要某种形式的容器来通过某种方式来group这些file。hadoop提供了一些选择：</p>
<h5 id="HAR-files"><a href="#HAR-files" class="headerlink" title="HAR files"></a>HAR files</h5><p>Hadoop Archives (HAR files)是在0.18.0版本中引入的，它的出现就是为了缓解大量小文件消耗namenode内存的问题。HAR文件是通过在HDFS上构建一个层次化的文件系统来工作。一个HAR文件是通过hadoop的archive命令来创建，而这个命令实 际上也是运行了一个MapReduce任务来将小文件打包成HAR。对于client端来说，使用HAR文件没有任何影响。所有的原始文件都 visible &amp;&amp; accessible（using har://URL）。但在HDFS端它内部的文件数减少了。</p>
<p>通过HAR来读取一个文件并不会比直接从HDFS中读取文件高效，而且实际上可能还会稍微低效一点，因为对每一个HAR文件的访问都需要完成两层index文件的读取和文件本身数据的读取(见上图)。并且尽管HAR文件可以被用来作为MapReduce job的input，但是并没有特殊的方法来使maps将HAR文件中打包的文件当作一个HDFS文件处理。可以考虑通过创建一种input format，利用HAR文件的优势来提高MapReduce的效率，但是目前还没有人作这种input format。需要注意的是：MultiFileInputSplit，即使在HADOOP-4565的改进(choose files in a split that are node local)，但始终还是需要seek per small file。</p>
<h5 id="Sequence-Files"><a href="#Sequence-Files" class="headerlink" title="Sequence Files"></a>Sequence Files</h5><p>通常对于“the small files problem”的回应会是：使用SequenceFile。这种方法是说，使用filename作为key，并且file contents作为value。实践中这种方式非常管用。回到10000个100KB的文件，可以写一个程序来将这些小文件写入到一个单独的SequenceFile中去，然后就可以在一个streaming fashion(directly or using mapreduce)中来使用这个sequenceFile。不仅如此，SequenceFiles也是splittable的，所以mapreduce可以break them into chunks，并且分别的被独立的处理。和HAR不同的是，这种方式还支持压缩。block的压缩在许多情况下都是最好的选择，因为它将多个records压缩到一起，而不是一个record一个压缩。</p>
<p>将已有的许多小文件转换成一个SequenceFiles可能会比较慢。但是，完全有可能通过并行的方式来创建一个一系列的SequenceFiles。(Stuart Sierra has written a very useful post about converting a tar file into a SequenceFile—tools like this are very useful)。更进一步，如果有可能最好设计自己的数据pipeline来将数据直接写入一个SequenceFile。</p>
<h1 id="hive中的小文件问题"><a href="#hive中的小文件问题" class="headerlink" title="hive中的小文件问题"></a>hive中的小文件问题</h1><h3 id="小文件是如何产生的"><a href="#小文件是如何产生的" class="headerlink" title="小文件是如何产生的"></a>小文件是如何产生的</h3><p>1.动态分区插入数据，产生大量的小文件，从而导致map数量剧增。<br>2.reduce数量越多，小文件也越多(reduce的个数和输出文件是对应的)。<br>3.数据源本身就包含大量的小文件。</p>
<h3 id="小文件问题的影响"><a href="#小文件问题的影响" class="headerlink" title="小文件问题的影响"></a>小文件问题的影响</h3><p>1.从Hive的角度看，小文件会开很多map，一个map开一个JVM去执行，所以这些任务的初始化，启动，执行会浪费大量的资源，严重影响性能。<br>2.在HDFS中，每个小文件对象约占150byte，如果小文件过多会占用大量内存。这样NameNode内存容量严重制约了集群的扩展。</p>
<h3 id="小文件问题的解决方案"><a href="#小文件问题的解决方案" class="headerlink" title="小文件问题的解决方案"></a>小文件问题的解决方案</h3><h5 id="从小文件产生的途经就可以从源头上控制小文件数量，方法如下："><a href="#从小文件产生的途经就可以从源头上控制小文件数量，方法如下：" class="headerlink" title="从小文件产生的途经就可以从源头上控制小文件数量，方法如下："></a>从小文件产生的途经就可以从源头上控制小文件数量，方法如下：</h5><p>1.使用Sequencefile作为表存储格式，不要用textfile，在一定程度上可以减少小文件。<br>2.减少reduce的数量(可以使用参数进行控制)。<br>3.少用动态分区，用时记得按distribute by分区。</p>
<h5 id="对于已有的小文件，我们可以通过以下几种方案解决："><a href="#对于已有的小文件，我们可以通过以下几种方案解决：" class="headerlink" title="对于已有的小文件，我们可以通过以下几种方案解决："></a>对于已有的小文件，我们可以通过以下几种方案解决：</h5><p>1.使用hadoop archive命令把小文件进行归档。<br>2.重建表，建表时减少reduce数量。<br>3.通过参数进行调节，设置map/reduce端的相关参数，如下：</p>
<h6 id="设置map输入合并小文件的相关参数："><a href="#设置map输入合并小文件的相关参数：" class="headerlink" title="设置map输入合并小文件的相关参数："></a>设置map输入合并小文件的相关参数：</h6><p>//每个Map最大输入大小(这个值决定了合并后文件的数量)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapred.max.split.size=256000000;</span><br></pre></td></tr></table></figure>
<p>//一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapred.min.split.size.per.node=100000000;</span><br></pre></td></tr></table></figure>
<p>//一个交换机下split的至少的大小(这个值决定了多个交换机上的文件是否需要合并)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapred.min.split.size.per.rack=100000000;</span><br></pre></td></tr></table></figure>
<p>//执行Map前进行小文件合并</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure>



<h6 id="设置map输出和reduce输出进行合并的相关参数："><a href="#设置map输出和reduce输出进行合并的相关参数：" class="headerlink" title="设置map输出和reduce输出进行合并的相关参数："></a>设置map输出和reduce输出进行合并的相关参数：</h6><p>//设置map端输出进行合并，默认为true</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.merge.mapfiles = true</span><br></pre></td></tr></table></figure>
<p>//设置reduce端输出进行合并，默认为false</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.merge.mapredfiles = true</span><br></pre></td></tr></table></figure>
<p>//设置合并文件的大小</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.merge.size.per.task = 25610001000</span><br></pre></td></tr></table></figure>
<p>//当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件merge。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.merge.smallfiles.avgsize=16000000</span><br></pre></td></tr></table></figure>

<h1 id="spark中的小文件问题"><a href="#spark中的小文件问题" class="headerlink" title="spark中的小文件问题"></a>spark中的小文件问题</h1><h3 id="SparkStreaming如何解决小文件问题"><a href="#SparkStreaming如何解决小文件问题" class="headerlink" title="SparkStreaming如何解决小文件问题"></a>SparkStreaming如何解决小文件问题</h3><p>使用sparkstreaming时，如果实时计算结果要写入到HDFS，那么不可避免的会遇到一个问题，那就是在默认情况下会产生非常多的小文件，这是由sparkstreaming的微批处理模式和DStream(RDD)的分布式(partition)特性导致的，sparkstreaming为每个partition启动一个独立的线程来处理数据，一旦文件输出到HDFS，那么这个文件流就关闭了，再来一个batch的parttition任务，就再使用一个新的文件流，那么假设，一个batch为10s，每个输出的DStream有32个partition，那么一个小时产生的文件数将会达到(3600/10)*32=11520个之多。众多小文件带来的结果是有大量的文件元信息，比如文件的location、文件大小、block number等需要NameNode来维护，NameNode会因此鸭梨山大。不管是什么格式的文件，parquet、text,、JSON或者 Avro，都会遇到这种小文件问题，这里讨论几种处理Sparkstreaming小文件的典型方法。</p>
<h5 id="1-增加batch大小"><a href="#1-增加batch大小" class="headerlink" title="1 增加batch大小"></a>1 增加batch大小</h5><p>这种方法很容易理解，batch越大，从外部接收的event就越多，内存积累的数据也就越多，那么输出的文件数也就回变少，比如上边的时间从10s增加为100s，那么一个小时的文件数量就会减少到1152个。但别高兴太早，实时业务能等那么久吗，本来人家10s看到结果更新一次，现在要等快两分钟，是人都会骂娘。所以这种方法适用的场景是消息实时到达，但不想挤压在一起处理，因为挤压在一起处理的话，批处理任务在干等，这时就可以采用这种方法(是不是很像spark内部的pipeline模式，但是要注意区别哦)。</p>
<h5 id="2-Coalesce大法好？"><a href="#2-Coalesce大法好？" class="headerlink" title="2 Coalesce大法好？"></a>2 Coalesce大法好？</h5><p>文章开头讲了，小文件的基数是：batch_number*partition_number，而第一种方法是减少batch_number，那么这种方法就是减少partition_number了，这个api不细说，就是减少初始的分区个数。看过spark源码的童鞋都知道，对于窄依赖，一个子RDD的partition规则继承父RDD，对于宽依赖(就是那些个叉叉叉ByKey操作)，如果没有特殊指定分区个数，也继承自父rdd。那么初始的SourceDstream是几个partiion，最终的输出就是几个partition。所以Coalesce大法的好处就是，可以在最终要输出的时候，来减少一把partition个数。但是这个方法的缺点也很明显，本来是32个线程在写256M数据，现在可能变成了4个线程在写256M数据，而没有写完成这256M数据，这个batch是不算做结束的。那么一个batch的处理时延必定增长，batch挤压会逐渐增大。这种方法也要慎用，切鸡切鸡啊！</p>
<h5 id="3-SparkStreaming外部来处理"><a href="#3-SparkStreaming外部来处理" class="headerlink" title="3 SparkStreaming外部来处理"></a>3 SparkStreaming外部来处理</h5><p>我们既然把数据输出到hdfs，那么说明肯定是要用hive或者sparksql这样的“sql on hadoop”系统类进一步进行数据分析，而这些表一般都是按照半小时或者一小时、一天，这样来分区的(注意不要和sparkStreaming的分区混淆，这里的分区，是用来做分区裁剪优化的)，那么我们可以考虑在SparkStreaming外再启动定时的批处理任务来合并SparkStreaming产生的小文件。这种方法不是很直接，但是却比较有用，“性价比”较高，唯一要注意的是，批处理的合并任务在时间切割上要把握好，搞不好就可能回去合并一个还在写入的SparkStreaming小文件。</p>
<h5 id="自己调用foreach去append"><a href="#自己调用foreach去append" class="headerlink" title="自己调用foreach去append"></a>自己调用foreach去append</h5><p>SparkStreaming提供的foreach这个outout类api，可以让我们自定义输出计算结果的方法。那么我们其实也可以利用这个特性，那就是每个batch在要写文件时，并不是去生成一个新的文件流，而是把之前的文件打开。考虑这种方法的可行性，首先，HDFS上的文件不支持修改，但是很多都支持追加，那么每个batch的每个partition就对应一个输出文件，每次都去追加这个partition对应的输出文件，这样也可以实现减少文件数量的目的。这种方法要注意的就是不能无限制的追加，当判断一个文件已经达到某一个阈值时，就要产生一个新的文件进行追加了。</p>
<h1 id="如何避免Spark-SQL做数据导入时产生大量小文件"><a href="#如何避免Spark-SQL做数据导入时产生大量小文件" class="headerlink" title="如何避免Spark SQL做数据导入时产生大量小文件"></a>如何避免Spark SQL做数据导入时产生大量小文件</h1><h3 id="什么是小文件？"><a href="#什么是小文件？" class="headerlink" title="什么是小文件？"></a>什么是小文件？</h3><p>生产上，我们往往将Spark SQL作为Hive的替代方案，来获得SQL on Hadoop更出色的性能。因此，本文所讲的是指存储于HDFS中小文件，即指文件的大小远小于HDFS上块（dfs.block.size）大小的文件。</p>
<h3 id="小文件问题的影响-1"><a href="#小文件问题的影响-1" class="headerlink" title="小文件问题的影响"></a>小文件问题的影响</h3><p>一方面，大量的小文件会给Hadoop集群的扩展性和性能带来严重的影响。NameNode在内存中维护整个文件系统的元数据镜像，用户HDFS的管理；其中每个HDFS文件元信息（位置，大小，分块等）对象约占150字节，如果小文件过多，会占用大量内存，直接影响NameNode的性能。相对的，HDFS读写小文件也会更加耗时，因为每次都需要从NameNode获取元信息，并与对应的DataNode建立连接。如果NameNode在宕机中恢复，也需要更多的时间从元数据文件中加载。</p>
<p>另一方面，也会给Spark SQL等查询引擎造成查询性能的损耗，大量的数据分片信息以及对应产生的Task元信息也会给Spark Driver的内存造成压力，带来单点问题。此外，入库操作最后的commit job操作，在Spark Driver端单点做，很容易出现单点的性能问题。</p>
<h3 id="Spark小文件产生的过程"><a href="#Spark小文件产生的过程" class="headerlink" title="Spark小文件产生的过程"></a>Spark小文件产生的过程</h3><p>数据源本身就是就含大量小文件<br>动态分区插入数据，没有Shuffle的情况下，输入端有多少个逻辑分片，对应的HadoopRDD就会产生多少个HadoopPartition，每个Partition对应于Spark作业的Task（个数为M），分区数为N。最好的情况就是（M=N） &amp;&amp; （M中的数据也是根据N来预先打散的），那就刚好写N个文件；最差的情况下，每个Task中都有各个分区的记录，那文件数最终文件数将达到M * N个。这种情况下是极易产生小文件的。</p>
<p>比如我们拿TPCDS测试集中的store_sales进行举例， sql如下所示<br>use tpcds_1t_parquet;</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">INSERT overwrite table store_sales partition </span><br><span class="line">       ( </span><br><span class="line">              ss_sold_date_sk </span><br><span class="line">       ) </span><br><span class="line">SELECT ss_sold_time_sk, </span><br><span class="line">       ss_item_sk, </span><br><span class="line">       ss_customer_sk, </span><br><span class="line">       ss_cdemo_sk, </span><br><span class="line">       ss_hdemo_sk, </span><br><span class="line">       ss_addr_sk, </span><br><span class="line">       ss_store_sk, </span><br><span class="line">       ss_promo_sk, </span><br><span class="line">       ss_ticket_number, </span><br><span class="line">       ss_quantity, </span><br><span class="line">       ss_wholesale_cost, </span><br><span class="line">       ss_list_price, </span><br><span class="line">       ss_sales_price, </span><br><span class="line">       ss_ext_discount_amt, </span><br><span class="line">       ss_ext_sales_price, </span><br><span class="line">       ss_ext_wholesale_cost, </span><br><span class="line">       ss_ext_list_price, </span><br><span class="line">       ss_ext_tax, </span><br><span class="line">       ss_coupon_amt, </span><br><span class="line">       ss_net_paid, </span><br><span class="line">       ss_net_paid_inc_tax, </span><br><span class="line">       ss_net_profit, </span><br><span class="line">       ss_sold_date_sk </span><br><span class="line">FROM   tpcds_1t_ext.et_store_sales;</span><br></pre></td></tr></table></figure>

<p>首先我们得到其执行计划，如下所示，<br><font color='yellow'> Physical Plan </font></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">InsertIntoHiveTable MetastoreRelation tpcds_1t_parquet, store_sales, Map(ss_sold_date_sk -&gt; None), true, false</span><br><span class="line">+- HiveTableScan [ss_sold_time_sk#4L, ss_item_sk#5L, ss_customer_sk#6L, ss_cdemo_sk#7L, ss_hdemo_sk#8L, ss_addr_sk#9L, ss_store_sk#10L, ss_promo_sk#11L, ss_ticket_number#12L, ss_quantity#13, ss_wholesale_cost#14, ss_list_price#15, ss_sales_price#16, ss_ext_discount_amt#17, ss_ext_sales_price#18, ss_ext_wholesale_cost#19, ss_ext_list_price#20, ss_ext_tax#21, ss_coupon_amt#22, ss_net_paid#23, ss_net_paid_inc_tax#24, ss_net_profit#25, ss_sold_date_sk#3L], MetastoreRelation tpcds_1t_ext, et_store_sales</span><br></pre></td></tr></table></figure>

<p>store_sales的原生文件包含1616逻辑分片，对应生成1616 个Spark Task，插入动态分区表之后生成1824个数据分区加一个NULL值的分区，每个分区下都有可能生成1616个文件，这种情况下，最终的文件数量极有可能达到2949200。1T的测试集store_sales也就大概300g，这种情况每个文件可能就零点几M。</p>
<p>动态分区插入数据，有Shuffle的情况下，上面的M值就变成了spark.sql.shuffle.partitions(默认值200)这个参数值，文件数的算法和范围和2中基本一致。</p>
<p>比如，为了防止Shuffle阶段的数据倾斜我们可以在上面的sql中加上 distribute by rand()，这样我们的执行计划就变成了，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">InsertIntoHiveTable MetastoreRelation tpcds_1t_parquet, store_sales, Map(ss_sold_date_sk -&gt; None), true, false</span><br><span class="line">+- *Project [ss_sold_time_sk#4L, ss_item_sk#5L, ss_customer_sk#6L, ss_cdemo_sk#7L, ss_hdemo_sk#8L, ss_addr_sk#9L, ss_store_sk#10L, ss_promo_sk#11L, ss_ticket_number#12L, ss_quantity#13, ss_wholesale_cost#14, ss_list_price#15, ss_sales_price#16, ss_ext_discount_amt#17, ss_ext_sales_price#18, ss_ext_wholesale_cost#19, ss_ext_list_price#20, ss_ext_tax#21, ss_coupon_amt#22, ss_net_paid#23, ss_net_paid_inc_tax#24, ss_net_profit#25, ss_sold_date_sk#3L]</span><br><span class="line">   +- Exchange(coordinator id: 1080882047) hashpartitioning(_nondeterministic#49, 2048), coordinator[target post-shuffle partition size: 67108864]</span><br><span class="line">      +- *Project [ss_sold_time_sk#4L, ss_item_sk#5L, ss_customer_sk#6L, ss_cdemo_sk#7L, ss_hdemo_sk#8L, ss_addr_sk#9L, ss_store_sk#10L, ss_promo_sk#11L, ss_ticket_number#12L, ss_quantity#13, ss_wholesale_cost#14, ss_list_price#15, ss_sales_price#16, ss_ext_discount_amt#17, ss_ext_sales_price#18, ss_ext_wholesale_cost#19, ss_ext_list_price#20, ss_ext_tax#21, ss_coupon_amt#22, ss_net_paid#23, ss_net_paid_inc_tax#24, ss_net_profit#25, ss_sold_date_sk#3L, rand(4184439864130379921) AS _nondeterministic#49]</span><br><span class="line">         +- HiveTableScan [ss_sold_date_sk#3L, ss_sold_time_sk#4L, ss_item_sk#5L, ss_customer_sk#6L, ss_cdemo_sk#7L, ss_hdemo_sk#8L, ss_addr_sk#9L, ss_store_sk#10L, ss_promo_sk#11L, ss_ticket_number#12L, ss_quantity#13, ss_wholesale_cost#14, ss_list_price#15, ss_sales_price#16, ss_ext_discount_amt#17, ss_ext_sales_price#18, ss_ext_wholesale_cost#19, ss_ext_list_price#20, ss_ext_tax#21, ss_coupon_amt#22, ss_net_paid#23, ss_net_paid_inc_tax#24, ss_net_profit#25], MetastoreRelation tpcds_1t_ext, et_store_sales</span><br></pre></td></tr></table></figure>
<p>这种情况下，这样我们的文件数妥妥的就是spark.sql.shuffle.partitions * N，因为rand函数一般会把数据打散的非常均匀。当spark.sql.shuffle.partitions设置过大时，小文件问题就产生了；当spark.sql.shuffle.partitions设置过小时，任务的并行度就下降了，性能随之受到影响。<br>最理想的情况，当然是根据分区字段进行shuffle，在上面的sql中加上distribute by ss_sold_date_sk。 把同一分区的记录都哈希到同一个分区中去，由一个Spark的Task进行写入，这样的话只会产生N个文件，在我们的case中store_sales，在1825个分区下各种生成了一个数据文件。<br>但是这种情况下也容易出现数据倾斜的问题，比如双11的销售数据就很容易在这种情况下发生倾斜。</p>
<p>基于分区字段Shuffle可能出现数据倾斜</p>
<p>如上图所示，在我们插入store_sales时，就发生了null值的倾斜，大大的拖慢的数据入库的时间。</p>
<p>如何解决Spark SQL产生小文件问题</p>
<p>前面已经提到根据分区字段进行分区，除非每个分区下本身的数据较少，分区字段选择不合理，那么小文件问题基本上就不存在了，但是也有可能由于shuffle引入新的数据倾斜问题。<br>我们首先可以尝试是否可以将两者结合使用， 在之前的sql上加上distribute by ss_sold_date_sk，cast(rand() * 5 as int)， 这个类似于我们处理数据倾斜问题时候给字段加上后缀的形式。如，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">use tpcds_1t_parquet;</span><br><span class="line"></span><br><span class="line">INSERT overwrite table store_sales partition </span><br><span class="line">       ( </span><br><span class="line">              ss_sold_date_sk </span><br><span class="line">       ) </span><br><span class="line">SELECT ss_sold_time_sk, </span><br><span class="line">       ss_item_sk, </span><br><span class="line">       ss_customer_sk, </span><br><span class="line">       ss_cdemo_sk, </span><br><span class="line">       ss_hdemo_sk, </span><br><span class="line">       ss_addr_sk, </span><br><span class="line">       ss_store_sk, </span><br><span class="line">       ss_promo_sk, </span><br><span class="line">       ss_ticket_number, </span><br><span class="line">       ss_quantity, </span><br><span class="line">       ss_wholesale_cost, </span><br><span class="line">       ss_list_price, </span><br><span class="line">       ss_sales_price, </span><br><span class="line">       ss_ext_discount_amt, </span><br><span class="line">       ss_ext_sales_price, </span><br><span class="line">       ss_ext_wholesale_cost, </span><br><span class="line">       ss_ext_list_price, </span><br><span class="line">       ss_ext_tax, </span><br><span class="line">       ss_coupon_amt, </span><br><span class="line">       ss_net_paid, </span><br><span class="line">       ss_net_paid_inc_tax, </span><br><span class="line">       ss_net_profit, </span><br><span class="line">       ss_sold_date_sk </span><br><span class="line">FROM   tpcds_1t_ext.et_store_sales</span><br><span class="line">distribute by ss_sold_date_sk, cast(rand() * 5 as int);</span><br></pre></td></tr></table></figure>

<p>按照之前的推算，每个分区下将产生5个文件，同时null值倾斜部分的数据也被打散成五份进行计算，缓解了数据倾斜的问题 ，我们最终将得到1825 *5=9105个文件，如下所示<br>1825 9105 247111074494 /user/kyuubi/hive_db/tpcds_1t_parquet.db/store_sales</p>
<p>如果我们将5改得更小，文件数也会越少，但相应的倾斜key的计算时间也会上去。<br>在我们知道那个分区键倾斜的情况下，我们也可以将入库的SQL拆成几个部分，比如我们store_sales是因为null值倾斜，我们就可以通过where ss_sold_date_sk is not null 和 where ss_sold_date_sk is null 将原始数据分成两个部分。前者可以基于分区字段进行分区，如distribute by ss_sold_date_sk;后者可以基于随机值进行分区，distribute by cast(rand() * 5 as int), 这样可以静态的将null值部分分成五个文件。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">FROM   tpcds_1t_ext.et_store_sales </span><br><span class="line">where ss_sold_date_sk is not null</span><br><span class="line">distribute by ss_sold_date_sk;</span><br><span class="line"></span><br><span class="line">FROM   tpcds_1t_ext.et_store_sales </span><br><span class="line">where ss_sold_date_sk is null</span><br><span class="line">distribute by distribute by cast(rand() * 5 as int);</span><br></pre></td></tr></table></figure>

<p>对于倾斜部分的数据，我们可以开启Spark SQL的自适应功能，spark.sql.adaptive.enabled=true来动态调整每个相当于Spark的reduce端task处理的数据量，这样我们就不需要认为的感知随机值的规模了，我们可以直接</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FROM   tpcds_1t_ext.et_store_sales </span><br><span class="line">where ss_sold_date_sk is null</span><br><span class="line">distribute by distribute by rand() ;</span><br></pre></td></tr></table></figure>

<p>然后Spark在Shuffle 阶段会自动的帮我们将数据尽量的合并成spark.sql.adaptive.shuffle.targetPostShuffleInputSize（默认64m）的大小，以减少输出端写文件线程的总量，最后减少个数。<br>对于spark.sql.adaptive.shuffle.targetPostShuffleInputSize参数而言，我们也可以设置成为dfs.block.size的大小，这样可以做到和块对齐，文件大小可以设置的最为合理。</p>
<h1 id="sparkstreaming实时写入hive后合并小文件问题"><a href="#sparkstreaming实时写入hive后合并小文件问题" class="headerlink" title="sparkstreaming实时写入hive后合并小文件问题"></a>sparkstreaming实时写入hive后合并小文件问题</h1><p>今天主要来说一下sparksql写入hive后小文件太多,影响查询性能的问题.在另外一篇博客里面也稍微提到了一下,但还是感觉要单独说一下,首先我们要知道hive里面文件的数量=executor-coresnum-executorsjob数,所以如果我们batchDuration的设置的比较小的话,每天在一个分区里面就会生成很多的小文件,我们在hive里面查询的时候就会非常的影响性能,下面介绍两种方法优化小文件:</p>
<p>第一种,可以在创建的DataFrame的时候,cache一下,然后对DataFrame进行重新分区,可以把分区设置为1,可以用reparation,当然也可以用coalesce,这两个的区别,可以看我的另外一篇博客,这个时候就会一个job产生一个文件.但是这么做就降低了写入的性能,所以数据量不是特别大的时候,还是可以用的,但是如果数据量很大,就需谨慎使用,</p>
<p>第二种方法是利用sql定时执行一下,insert overwrite table a select * from a;这个时候会覆盖表的数据达到合并小文件的目的,具体的sql下面会有.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val df = spark.createDataFrame(rowRDD, schema).cache()</span><br><span class="line">          df.coalesce(1).createOrReplaceTempView(&quot;tempTable&quot;)</span><br><span class="line">          val sq = &quot;insert into combine_data partition(day_time=&#x27;&quot; + day_time + &quot;&#x27;) select * from tempTable&quot;</span><br><span class="line">          sql(sq)</span><br><span class="line">          println(&quot;插入hive成功了&quot;)</span><br><span class="line">          df.unpersist(true)</span><br><span class="line">insert overwrite table combine_data partition (day_time=&#x27;2018-08-01&#x27;) select data,enter_time from combine_data where day_time = &#x27;2018-08-01&#x27;</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiangzhiqi4551.github.io/2021/06/08/Spark%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%89%EF%BC%89%E9%94%AE%E5%80%BC%E5%AF%B9%E6%93%8D%E4%BD%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo1.jpeg">
      <meta itemprop="name" content="Master Jiang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiang's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/08/Spark%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%89%EF%BC%89%E9%94%AE%E5%80%BC%E5%AF%B9%E6%93%8D%E4%BD%9C/" class="post-title-link" itemprop="url">Spark入门之基础知识（三）键值对操作</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-08 18:33:21" itemprop="dateCreated datePublished" datetime="2021-06-08T18:33:21+08:00">2021-06-08</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-10-10 18:16:20" itemprop="dateModified" datetime="2021-10-10T18:16:20+08:00">2021-10-10</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <span id="more"></span>

<h2 id="键值对RDD在实际生产中很常用，通常用来进行聚合计算，并且Spark对键值对RDD也提供了新的操作接口可以做更多操作，本文简单介绍一些键值对RDD的基础操作。"><a href="#键值对RDD在实际生产中很常用，通常用来进行聚合计算，并且Spark对键值对RDD也提供了新的操作接口可以做更多操作，本文简单介绍一些键值对RDD的基础操作。" class="headerlink" title="键值对RDD在实际生产中很常用，通常用来进行聚合计算，并且Spark对键值对RDD也提供了新的操作接口可以做更多操作，本文简单介绍一些键值对RDD的基础操作。"></a>键值对RDD在实际生产中很常用，通常用来进行聚合计算，并且Spark对键值对RDD也提供了新的操作接口可以做更多操作，本文简单介绍一些键值对RDD的基础操作。</h2><h3 id="如何创建Pair-RDD"><a href="#如何创建Pair-RDD" class="headerlink" title="如何创建Pair RDD"></a>如何创建Pair RDD</h3><p>1）键值对格式的数据可以直接读入，返回Pair RDD<br>2）使用map()把一个普通的RDD转化为Pair RDD<br>读取text文件，取每行文本的第一个单词做key，该行文本做value</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val lines = context.textFile(&quot;text&quot;)</span><br><span class="line">lines.map(x =&gt; (x.split(&quot; &quot;)(0), x))</span><br></pre></td></tr></table></figure>

<h3 id="Pair-RDD的转化操作"><a href="#Pair-RDD的转化操作" class="headerlink" title="Pair RDD的转化操作"></a>Pair RDD的转化操作</h3><p>Pair RDD也是RDD，对RDD可用的操作对于Pair RDD也可用。</p>
<p>首先创建出一个Pair RDD</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  val data = sc.parallelize(List(1,2,3,4,4))</span><br><span class="line">data: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[7] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val data2 = data.map(x =&gt; (x,1))</span><br><span class="line">data2: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[8] at map at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line">scala&gt; data2.take(10)</span><br><span class="line">res16: Array[(Int, Int)] = Array((1,1), (2,1), (3,1), (4,1), (4,1))</span><br></pre></td></tr></table></figure>

<p>1)reduceByKey<br>合并具有相同键的值</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;     val result1 = data2.reduceByKey((x, y) =&gt; x + y)</span><br><span class="line">result1: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[10] at reduceByKey at &lt;console&gt;:28</span><br><span class="line">scala&gt; result1.collect()</span><br><span class="line">res19: Array[(Int, Int)] = Array((4,2), (2,1), (1,1), (3,1))     </span><br></pre></td></tr></table></figure>

<p>2）groupByKey<br>对相同键的值进行分组</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;     data2.groupByKey().collect()</span><br><span class="line">res20: Array[(Int, Iterable[Int])] = Array((4,CompactBuffer(1, 1)), (2,CompactBuffer(1)), (1,CompactBuffer(1)), (3,CompactBuffer(1)))</span><br></pre></td></tr></table></figure>

<p>3）mapValues<br>对Pair RDD中的每个值应用函数而不改变键</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;     data2.mapValues(x =&gt; x * 10).collect()</span><br><span class="line">res21: Array[(Int, Int)] = Array((1,10), (2,10), (3,10), (4,10), (4,10))  </span><br></pre></td></tr></table></figure>

<p>4）flatMapValues<br>应用函数到键值对中的值上，每一个KV对的Value都会被映射成一系列的值，这些值再和K重新组合成多个KV对</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;     data2.flatMapValues(x =&gt; x to (4)).collect()</span><br><span class="line">res22: Array[(Int, Int)] = Array((1,1), (1,2), (1,3), (1,4), (2,1), (2,2), (2,3), (2,4), (3,1), (3,2), (3,3), (3,4), (4,1), (4,2), (4,3), (4,4), (4,1), (4,2), (4,3), (4,4))</span><br></pre></td></tr></table></figure>

<p>5）keys<br>返回一个仅包含键值得RDD</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;     data2.keys.collect()</span><br><span class="line">res23: Array[Int] = Array(1, 2, 3, 4, 4)      </span><br></pre></td></tr></table></figure>

<p>6）values<br>返回一个仅包含值得RDD</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;     data2.values.collect()</span><br><span class="line">res24: Array[Int] = Array(1, 1, 1, 1, 1)</span><br></pre></td></tr></table></figure>

<p>7）sortByKey<br>返回一个根据键值排序的RDD（范围分区）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;     data2.sortByKey().collect()</span><br><span class="line">res25: Array[(Int, Int)] = Array((1,1), (2,1), (3,1), (4,1), (4,1))</span><br></pre></td></tr></table></figure>

<p>8）combineByKey<br>基于键进行聚合的函数，对于分区中的每一个键值，要么是已经遍历过的，要么是还没遍历过的。</p>
<p>如果是已经遍历过的：<br>使用 mergeValue 进行处理</p>
<p>如果是还没遍历过的：<br>使用createCombiner进行处理</p>
<p>如果多个分区中都有同一个键值：<br>使用mergeCombiner进行处理</p>
<p>计算平均值的例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">val data = sc.parallelize(List(&quot;a&quot;, &quot;aa&quot;, &quot;aaa&quot;, &quot;aaa&quot;, &quot;aaaa&quot;))</span><br><span class="line">val pair = data.map(x =&gt; (x, x.length))</span><br><span class="line">val result = pair.combineByKey(</span><br><span class="line">// 对于每一个新出现的key ，保存对应的value值  同时将个数初始化为1</span><br><span class="line">  valueOfKey =&gt; (valueOfKey, 1),   </span><br><span class="line">//  对于已经出现过的key值，将新出现的key对应的value进行累加，同时个数加一</span><br><span class="line">  (tmp: (Int, Int), newValue) =&gt; (tmp._1 + newValue, tmp._2 + 1),  </span><br><span class="line">//  多个分区进行合并时，如果两个分区有相同的key值，则把两个分区统计完的总value进行相加，同时计算值的个数</span><br><span class="line">  (tmp1: (Int, Int), tmp2: (Int, Int)) =&gt; (tmp1._1 + tmp2._1, tmp1._2 + tmp2._2)  </span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; result.take(10)</span><br><span class="line">res5: Array[(String, (Int, Int))] = Array((aa,(2,1)), (aaaa,(4,1)), (a,(1,1)), (aaa,(6,2)))</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// 计算平均值</span><br><span class="line">    val avg = result.map&#123; case (key, value) =&gt; (key, value._1 / value._2.toFloat) &#125;</span><br><span class="line"></span><br><span class="line">scala&gt; avg.take(10)</span><br><span class="line">res6: Array[(String, Float)] = Array((aa,2.0), (aaaa,4.0), (a,1.0), (aaa,3.0))</span><br></pre></td></tr></table></figure>


<p>9）查看分区数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;     val partitionSize = result.partitions.size</span><br><span class="line">partitionSize: Int = 2</span><br></pre></td></tr></table></figure>
<p>可以使用repartition或者coalesce进行重分区。</p>
<p>repartition和coalesce的区别：<br>repartition的底层实现为简单调用了coalesce，并将shuffle 设置为 true。<br>如果我们目前有1000个分区，想要重分区成100个，最好调用coalesce，因为整个过程不会发生shuffle。<br>如果我们有100个分区，想要重分区成1000个，这时候需要调用repartition，调用coalesce是无效的，因为不经过shuffle无法增加分区。</p>
<p>10）groupBy<br>对RDD中每个元素应用函数，函数的结果作为该元素的key，再根据key进行分组</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val data3 = sc.parallelize(List(1,2,3,4,5,6))</span><br><span class="line">val data4 = data3.groupBy( x =&gt; (x%2))</span><br><span class="line">data4.take(10)</span><br><span class="line"></span><br><span class="line">res7: Array[(Int, Iterable[Int])] = Array((0,CompactBuffer(2, 4, 6)), (1,CompactBuffer(1, 3, 5)))</span><br></pre></td></tr></table></figure>
<p>在上面的代码中我们将奇数分为一组，偶数分为一组。</p>
<p>11）join连接操作</p>
<p>内连接</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">	val ori1 = sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;))</span><br><span class="line">    val first = ori1.map(x =&gt; (x,1))</span><br><span class="line">    val ori2 = sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;e&quot;))</span><br><span class="line">    val second = ori2.map(x =&gt; (x,2))</span><br><span class="line">    val joinResult = first.join(second)</span><br><span class="line">    joinResult.take(10)</span><br><span class="line"></span><br><span class="line">res8: Array[(String, (Int, Int))] = Array((b,(1,2)), (a,(1,2)), (c,(1,2)))      </span><br></pre></td></tr></table></figure>

<p>左外连接（右外连接同理）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">	val leftResult = first.leftOuterJoin(second)</span><br><span class="line">    leftResult.take(10)</span><br><span class="line"></span><br><span class="line">scala&gt;     leftResult.take(10)</span><br><span class="line">res9: Array[(String, (Int, Option[Int]))] = Array((d,(1,None)), (b,(1,Some(2))), (a,(1,Some(2))), (c,(1,Some(2))))</span><br></pre></td></tr></table></figure>


<p>join操作的执行过程：<br>默认情况下，连接操作会将两个数据集中所有键的哈希值都求出来，将哈希值相同的记录通过网络传输到同一台机器上，在该机器上对所有键相同的记录执行连接操作。</p>
<h3 id="Pair-RDD的行动操作"><a href="#Pair-RDD的行动操作" class="headerlink" title="Pair RDD的行动操作"></a>Pair RDD的行动操作</h3><p><img src="https://img-blog.csdnimg.cn/20191219171254168.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h6 id="Spark会为生成RDD设定好分区方式的操作"><a href="#Spark会为生成RDD设定好分区方式的操作" class="headerlink" title="Spark会为生成RDD设定好分区方式的操作"></a>Spark会为生成RDD设定好分区方式的操作</h6><p>cogroup<br>groupWith<br>join<br>leftOuterJoin<br>rightOuterJoin<br>groupByKey<br>reduceByKey<br>combineByKey<br>partitionBy<br>sort<br>mapValues<br>flatMapValues<br>filter</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiangzhiqi4551.github.io/2021/06/08/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%A6%82%E4%BD%95%E5%88%86%E5%B1%82/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo1.jpeg">
      <meta itemprop="name" content="Master Jiang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiang's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/08/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%A6%82%E4%BD%95%E5%88%86%E5%B1%82/" class="post-title-link" itemprop="url">数据仓库之如何分层</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-08 18:27:45" itemprop="dateCreated datePublished" datetime="2021-06-08T18:27:45+08:00">2021-06-08</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-10-10 18:16:20" itemprop="dateModified" datetime="2021-10-10T18:16:20+08:00">2021-10-10</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">数据仓库</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <span id="more"></span>

<h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1.前言"></a>1.前言</h1><p>数据仓库的建设是一个持续的工程。在这个过程中我们需要形成自己的规范，以方便管理和维护。在数据仓库的建设过程中，不仅会面临着公司业务迅速发展，业务系统迭代变更，需要对业务系统数据进行相应的整合，形成公司完整的统一数据视图；而且基于数据仓库的应用也是多样化的，比如支撑自己企业的数据可视化平台、即席查询、对策略提供数据支持等。参考目前已有的分层模型，结合自身实际数据情况，确定对数据仓库进行层次划分，不同的层次，承担不同的职责，方便模型的管理与维护。</p>
<h1 id="2-数仓模型"><a href="#2-数仓模型" class="headerlink" title="2.数仓模型"></a>2.数仓模型</h1><h3 id="2-1-数仓分层介绍"><a href="#2-1-数仓分层介绍" class="headerlink" title="2.1 数仓分层介绍"></a>2.1 数仓分层介绍</h3><table>
<thead>
<tr>
<th align="center">模型层次</th>
<th align="center">英文名称</th>
<th align="center">中文名称</th>
<th align="center">对应逻辑层</th>
<th align="center">存储数据</th>
</tr>
</thead>
<tbody><tr>
<td align="center">ODL</td>
<td align="center">Operational Data Layer</td>
<td align="center">操作数据层，原始数据层，日志层</td>
<td align="center">ods</td>
<td align="center">存储直接从业务系统或者日志系统接收的原始数据，只同步不做任何修改处理</td>
</tr>
<tr>
<td align="center">IDL</td>
<td align="center">Integrated Data Layer</td>
<td align="center">集成数据层，事实层，明细数据层</td>
<td align="center">dwd</td>
<td align="center">存放按照业务主题组织的事实数据，未补充维度，不做过度加工（不加分析的口径），保留原始事实</td>
</tr>
<tr>
<td align="center">CDL</td>
<td align="center">Component Data Layer</td>
<td align="center">元件数据层，（轻度）汇总层</td>
<td align="center">dws</td>
<td align="center">存放从dwd经过汇总或者跨业务主题的数据，以面向单个分析场景为主组织主题，可以引入指标口径</td>
</tr>
<tr>
<td align="center">MDL</td>
<td align="center">Mart Data Layer</td>
<td align="center">数据集市层，汇总宽表层</td>
<td align="center">dws</td>
<td align="center">存放从dwd,dws来源的多维度冗余的宽表，可面向多个分析场景组织</td>
</tr>
<tr>
<td align="center">ADL</td>
<td align="center">Application Data Layer</td>
<td align="center">应用数据层</td>
<td align="center">dm</td>
<td align="center">存放从dws,dwd来源的面向单个特定分析场景的灵活数据</td>
</tr>
<tr>
<td align="center">DIM</td>
<td align="center">Dimension Data Layer</td>
<td align="center">维度层</td>
<td align="center">dim</td>
<td align="center">存放维度数据</td>
</tr>
</tbody></table>
<h3 id="2-2-模型思想"><a href="#2-2-模型思想" class="headerlink" title="2.2 模型思想"></a>2.2 模型思想</h3><h5 id="ODL模型"><a href="#ODL模型" class="headerlink" title="ODL模型"></a><font color='yellow'>ODL模型</font></h5><p><em><strong>数据层次</strong></em></p>
<p>ODL（操作数据层），该层级主要临时存储从多种数据源（包括在线业务系统和点击流日志）抽取的业务数据。</p>
<p><em><strong>数据特点</strong></em></p>
<p>1.数据集结构及数据集间关系都和数据源基本保持一致</p>
<p>2.临时存储，数据存储一到两周即可删除或备份至廉价设备</p>
<p>3.数据集多为增量抽取，产生大量的Delta数据集</p>
<p><em><strong>数据处理</strong></em></p>
<p>1.数据集增量获取、分发</p>
<p>2.数据集轻度清洗，如字符集转换、脏数据过滤、第一类维值标准化处理等</p>
<p>3.点击流数据处理，完成日志获取、字符串处理、URL解析等</p>
<p><em><strong>数据抽取</strong></em></p>
<p>主要是增量抽取为主、有部分业务表涉及全量抽取；</p>
<p><em><strong>数据存储</strong></em></p>
<p>ODL层设计上分为两个层次，第一个层次存储近一段时间的增量数据（贴源）；</p>
<p>第二个层次存储全量数据信息，通过append delta表生成全量数据；</p>
<h5 id="IDL模型"><a href="#IDL模型" class="headerlink" title="IDL模型"></a><font color='yellow'>IDL模型</font></h5><p><em><strong>数据层次</strong></em></p>
<p>IDL（集成数据层），该层级按照业务主题组织数据，完成对ODL层数据的清洗和集成，为CDL层提供数据结构统一、业务语义标准的基础数据。</p>
<p><em><strong>数据特点</strong></em></p>
<p>1.明细数据，按照业务主题分类，以业务为驱动设计表结构和表间关系</p>
<p>2.数据集成，基于3NF设计模型，并在语义层达到统一和标准</p>
<p>3.数据带有仓库层的日期和状态标签，可追溯其生命周期中的所有变化状态</p>
<p><em><strong>数据处理</strong></em></p>
<p>1.对ODL数据进行集成整合，数据项进行重定义和清洗，完成业务数据的归一化处理</p>
<p>2.梳理第一类维表来源，即从源业务系统抽取的代码表，并完成缓慢变化维处理</p>
<p>3.使用ODL层的Delta（增、删、改）数据、全量数据更新当前表和历史表，数据存储上采用拉链和快照方式存储</p>
<p><em><strong>数据更新策略</strong></em></p>
<p>1.全量快照：每天存储一份最新的数据，来源数据为全量数据，且需要保留历史变化轨迹</p>
<p>2.拉链表：通过开闭链时间维护最新数据</p>
<p>3.增量表：增量插入当天分区，例如：日志表</p>
<p>4.全量覆盖：删除目标表全部数据，再插入当前数据；来源数据为全量数据，且无需保留历史轨迹，只使用最新状态数据</p>
<h5 id="CDL模型"><a href="#CDL模型" class="headerlink" title="CDL模型"></a><font color='yellow'>CDL模型</font></h5><p><em><strong>数据层次</strong></em></p>
<p>CDL（元件数据层），该层级按照分析主题组织数据，跨IDL层的业务主题，集成与该分析主题相关的所有数据，为ADL层的分析模型提供共享的、可复用的元件数据。</p>
<p><em><strong>数据特点</strong></em></p>
<p>1.数据模型相对稳定，无衍生指标，轻度汇总</p>
<p>2.多维模型：分析对象的状态（静态、描述）数据和相关事实表或维表关联形成以冗余宽表为中心的雪花或星型模型</p>
<p>3.基础指标库：分析对象的行为（主动、被动）数据汇总而成的一系列基础指标库</p>
<p><em><strong>数据处理</strong></em></p>
<p>1.分析对象和相关事实表或维表进行多表关联计算生成多维模型</p>
<p>2.对分析对象的行为数据进行汇总计算生成基础指标库</p>
<p>3.梳理两类维表来源，一是分析需求，二是仓库技术</p>
<p>4.对多维模型或基础指标数据进行轻度汇总，产生基础的、通用的汇总模型</p>
<p><em><strong>数据种类</strong></em></p>
<p>1.多维模型数据（Multidimensional Data）：采用维度建模方式建立的数据模型数据。</p>
<p>2.基础指标库数据(Stable Indicator Data)：基于某个分析实体的一系列基础指标集合。</p>
<p>3.常用通用的JOIN数据（Common Join Data）：从IDL层上来的一些实体对象，可能需要经常JOIN在一起使用，在此可以预先处理一些常用通用的JOIN逻辑。</p>
<p><em><strong>数据刷新</strong></em></p>
<p>保留每日数据的应用状态，存储采用每日数据快照的方式</p>
<h5 id="MDL模型"><a href="#MDL模型" class="headerlink" title="MDL模型"></a><font color='yellow'>MDL模型</font></h5><p><em><strong>数据层次</strong></em></p>
<p>MDL（数据集市层），该层次主要功能是加工多维度冗余的宽表（解决复杂的查询）、多角度分析的汇总表。</p>
<p><em><strong>数据特点</strong></em></p>
<p>1.数据模型相对稳定，有衍生指标</p>
<p>2.宽表模型：基础指标群、多维模型数据和相关事实表或维表关联形成通用或定制的冗余宽表</p>
<p>3.多角度汇总：从多个角度分析的汇总模型</p>
<p><em><strong>数据处理</strong></em></p>
<p>1.多维模型数据和相关事实表或维表进行多表关联计算生成宽表模型</p>
<p>2.对多维模型或基础指标数据进行汇总，产生个性的、通用的汇总模型</p>
<h5 id="ADL模型"><a href="#ADL模型" class="headerlink" title="ADL模型"></a><font color='yellow'>ADL模型</font></h5><p><em><strong>数据层次</strong></em></p>
<p>ADL（应用数据层），该层级按照项目和应用组织数据，以CDL层的半成品元件数据为基础，规划多样化、个性化的衍生指标体系、分析模型和数据应用。</p>
<p><em><strong>数据特点</strong></em></p>
<p>1.数据模型不稳定，随着分析算法和应用的变更随时变化或下线</p>
<p>2.数据高度汇总，可做交叉分析、上卷、下钻、切片、切块、旋转等多维分析操作</p>
<p>3.更高级的数据分析或挖掘应用，衍生出信息类、知识类数据</p>
<p><em><strong>数据处理</strong></em></p>
<p>1.根据不同的数据应用处理数据，所有的指标或者汇总都依赖于具体的业务分析主题和分析人员的定义，并直接交付信息给使用者</p>
<p>2.数据处理和信息交付方式多样，如报表、仪表盘、即席查询、多维分析、实时数据应用、数据挖掘应用等</p>
<h5 id="DIM模型"><a href="#DIM模型" class="headerlink" title="DIM模型"></a><font color='yellow'>DIM模型</font></h5><p>DIM层主要包括三类即简单、静态、代码类维表，存储仓库层归纳梳理的所有维表信息</p>
<p>1).从业务源系统抽取转化的维表，每日保留全量快照；</p>
<p>2).根据业务分析需求构建的维表，每日保留全量快照；</p>
<p>3).仓库技术常用维表，只保留当前信息；</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiangzhiqi4551.github.io/2021/06/08/Hadoop-NameNode-%E9%AB%98%E5%8F%AF%E7%94%A8-High-Availability-%E5%AE%9E%E7%8E%B0%E8%A7%A3%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo1.jpeg">
      <meta itemprop="name" content="Master Jiang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiang's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/08/Hadoop-NameNode-%E9%AB%98%E5%8F%AF%E7%94%A8-High-Availability-%E5%AE%9E%E7%8E%B0%E8%A7%A3%E6%9E%90/" class="post-title-link" itemprop="url">Hadoop NameNode 高可用 (High Availability) 实现解析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-08 18:25:38" itemprop="dateCreated datePublished" datetime="2021-06-08T18:25:38+08:00">2021-06-08</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-10-10 18:16:20" itemprop="dateModified" datetime="2021-10-10T18:16:20+08:00">2021-10-10</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <span id="more"></span>

<p><a target="_blank" rel="noopener" href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/index.html">原文链接</a></p>
<h1 id="NameNode-高可用整体架构概述"><a href="#NameNode-高可用整体架构概述" class="headerlink" title="NameNode 高可用整体架构概述"></a>NameNode 高可用整体架构概述</h1><p>在 Hadoop 1.0 时代，Hadoop 的两大核心组件 HDFS NameNode 和 JobTracker 都存在着单点问题，这其中以 NameNode 的单点问题尤为严重。因为 NameNode 保存了整个 HDFS 的元数据信息，一旦 NameNode 挂掉，整个 HDFS 就无法访问，同时 Hadoop 生态系统中依赖于 HDFS 的各个组件，包括 MapReduce、Hive、Pig 以及 HBase 等也都无法正常工作，并且重新启动 NameNode 和进行数据恢复的过程也会比较耗时。这些问题在给 Hadoop 的使用者带来困扰的同时，也极大地限制了 Hadoop 的使用场景，使得 Hadoop 在很长的时间内仅能用作离线存储和离线计算，无法应用到对可用性和数据一致性要求很高的在线应用场景中。</p>
<p>所幸的是，在 Hadoop2.0 中，HDFS NameNode 和 YARN ResourceManger(JobTracker 在 2.0 中已经被整合到 YARN ResourceManger 之中) 的单点问题都得到了解决，经过多个版本的迭代和发展，目前已经能用于生产环境。HDFS NameNode 和 YARN ResourceManger 的高可用 (High Availability，HA) 方案基本类似，两者也复用了部分代码，但是由于 HDFS NameNode 对于数据存储和数据一致性的要求比 YARN ResourceManger 高得多，所以 HDFS NameNode 的高可用实现更为复杂一些，本文从内部实现的角度对 HDFS NameNode 的高可用机制进行详细的分析。</p>
<p>HDFS NameNode 的高可用整体架构如图 1 所示 (图片来源于参考文献 [1])：<br>图 1.HDFS NameNode 高可用整体架构<br><img src="https://img-blog.csdnimg.cn/20200209113132878.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>从上图中，我们可以看出 NameNode 的高可用架构主要分为下面几个部分：</p>
<p>Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务。</p>
<p>主备切换控制器 ZKFailoverController：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换，当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换。</p>
<p>Zookeeper 集群：为主备切换控制器提供主备选举支持。</p>
<p>共享存储系统：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和</p>
<p>NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。</p>
<p>DataNode 节点：除了通过共享存储系统共享 HDFS 的元数据信息之外，主 NameNode 和备 NameNode 还需要共享 HDFS 的数据块和 DataNode 之间的映射关系。DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。</p>
<p>下面开始分别介绍 NameNode 的主备切换实现和共享存储系统的实现，在文章的最后会结合笔者的实践介绍一下在 NameNode 的高可用运维中的一些注意事项。</p>
<h1 id="NameNode-的主备切换实现"><a href="#NameNode-的主备切换实现" class="headerlink" title="NameNode 的主备切换实现"></a>NameNode 的主备切换实现</h1><p>NameNode 主备切换主要由 ZKFailoverController、HealthMonitor 和 ActiveStandbyElector 这 3 个组件来协同实现：</p>
<p>ZKFailoverController 作为 NameNode 机器上一个独立的进程启动 (在 hdfs 启动脚本之中的进程名为 zkfc)，启动的时候会创建 HealthMonitor 和 ActiveStandbyElector 这两个主要的内部组件，ZKFailoverController 在创建 HealthMonitor 和 ActiveStandbyElector 的同时，也会向 HealthMonitor 和 ActiveStandbyElector 注册相应的回调方法。</p>
<p>HealthMonitor 主要负责检测 NameNode 的健康状态，如果检测到 NameNode 的状态发生变化，会回调 ZKFailoverController 的相应方法进行自动的主备选举。</p>
<p>ActiveStandbyElector 主要负责完成自动的主备选举，内部封装了 Zookeeper 的处理逻辑，一旦 Zookeeper 主备选举完成，会回调 ZKFailoverController 的相应方法来进行 NameNode 的主备状态切换。</p>
<p>NameNode 实现主备切换的流程如图 2 所示，有以下几步：</p>
<p>1、HealthMonitor 初始化完成之后会启动内部的线程来定时调用对应 NameNode 的 HAServiceProtocol RPC 接口的方法，对 NameNode 的健康状态进行检测。<br>2、HealthMonitor 如果检测到 NameNode 的健康状态发生变化，会回调ZKFailoverController 注册的相应方法进行处理。<br>3、如果 ZKFailoverController 判断需要进行主备切换，会首先使用 ActiveStandbyElector 来进行自动的主备选举。<br>4、ActiveStandbyElector 与 Zookeeper 进行交互完成自动的主备选举。<br>5、ActiveStandbyElector 在主备选举完成后，会回调 ZKFailoverController 的相应方法来通知当前的 NameNode 成为主 NameNode 或备 NameNode。<br>6、ZKFailoverController 调用对应 NameNode 的 HAServiceProtocol RPC 接口的方法将 NameNode 转换为 Active 状态或 Standby 状态。<br>图 2.NameNode 的主备切换流程</p>
<p><img src="https://img-blog.csdnimg.cn/20200209113344704.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>下面分别对 HealthMonitor、ActiveStandbyElector 和 ZKFailoverController 的实现细节进行分析：</p>
<h2 id="HealthMonitor-实现分析"><a href="#HealthMonitor-实现分析" class="headerlink" title="HealthMonitor 实现分析"></a>HealthMonitor 实现分析</h2><p>ZKFailoverController 在初始化的时候会创建 HealthMonitor，HealthMonitor 在内部会启动一个线程来循环调用 NameNode 的 HAServiceProtocol RPC 接口的方法来检测 NameNode 的状态，并将状态的变化通过回调的方式来通知 ZKFailoverController。</p>
<p>HealthMonitor 主要检测 NameNode 的两类状态，分别是 HealthMonitor.State 和 HAServiceStatus。HealthMonitor.State 是通过 HAServiceProtocol RPC 接口的 monitorHealth 方法来获取的，反映了 NameNode 节点的健康状况，主要是磁盘存储资源是否充足。HealthMonitor.State 包括下面几种状态：</p>
<ol>
<li>INITIALIZING：HealthMonitor 在初始化过程中，还没有开始进行健康状况检测；</li>
<li>SERVICE_HEALTHY：NameNode 状态正常；</li>
<li>SERVICE_NOT_RESPONDING：调用 NameNode 的 monitorHealth 方法调用无响应或响应超时；</li>
<li>SERVICE_UNHEALTHY：NameNode 还在运行，但是 monitorHealth 方法返回状态不正常，磁盘存储资源不足；</li>
<li>HEALTH_MONITOR_FAILED：HealthMonitor 自己在运行过程中发生了异常，不能继续检测 NameNode 的健康状况，会导致 ZKFailoverController 进程退出；</li>
</ol>
<p>HealthMonitor.State 在状态检测之中起主要的作用，在 HealthMonitor.State 发生变化的时候，HealthMonitor 会回调 ZKFailoverController 的相应方法来进行处理，具体处理见后文 ZKFailoverController 部分所述。</p>
<p>而 HAServiceStatus 则是通过 HAServiceProtocol RPC 接口的 getServiceStatus 方法来获取的，主要反映的是 NameNode 的 HA 状态，包括：</p>
<ol>
<li>INITIALIZING：NameNode 在初始化过程中；</li>
<li>ACTIVE：当前 NameNode 为主 NameNode；</li>
<li>STANDBY：当前 NameNode 为备 NameNode；</li>
<li>STOPPING：当前 NameNode 已停止；</li>
</ol>
<p>HAServiceStatus 在状态检测之中只是起辅助的作用，在 HAServiceStatus 发生变化时，HealthMonitor 也会回调 ZKFailoverController 的相应方法来进行处理，具体处理见后文 ZKFailoverController 部分所述。</p>
<h2 id="ActiveStandbyElector-实现分析"><a href="#ActiveStandbyElector-实现分析" class="headerlink" title="ActiveStandbyElector 实现分析"></a>ActiveStandbyElector 实现分析</h2><p>Namenode(包括 YARN ResourceManager) 的主备选举是通过 ActiveStandbyElector 来完成的，ActiveStandbyElector 主要是利用了 Zookeeper 的写一致性和临时节点机制，具体的主备选举实现如下：</p>
<h4 id="创建锁节点"><a href="#创建锁节点" class="headerlink" title="创建锁节点"></a>创建锁节点</h4><p>如果 HealthMonitor 检测到对应的 NameNode 的状态正常，那么表示这个 NameNode 有资格参加 Zookeeper 的主备选举。如果目前还没有进行过主备选举的话，那么相应的 ActiveStandbyElector 就会发起一次主备选举，尝试在 Zookeeper 上创建一个路径为/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 的临时节点 (${dfs.nameservices} 为 Hadoop 的配置参数 dfs.nameservices 的值，下同)，Zookeeper 的写一致性会保证最终只会有一个 ActiveStandbyElector 创建成功，那么创建成功的 ActiveStandbyElector 对应的 NameNode 就会成为主 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的方法进一步将对应的 NameNode 切换为 Active 状态。而创建失败的 ActiveStandbyElector 对应的 NameNode 成为备 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的方法进一步将对应的 NameNode 切换为 Standby 状态。</p>
<h4 id="注册-Watcher-监听"><a href="#注册-Watcher-监听" class="headerlink" title="注册 Watcher 监听"></a>注册 Watcher 监听</h4><p>不管创建/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 节点是否成功，ActiveStandbyElector 随后都会向 Zookeeper 注册一个 Watcher 来监听这个节点的状态变化事件，ActiveStandbyElector 主要关注这个节点的 NodeDeleted 事件。</p>
<h4 id="自动触发主备选举"><a href="#自动触发主备选举" class="headerlink" title="自动触发主备选举"></a>自动触发主备选举</h4><p>如果 Active NameNode 对应的 HealthMonitor 检测到 NameNode 的状态异常时， ZKFailoverController 会主动删除当前在 Zookeeper 上建立的临时节点/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock，这样处于 Standby 状态的 NameNode 的 ActiveStandbyElector 注册的监听器就会收到这个节点的 NodeDeleted 事件。收到这个事件之后，会马上再次进入到创建/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 节点的流程，如果创建成功，这个本来处于 Standby 状态的 NameNode 就选举为主 NameNode 并随后开始切换为 Active 状态。</p>
<p>当然，如果是 Active 状态的 NameNode 所在的机器整个宕掉的话，那么根据 Zookeeper 的临时节点特性，/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 节点会自动被删除，从而也会自动进行一次主备切换。</p>
<p>ActiveStandbyElector 为了实现 fencing，会在成功创建 Zookeeper 节点 hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 从而成为 Active NameNode 之后，创建另外一个路径为/hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb 的持久节点，这个节点里面保存了这个 Active NameNode 的地址信息。Active NameNode 的 ActiveStandbyElector 在正常的状态下关闭 Zookeeper Session 的时候 (注意由于/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 是临时节点，也会随之删除)，会一起删除节点/hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb。但是如果 ActiveStandbyElector 在异常的状态下 Zookeeper Session 关闭 (比如前述的 Zookeeper 假死)，那么由于/hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb 是持久节点，会一直保留下来。后面当另一个 NameNode 选主成功之后，会注意到上一个 Active NameNode 遗留下来的这个节点，从而会回调 ZKFailoverController 的方法对旧的 Active NameNode 进行 fencing，具体处理见后文 ZKFailoverController 部分所述。</p>
<h4 id="防止脑裂"><a href="#防止脑裂" class="headerlink" title="防止脑裂"></a>防止脑裂</h4><p>Zookeeper 在工程实践的过程中经常会发生的一个现象就是 Zookeeper 客户端“假死”，所谓的“假死”是指如果 Zookeeper 客户端机器负载过高或者正在进行 JVM Full GC，那么可能会导致 Zookeeper 客户端到 Zookeeper 服务端的心跳不能正常发出，一旦这个时间持续较长，超过了配置的 Zookeeper Session Timeout 参数的话，Zookeeper 服务端就会认为客户端的 session 已经过期从而将客户端的 Session 关闭。“假死”有可能引起分布式系统常说的双主或脑裂 (brain-split) 现象。具体到本文所述的 NameNode，假设 NameNode1 当前为 Active 状态，NameNode2 当前为 Standby 状态。如果某一时刻 NameNode1 对应的 ZKFailoverController 进程发生了“假死”现象，那么 Zookeeper 服务端会认为 NameNode1 挂掉了，根据前面的主备切换逻辑，NameNode2 会替代 NameNode1 进入 Active 状态。但是此时 NameNode1 可能仍然处于 Active 状态正常运行，即使随后 NameNode1 对应的 ZKFailoverController 因为负载下降或者 Full GC 结束而恢复了正常，感知到自己和 Zookeeper 的 Session 已经关闭，但是由于网络的延迟以及 CPU 线程调度的不确定性，仍然有可能会在接下来的一段时间窗口内 NameNode1 认为自己还是处于 Active 状态。这样 NameNode1 和 NameNode2 都处于 Active 状态，都可以对外提供服务。这种情况对于 NameNode 这类对数据一致性要求非常高的系统来说是灾难性的，数据会发生错乱且无法恢复。Zookeeper 社区对这种问题的解决方法叫做 fencing，中文翻译为隔离，也就是想办法把旧的 Active NameNode 隔离起来，使它不能正常对外提供服务。</p>
<h2 id="ZKFailoverController-实现分析"><a href="#ZKFailoverController-实现分析" class="headerlink" title="ZKFailoverController 实现分析"></a>ZKFailoverController 实现分析</h2><p>ZKFailoverController 在创建 HealthMonitor 和 ActiveStandbyElector 的同时，会向 HealthMonitor 和 ActiveStandbyElector 注册相应的回调函数，ZKFailoverController 的处理逻辑主要靠 HealthMonitor 和 ActiveStandbyElector 的回调函数来驱动。</p>
<h4 id="对-HealthMonitor-状态变化的处理"><a href="#对-HealthMonitor-状态变化的处理" class="headerlink" title="对 HealthMonitor 状态变化的处理"></a>对 HealthMonitor 状态变化的处理</h4><p>如前所述，HealthMonitor 会检测 NameNode 的两类状态，HealthMonitor.State 在状态检测之中起主要的作用，ZKFailoverController 注册到 HealthMonitor 上的处理 HealthMonitor.State 状态变化的回调函数主要关注 SERVICE_HEALTHY、SERVICE_NOT_RESPONDING 和 SERVICE_UNHEALTHY 这 3 种状态：</p>
<ol>
<li>如果检测到状态为 SERVICE_HEALTHY，表示当前的 NameNode 有资格参加 Zookeeper 的主备选举，如果目前还没有进行过主备选举的话，ZKFailoverController 会调用 ActiveStandbyElector 的 joinElection 方法发起一次主备选举。</li>
<li>如果检测到状态为 SERVICE_NOT_RESPONDING 或者是 SERVICE_UNHEALTHY，就表示当前的 NameNode 出现问题了，ZKFailoverController 会调用 ActiveStandbyElector 的 quitElection 方法删除当前已经在 Zookeeper 上建立的临时节点退出主备选举，这样其它的 NameNode 就有机会成为主 NameNode。</li>
</ol>
<p>而 HAServiceStatus 在状态检测之中仅起辅助的作用，在 HAServiceStatus 发生变化时，ZKFailoverController 注册到 HealthMonitor 上的处理 HAServiceStatus 状态变化的回调函数会判断 NameNode 返回的 HAServiceStatus 和 ZKFailoverController 所期望的是否一致，如果不一致的话，ZKFailoverController 也会调用 ActiveStandbyElector 的 quitElection 方法删除当前已经在 Zookeeper 上建立的临时节点退出主备选举。</p>
<h4 id="对-ActiveStandbyElector-主备选举状态变化的处理"><a href="#对-ActiveStandbyElector-主备选举状态变化的处理" class="headerlink" title="对 ActiveStandbyElector 主备选举状态变化的处理"></a>对 ActiveStandbyElector 主备选举状态变化的处理</h4><p>在 ActiveStandbyElector 的主备选举状态发生变化时，会回调 ZKFailoverController 注册的回调函数来进行相应的处理：</p>
<ol>
<li>如果 ActiveStandbyElector 选主成功，那么 ActiveStandbyElector 对应的 NameNode 成为主 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的 becomeActive 方法，这个方法通过调用对应的 NameNode 的 HAServiceProtocol RPC 接口的 transitionToActive 方法，将 NameNode 转换为 Active 状态。</li>
<li>如果 ActiveStandbyElector 选主失败，那么 ActiveStandbyElector 对应的 NameNode 成为备 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的 becomeStandby 方法，这个方法通过调用对应的 NameNode 的 HAServiceProtocol RPC 接口的 transitionToStandby 方法，将 NameNode 转换为 Standby 状态。</li>
<li>如果 ActiveStandbyElector 选主成功之后，发现了上一个 Active NameNode 遗留下来的/hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb 节点 (见“ActiveStandbyElector 实现分析”一节“防止脑裂”部分所述)，那么 ActiveStandbyElector 会首先回调 ZKFailoverController 注册的 fenceOldActive 方法，尝试对旧的 Active NameNode 进行 fencing，在进行 fencing 的时候，会执行以下的操作：</li>
</ol>
<p>首先尝试调用这个旧 Active NameNode 的 HAServiceProtocol RPC 接口的 transitionToStandby 方法，看能不能把它转换为 Standby 状态。<br>如果 transitionToStandby 方法调用失败，那么就执行 Hadoop 配置文件之中预定义的隔离措施，Hadoop 目前主要提供两种隔离措施，通常会选择 sshfence：</p>
<p>sshfence：通过 SSH 登录到目标机器上，执行命令 fuser 将对应的进程杀死；<br>shellfence：执行一个用户自定义的 shell 脚本来将对应的进程隔离；</p>
<p>只有在成功地执行完成 fencing 之后，选主成功的 ActiveStandbyElector 才会回调 ZKFailoverController 的 becomeActive 方法将对应的 NameNode 转换为 Active 状态，开始对外提供服务。</p>
<h1 id="NameNode-的共享存储实现"><a href="#NameNode-的共享存储实现" class="headerlink" title="NameNode 的共享存储实现"></a>NameNode 的共享存储实现</h1><p>过去几年中 Hadoop 社区涌现过很多的 NameNode 共享存储方案，比如 shared NAS+NFS、BookKeeper、BackupNode 和 QJM(Quorum Journal Manager) 等等。目前社区已经把由 Clouderea 公司实现的基于 QJM 的方案合并到 HDFS 的 trunk 之中并且作为默认的共享存储实现，本部分只针对基于 QJM 的共享存储方案的内部实现原理进行分析。为了理解 QJM 的设计和实现，首先要对 NameNode 的元数据存储结构有所了解。</p>
<h4 id="NameNode-的元数据存储概述"><a href="#NameNode-的元数据存储概述" class="headerlink" title="NameNode 的元数据存储概述"></a>NameNode 的元数据存储概述</h4><p>一个典型的 NameNode 的元数据存储目录结构如图 3 所示 (图片来源于参考文献 [4])，这里主要关注其中的 EditLog 文件和 FSImage 文件：</p>
<p>图 3 .NameNode 的元数据存储目录结构<br><img src="https://img-blog.csdnimg.cn/20200209114111102.png" alt="在这里插入图片描述"></p>
<p>NameNode 在执行 HDFS 客户端提交的创建文件或者移动文件这样的写操作的时候，会首先把这些操作记录在 EditLog 文件之中，然后再更新内存中的文件系统镜像。内存中的文件系统镜像用于 NameNode 向客户端提供读服务，而 EditLog 仅仅只是在数据恢复的时候起作用。记录在 EditLog 之中的每一个操作又称为一个事务，每个事务有一个整数形式的事务 id 作为编号。EditLog 会被切割为很多段，每一段称为一个 Segment。正在写入的 EditLog Segment 处于 in-progress 状态，其文件名形如 edits_inprogress_${start_txid}，其中${start_txid} 表示这个 segment 的起始事务 id，例如上图中的 edits_inprogress_0000000000000000020。而已经写入完成的 EditLog Segment 处于 finalized 状态，其文件名形如 edits_${start_txid}-${end_txid}，其中${start_txid} 表示这个 segment 的起始事务 id，${end_txid} 表示这个 segment 的结束事务 id，例如上图中的 edits_0000000000000000001-0000000000000000019。</p>
<p>NameNode 会定期对内存中的文件系统镜像进行 checkpoint 操作，在磁盘上生成 FSImage 文件，FSImage 文件的文件名形如 fsimage_${end_txid}，其中${end_txid} 表示这个 fsimage 文件的结束事务 id，例如上图中的 fsimage_0000000000000000020。在 NameNode 启动的时候会进行数据恢复，首先把 FSImage 文件加载到内存中形成文件系统镜像，然后再把 EditLog 之中 FsImage 的结束事务 id 之后的 EditLog 回放到这个文件系统镜像上。</p>
<h4 id="基于-QJM-的共享存储系统的总体架构"><a href="#基于-QJM-的共享存储系统的总体架构" class="headerlink" title="基于 QJM 的共享存储系统的总体架构"></a>基于 QJM 的共享存储系统的总体架构</h4><p>基于 QJM 的共享存储系统主要用于保存 EditLog，并不保存 FSImage 文件。FSImage 文件还是在 NameNode 的本地磁盘上。QJM 共享存储的基本思想来自于 Paxos 算法 (参见参考文献 [3])，采用多个称为 JournalNode 的节点组成的 JournalNode 集群来存储 EditLog。每个 JournalNode 保存同样的 EditLog 副本。每次 NameNode 写 EditLog 的时候，除了向本地磁盘写入 EditLog 之外，也会并行地向 JournalNode 集群之中的每一个 JournalNode 发送写请求，只要大多数 (majority) 的 JournalNode 节点返回成功就认为向 JournalNode 集群写入 EditLog 成功。如果有 2N+1 台 JournalNode，那么根据大多数的原则，最多可以容忍有 N 台 JournalNode 节点挂掉。</p>
<p>基于 QJM 的共享存储系统的内部实现架构图如图 4 所示，主要包含下面几个主要的组件：</p>
<p>图 4 . 基于 QJM 的共享存储系统的内部实现架构图</p>
<p><img src="https://img-blog.csdnimg.cn/20200209114154714.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>FSEditLog：这个类封装了对 EditLog 的所有操作，是 NameNode 对 EditLog 的所有操作的入口。</p>
<p>JournalSet： 这个类封装了对本地磁盘和 JournalNode 集群上的 EditLog 的操作，内部包含了两类 JournalManager，一类为 FileJournalManager，用于实现对本地磁盘上 EditLog 的操作。一类为 QuorumJournalManager，用于实现对 JournalNode 集群上共享目录的 EditLog 的操作。FSEditLog 只会调用 JournalSet 的相关方法，而不会直接使用 FileJournalManager 和 QuorumJournalManager。</p>
<p>FileJournalManager：封装了对本地磁盘上的 EditLog 文件的操作，不仅 NameNode 在向本地磁盘上写入 EditLog 的时候使用 FileJournalManager，JournalNode 在向本地磁盘写入 EditLog 的时候也复用了 FileJournalManager 的代码和逻辑。</p>
<p>QuorumJournalManager：封装了对 JournalNode 集群上的 EditLog 的操作，它会根据 JournalNode 集群的 URI 创建负责与 JournalNode 集群通信的类 AsyncLoggerSet， QuorumJournalManager 通过 AsyncLoggerSet 来实现对 JournalNode 集群上的 EditLog 的写操作，对于读操作，QuorumJournalManager 则是通过 Http 接口从 JournalNode 上的 JournalNodeHttpServer 读取 EditLog 的数据。</p>
<p>AsyncLoggerSet：内部包含了与 JournalNode 集群进行通信的 AsyncLogger 列表，每一个 AsyncLogger 对应于一个 JournalNode 节点，另外 AsyncLoggerSet 也包含了用于等待大多数 JournalNode 返回结果的工具类方法给 QuorumJournalManager 使用。</p>
<p>AsyncLogger：具体的实现类是 IPCLoggerChannel，IPCLoggerChannel 在执行方法调用的时候，会把调用提交到一个单线程的线程池之中，由线程池线程来负责向对应的 JournalNode 的 JournalNodeRpcServer 发送 RPC 请求。</p>
<p>JournalNodeRpcServer：运行在 JournalNode 节点进程中的 RPC 服务，接收 NameNode 端的 AsyncLogger 的 RPC 请求。</p>
<p>JournalNodeHttpServer：运行在 JournalNode 节点进程中的 Http 服务，用于接收处于 Standby 状态的 NameNode 和其它 JournalNode 的同步 EditLog 文件流的请求。</p>
<p>下面对基于 QJM 的共享存储系统的两个关键性问题同步数据和恢复数据进行详细分析。</p>
<h4 id="基于-QJM-的共享存储系统的数据同步机制分析"><a href="#基于-QJM-的共享存储系统的数据同步机制分析" class="headerlink" title="基于 QJM 的共享存储系统的数据同步机制分析"></a>基于 QJM 的共享存储系统的数据同步机制分析</h4><p>Active NameNode 和 StandbyNameNode 使用 JouranlNode 集群来进行数据同步的过程如图 5 所示，Active NameNode 首先把 EditLog 提交到 JournalNode 集群，然后 Standby NameNode 再从 JournalNode 集群定时同步 EditLog：</p>
<p>图 5 . 基于 QJM 的共享存储的数据同步机制</p>
<p><img src="https://img-blog.csdnimg.cn/20200209114225733.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h6 id="Active-NameNode-提交-EditLog-到-JournalNode-集群"><a href="#Active-NameNode-提交-EditLog-到-JournalNode-集群" class="headerlink" title="Active NameNode 提交 EditLog 到 JournalNode 集群"></a>Active NameNode 提交 EditLog 到 JournalNode 集群</h6><p>当处于 Active 状态的 NameNode 调用 FSEditLog 类的 logSync 方法来提交 EditLog 的时候，会通过 JouranlSet 同时向本地磁盘目录和 JournalNode 集群上的共享存储目录写入 EditLog。写入 JournalNode 集群是通过并行调用每一个 JournalNode 的 QJournalProtocol RPC 接口的 journal 方法实现的，如果对大多数 JournalNode 的 journal 方法调用成功，那么就认为提交 EditLog 成功，否则 NameNode 就会认为这次提交 EditLog 失败。提交 EditLog 失败会导致 Active NameNode 关闭 JournalSet 之后退出进程，留待处于 Standby 状态的 NameNode 接管之后进行数据恢复。</p>
<p>从上面的叙述可以看出，Active NameNode 提交 EditLog 到 JournalNode 集群的过程实际上是同步阻塞的，但是并不需要所有的 JournalNode 都调用成功，只要大多数 JournalNode 调用成功就可以了。如果无法形成大多数，那么就认为提交 EditLog 失败，NameNode 停止服务退出进程。如果对应到分布式系统的 CAP 理论的话，虽然采用了 Paxos 的“大多数”思想对 C(consistency，一致性) 和 A(availability，可用性) 进行了折衷，但还是可以认为 NameNode 选择了 C 而放弃了 A，这也符合 NameNode 对数据一致性的要求。</p>
<h6 id="Standby-NameNode-从-JournalNode-集群同步-EditLog"><a href="#Standby-NameNode-从-JournalNode-集群同步-EditLog" class="headerlink" title="Standby NameNode 从 JournalNode 集群同步 EditLog"></a>Standby NameNode 从 JournalNode 集群同步 EditLog</h6><p>当 NameNode 进入 Standby 状态之后，会启动一个 EditLogTailer 线程。这个线程会定期调用 EditLogTailer 类的 doTailEdits 方法从 JournalNode 集群上同步 EditLog，然后把同步的 EditLog 回放到内存之中的文件系统镜像上 (并不会同时把 EditLog 写入到本地磁盘上)。</p>
<p>这里需要关注的是：从 JournalNode 集群上同步的 EditLog 都是处于 finalized 状态的 EditLog Segment。“NameNode 的元数据存储概述”一节说过 EditLog Segment 实际上有两种状态，处于 in-progress 状态的 Edit Log 当前正在被写入，被认为是处于不稳定的中间态，有可能会在后续的过程之中发生修改，比如被截断。Active NameNode 在完成一个 EditLog Segment 的写入之后，就会向 JournalNode 集群发送 finalizeLogSegment RPC 请求，将完成写入的 EditLog Segment finalized，然后开始下一个新的 EditLog Segment。一旦 finalizeLogSegment 方法在大多数的 JournalNode 上调用成功，表明这个 EditLog Segment 已经在大多数的 JournalNode 上达成一致。一个 EditLog Segment 处于 finalized 状态之后，可以保证它再也不会变化。</p>
<p>从上面描述的过程可以看出，虽然 Active NameNode 向 JournalNode 集群提交 EditLog 是同步的，但 Standby NameNode 采用的是定时从 JournalNode 集群上同步 EditLog 的方式，那么 Standby NameNode 内存中文件系统镜像有很大的可能是落后于 Active NameNode 的，所以 Standby NameNode 在转换为 Active NameNode 的时候需要把落后的 EditLog 补上来。</p>
<h4 id="基于-QJM-的共享存储系统的数据恢复机制分析"><a href="#基于-QJM-的共享存储系统的数据恢复机制分析" class="headerlink" title="基于 QJM 的共享存储系统的数据恢复机制分析"></a>基于 QJM 的共享存储系统的数据恢复机制分析</h4><p>处于 Standby 状态的 NameNode 转换为 Active 状态的时候，有可能上一个 Active NameNode 发生了异常退出，那么 JournalNode 集群中各个 JournalNode 上的 EditLog 就可能会处于不一致的状态，所以首先要做的事情就是让 JournalNode 集群中各个节点上的 EditLog 恢复为一致。另外如前所述，当前处于 Standby 状态的 NameNode 的内存中的文件系统镜像有很大的可能是落后于旧的 Active NameNode 的，所以在 JournalNode 集群中各个节点上的 EditLog 达成一致之后，接下来要做的事情就是从 JournalNode 集群上补齐落后的 EditLog。只有在这两步完成之后，当前新的 Active NameNode 才能安全地对外提供服务。</p>
<p>补齐落后的 EditLog 的过程复用了前面描述的 Standby NameNode 从 JournalNode 集群同步 EditLog 的逻辑和代码，最终调用 EditLogTailer 类的 doTailEdits 方法来完成 EditLog 的补齐。使 JournalNode 集群上的 EditLog 达成一致的过程是一致性算法 Paxos 的典型应用场景，QJM 对这部分的处理可以看做是 Single Instance Paxos(参见参考文献 [3]) 算法的一个实现，在达成一致的过程中，Active NameNode 和 JournalNode 集群之间的交互流程如图 6 所示，具体描述如下：</p>
<p>图 6.Active NameNode 和 JournalNode 集群的交互流程图</p>
<p><img src="https://img-blog.csdnimg.cn/20200209114323734.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>生成一个新的 Epoch</p>
<p>Epoch 是一个单调递增的整数，用来标识每一次 Active NameNode 的生命周期，每发生一次 NameNode 的主备切换，Epoch 就会加 1。这实际上是一种 fencing 机制，为什么需要 fencing 已经在前面“ActiveStandbyElector 实现分析”一节的“防止脑裂”部分进行了说明。产生新 Epoch 的流程与 Zookeeper 的 ZAB(Zookeeper Atomic Broadcast) 协议在进行数据恢复之前产生新 Epoch 的过程完全类似：</p>
<ol>
<li><p>Active NameNode 首先向 JournalNode 集群发送 getJournalState RPC 请求，每个 JournalNode 会返回自己保存的最近的那个 Epoch(代码中叫 lastPromisedEpoch)。</p>
</li>
<li><p>NameNode 收到大多数的 JournalNode 返回的 Epoch 之后，在其中选择最大的一个加 1 作为当前的新 Epoch，然后向各个 JournalNode 发送 newEpoch RPC 请求，把这个新的 Epoch 发给各个 JournalNode。</p>
</li>
<li><p>每一个 JournalNode 在收到新的 Epoch 之后，首先检查这个新的 Epoch 是否比它本地保存的 lastPromisedEpoch 大，如果大的话就把 lastPromisedEpoch 更新为这个新的 Epoch，并且向 NameNode 返回它自己的本地磁盘上最新的一个 EditLogSegment 的起始事务 id，为后面的数据恢复过程做好准备。如果小于或等于的话就向 NameNode 返回错误。</p>
</li>
<li><p>NameNode 收到大多数 JournalNode 对 newEpoch 的成功响应之后，就会认为生成新的 Epoch 成功。</p>
</li>
</ol>
<p>在生成新的 Epoch 之后，每次 NameNode 在向 JournalNode 集群提交 EditLog 的时候，都会把这个 Epoch 作为参数传递过去。每个 JournalNode 会比较传过来的 Epoch 和它自己保存的 lastPromisedEpoch 的大小，如果传过来的 epoch 的值比它自己保存的 lastPromisedEpoch 小的话，那么这次写相关操作会被拒绝。一旦大多数 JournalNode 都拒绝了这次写操作，那么这次写操作就失败了。如果原来的 Active NameNode 恢复正常之后再向 JournalNode 写 EditLog，那么因为它的 Epoch 肯定比新生成的 Epoch 小，并且大多数的 JournalNode 都接受了这个新生成的 Epoch，所以拒绝写入的 JournalNode 数目至少是大多数，这样原来的 Active NameNode 写 EditLog 就肯定会失败，失败之后这个 NameNode 进程会直接退出，这样就实现了对原来的 Active NameNode 的隔离了。</p>
<h6 id="选择需要数据恢复的-EditLog-Segment-的-id"><a href="#选择需要数据恢复的-EditLog-Segment-的-id" class="headerlink" title="选择需要数据恢复的 EditLog Segment 的 id"></a>选择需要数据恢复的 EditLog Segment 的 id</h6><p>需要恢复的 Edit Log 只可能是各个 JournalNode 上的最后一个 Edit Log Segment，如前所述，JournalNode 在处理完 newEpoch RPC 请求之后，会向 NameNode 返回它自己的本地磁盘上最新的一个 EditLog Segment 的起始事务 id，这个起始事务 id 实际上也作为这个 EditLog Segment 的 id。NameNode 会在所有这些 id 之中选择一个最大的 id 作为要进行数据恢复的 EditLog Segment 的 id。</p>
<h6 id="向-JournalNode-集群发送-prepareRecovery-RPC-请求"><a href="#向-JournalNode-集群发送-prepareRecovery-RPC-请求" class="headerlink" title="向 JournalNode 集群发送 prepareRecovery RPC 请求"></a>向 JournalNode 集群发送 prepareRecovery RPC 请求</h6><p>NameNode 接下来向 JournalNode 集群发送 prepareRecovery RPC 请求，请求的参数就是选出的 EditLog Segment 的 id。JournalNode 收到请求后返回本地磁盘上这个 Segment 的起始事务 id、结束事务 id 和状态 (in-progress 或 finalized)。</p>
<p>这一步对应于 Paxos 算法的 Phase 1a 和 Phase 1b(参见参考文献 [3]) 两步。Paxos 算法的 Phase1 是 prepare 阶段，这也与方法名 prepareRecovery 相对应。并且这里以前面产生的新的 Epoch 作为 Paxos 算法中的提案编号 (proposal number)。只要大多数的 JournalNode 的 prepareRecovery RPC 调用成功返回，NameNode 就认为成功。</p>
<p>选择进行同步的基准数据源，向 JournalNode 集群发送 acceptRecovery RPC 请求 NameNode 根据 prepareRecovery 的返回结果，选择一个 JournalNode 上的 EditLog Segment 作为同步的基准数据源。选择基准数据源的原则大致是：在 in-progress 状态和 finalized 状态的 Segment 之间优先选择 finalized 状态的 Segment。如果都是 in-progress 状态的话，那么优先选择 Epoch 比较高的 Segment(也就是优先选择更新的)，如果 Epoch 也一样，那么优先选择包含的事务数更多的 Segment。</p>
<p>在选定了同步的基准数据源之后，NameNode 向 JournalNode 集群发送 acceptRecovery RPC 请求，将选定的基准数据源作为参数。JournalNode 接收到 acceptRecovery RPC 请求之后，从基准数据源 JournalNode 的 JournalNodeHttpServer 上下载 EditLog Segment，将本地的 EditLog Segment 替换为下载的 EditLog Segment。</p>
<p>这一步对应于 Paxos 算法的 Phase 2a 和 Phase 2b(参见参考文献 [3]) 两步。Paxos 算法的 Phase2 是 accept 阶段，这也与方法名 acceptRecovery 相对应。只要大多数 JournalNode 的 acceptRecovery RPC 调用成功返回，NameNode 就认为成功。</p>
<h6 id="向-JournalNode-集群发送-finalizeLogSegment-RPC-请求，数据恢复完成"><a href="#向-JournalNode-集群发送-finalizeLogSegment-RPC-请求，数据恢复完成" class="headerlink" title="向 JournalNode 集群发送 finalizeLogSegment RPC 请求，数据恢复完成"></a>向 JournalNode 集群发送 finalizeLogSegment RPC 请求，数据恢复完成</h6><p>上一步执行完成之后，NameNode 确认大多数 JournalNode 上的 EditLog Segment 已经从基准数据源进行了同步。接下来，NameNode 向 JournalNode 集群发送 finalizeLogSegment RPC 请求，JournalNode 接收到请求之后，将对应的 EditLog Segment 从 in-progress 状态转换为 finalized 状态，实际上就是将文件名从 edits_inprogress_${startTxid} 重命名为 edits_${startTxid}-${endTxid}，见“NameNode 的元数据存储概述”一节的描述。</p>
<p>只要大多数 JournalNode 的 finalizeLogSegment RPC 调用成功返回，NameNode 就认为成功。此时可以保证 JournalNode 集群的大多数节点上的 EditLog 已经处于一致的状态，这样 NameNode 才能安全地从 JournalNode 集群上补齐落后的 EditLog 数据。</p>
<p>需要注意的是，尽管基于 QJM 的共享存储方案看起来理论完备，设计精巧，但是仍然无法保证数据的绝对强一致，下面选取参考文献 [2] 中的一个例子来说明：</p>
<p>假设有 3 个 JournalNode：JN1、JN2 和 JN3，Active NameNode 发送了事务 id 为 151、152 和 153 的 3 个事务到 JournalNode 集群，这 3 个事务成功地写入了 JN2，但是在还没能写入 JN1 和 JN3 之前，Active NameNode 就宕机了。同时，JN3 在整个写入的过程中延迟较大，落后于 JN1 和 JN2。最终成功写入 JN1 的事务 id 为 150，成功写入 JN2 的事务 id 为 153，而写入到 JN3 的事务 id 仅为 125，如图 7 所示 (图片来源于参考文献 [2])。按照前面描述的只有成功地写入了大多数的 JournalNode 才认为写入成功的原则，显然事务 id 为 151、152 和 153 的这 3 个事务只能算作写入失败。在进行数据恢复的过程中，会发生下面两种情况：</p>
<p>图 7.JournalNode 集群写入的事务 id 情况</p>
<p><img src="https://img-blog.csdnimg.cn/20200209114457736.png" alt="在这里插入图片描述"></p>
<p>如果随后的 Active NameNode 进行数据恢复时在 prepareRecovery 阶段收到了 JN2 的回复，那么肯定会以 JN2 对应的 EditLog Segment 为基准来进行数据恢复，这样最后在多数 JournalNode 上的 EditLog Segment 会恢复到事务 153。从恢复的结果来看，实际上可以认为前面宕机的 Active NameNode 对事务 id 为 151、152 和 153 的这 3 个事务的写入成功了。但是如果从 NameNode 自身的角度来看，这显然就发生了数据不一致的情况。<br>如果随后的 Active NameNode 进行数据恢复时在 prepareRecovery 阶段没有收到 JN2 的回复，那么肯定会以 JN1 对应的 EditLog Segment 为基准来进行数据恢复，这样最后在多数 JournalNode 上的 EditLog Segment 会恢复到事务 150。在这种情况下，如果从 NameNode 自身的角度来看的话，数据就是一致的了。</p>
<p>事实上不光本文描述的基于 QJM 的共享存储方案无法保证数据的绝对一致，大家通常认为的一致性程度非常高的 Zookeeper 也会发生类似的情况，这也从侧面说明了要实现一个数据绝对一致的分布式存储系统的确非常困难。</p>
<h4 id="NameNode-在进行状态转换时对共享存储的处理"><a href="#NameNode-在进行状态转换时对共享存储的处理" class="headerlink" title="NameNode 在进行状态转换时对共享存储的处理"></a>NameNode 在进行状态转换时对共享存储的处理</h4><p>下面对 NameNode 在进行状态转换的过程中对共享存储的处理进行描述，使得大家对基于 QJM 的共享存储方案有一个完整的了解，同时也作为本部分的总结。</p>
<h6 id="NameNode-初始化启动，进入-Standby-状态"><a href="#NameNode-初始化启动，进入-Standby-状态" class="headerlink" title="NameNode 初始化启动，进入 Standby 状态"></a>NameNode 初始化启动，进入 Standby 状态</h6><p>在 NameNode 以 HA 模式启动的时候，NameNode 会认为自己处于 Standby 模式，在 NameNode 的构造函数中会加载 FSImage 文件和 EditLog Segment 文件来恢复自己的内存文件系统镜像。在加载 EditLog Segment 的时候，调用 FSEditLog 类的 initSharedJournalsForRead 方法来创建只包含了在 JournalNode 集群上的共享目录的 JournalSet，也就是说，这个时候只会从 JournalNode 集群之中加载 EditLog，而不会加载本地磁盘上的 EditLog。另外值得注意的是，加载的 EditLog Segment 只是处于 finalized 状态的 EditLog Segment，而处于 in-progress 状态的 Segment 需要后续在切换为 Active 状态的时候，进行一次数据恢复过程，将 in-progress 状态的 Segment 转换为 finalized 状态的 Segment 之后再进行读取。</p>
<p>加载完 FSImage 文件和共享目录上的 EditLog Segment 文件之后，NameNode 会启动 EditLogTailer 线程和 StandbyCheckpointer 线程，正式进入 Standby 模式。如前所述，EditLogTailer 线程的作用是定时从 JournalNode 集群上同步 EditLog。而 StandbyCheckpointer 线程的作用其实是为了替代 Hadoop 1.x 版本之中的 Secondary NameNode 的功能，StandbyCheckpointer 线程会在 Standby NameNode 节点上定期进行 Checkpoint，将 Checkpoint 之后的 FSImage 文件上传到 Active NameNode 节点。</p>
<h6 id="NameNode-从-Standby-状态切换为-Active-状态"><a href="#NameNode-从-Standby-状态切换为-Active-状态" class="headerlink" title="NameNode 从 Standby 状态切换为 Active 状态"></a>NameNode 从 Standby 状态切换为 Active 状态</h6><p>当 NameNode 从 Standby 状态切换为 Active 状态的时候，首先需要做的就是停止它在 Standby 状态的时候启动的线程和相关的服务，包括上面提到的 EditLogTailer 线程和 StandbyCheckpointer 线程，然后关闭用于读取 JournalNode 集群的共享目录上的 EditLog 的 JournalSet，接下来会调用 FSEditLog 的 initJournalSetForWrite 方法重新打开 JournalSet。不同的是，这个 JournalSet 内部同时包含了本地磁盘目录和 JournalNode 集群上的共享目录。这些工作完成之后，就开始执行“基于 QJM 的共享存储系统的数据恢复机制分析”一节所描述的流程，调用 FSEditLog 类的 recoverUnclosedStreams 方法让 JournalNode 集群中各个节点上的 EditLog 达成一致。然后调用 EditLogTailer 类的 catchupDuringFailover 方法从 JournalNode 集群上补齐落后的 EditLog。最后打开一个新的 EditLog Segment 用于新写入数据，同时启动 Active NameNode 所需要的线程和服务。</p>
<h6 id="NameNode-从-Active-状态切换为-Standby-状态"><a href="#NameNode-从-Active-状态切换为-Standby-状态" class="headerlink" title="NameNode 从 Active 状态切换为 Standby 状态"></a>NameNode 从 Active 状态切换为 Standby 状态</h6><p>当 NameNode 从 Active 状态切换为 Standby 状态的时候，首先需要做的就是停止它在 Active 状态的时候启动的线程和服务，然后关闭用于读取本地磁盘目录和 JournalNode 集群上的共享目录的 EditLog 的 JournalSet。接下来会调用 FSEditLog 的 initSharedJournalsForRead 方法重新打开用于读取 JournalNode 集群上的共享目录的 JournalSet。这些工作完成之后，就会启动 EditLogTailer 线程和 StandbyCheckpointer 线程，EditLogTailer 线程会定时从 JournalNode 集群上同步 Edit Log。</p>
<h1 id="NameNode-高可用运维中的注意事项"><a href="#NameNode-高可用运维中的注意事项" class="headerlink" title="NameNode 高可用运维中的注意事项"></a>NameNode 高可用运维中的注意事项</h1><p>本节结合笔者的实践，从初始化部署和日常运维两个方面介绍一些在 NameNode 高可用运维中的注意事项。</p>
<h4 id="初始化部署"><a href="#初始化部署" class="headerlink" title="初始化部署"></a>初始化部署</h4><p>如果在开始部署 Hadoop 集群的时候就启用 NameNode 的高可用的话，那么相对会比较容易。但是如果在采用传统的单 NameNode 的架构运行了一段时间之后，升级为 NameNode 的高可用架构的话，就要特别注意在升级的时候需要按照以下的步骤进行操作：</p>
<ol>
<li>对 Zookeeper 进行初始化，创建 Zookeeper 上的/hadoop-ha/${dfs.nameservices} 节点。创建节点是为随后通过 Zookeeper 进行主备选举做好准备，在进行主备选举的时候会在这个节点下面创建子节点 (具体可参照“ActiveStandbyElector 实现分析”一节的叙述)。这一步通过在原有的 NameNode 上执行命令 hdfs zkfc -formatZK 来完成。</li>
<li>启动所有的 JournalNode，这通过脚本命令 hadoop-daemon.sh start journalnode 来完成。</li>
<li>对 JouranlNode 集群的共享存储目录进行格式化，并且将原有的 NameNode 本地磁盘上最近一次 checkpoint 操作生成 FSImage 文件 (具体可参照“NameNode 的元数据存储概述”一节的叙述) 之后的 EditLog 拷贝到 JournalNode 集群上的共享目录之中，这通过在原有的 NameNode 上执行命令 hdfs namenode -initializeSharedEdits 来完成。</li>
<li>启动原有的 NameNode 节点，这通过脚本命令 hadoop-daemon.sh start namenode 完成。</li>
<li>对新增的 NameNode 节点进行初始化，将原有的 NameNode 本地磁盘上最近一次 checkpoint 操作生成 FSImage 文件拷贝到这个新增的 NameNode 的本地磁盘上，同时需要验证 JournalNode 集群的共享存储目录上已经具有了这个 FSImage 文件之后的 EditLog(已经在第 3 步完成了)。这一步通过在新增的 NameNode 上执行命令 hdfs namenode -bootstrapStandby 来完成。</li>
<li>启动新增的 NameNode 节点，这通过脚本命令 hadoop-daemon.sh start namenode 完成。</li>
<li>在这两个 NameNode 上启动 zkfc(ZKFailoverController) 进程，谁通过 Zookeeper 选主成功，谁就是主 NameNode，另一个为备 NameNode。这通过脚本命令 hadoop-daemon.sh start zkfc 完成。</li>
</ol>
<h4 id="日常维护"><a href="#日常维护" class="headerlink" title="日常维护"></a>日常维护</h4><p>笔者在日常的维护之中主要遇到过下面两种问题：</p>
<p>Zookeeper 过于敏感：Hadoop 的配置项中 Zookeeper 的 session timeout 的配置参数 ha.zookeeper.session-timeout.ms 的默认值为 5000，也就是 5s，这个值比较小，会导致 Zookeeper 比较敏感，可以把这个值尽量设置得大一些，避免因为网络抖动等原因引起 NameNode 进行无谓的主备切换。</p>
<p>单台 JouranlNode 故障时会导致主备无法切换：在理论上，如果有 3 台或者更多的 JournalNode，那么挂掉一台 JouranlNode 应该仍然可以进行正常的主备切换。但是笔者在某次 NameNode 重启的时候，正好赶上一台 JournalNode 挂掉宕机了，这个时候虽然某一台 NameNode 通过 Zookeeper 选主成功，但是这台被选为主的 NameNode 无法成功地从 Standby 状态切换为 Active 状态。事后追查原因发现，被选为主的 NameNode 卡在退出 Standby 状态的最后一步，这个时候它需要等待到 JournalNode 的请求全部完成之后才能退出。但是由于有一台 JouranlNode 宕机，到这台 JournalNode 的请求都积压在一起并且在不断地进行重试，同时在 Hadoop 的配置项中重试次数的默认值非常大，所以就会导致被选为主的 NameNode 无法及时退出 Standby 状态。这个问题主要是 Hadoop 内部的 RPC 通信框架的设计缺陷引起的，Hadoop HA 的源代码 IPCLoggerChannel 类中有关于这个问题的 TODO，但是截止到社区发布的 2.7.1 版本这个问题仍然存在。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiangzhiqi4551.github.io/2021/06/08/JVM%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84%E5%8F%8AJava%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%EF%BC%88GC%E7%AE%97%E6%B3%95%E3%80%81GC%E6%94%B6%E9%9B%86%E5%99%A8%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo1.jpeg">
      <meta itemprop="name" content="Master Jiang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiang's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/08/JVM%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84%E5%8F%8AJava%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%EF%BC%88GC%E7%AE%97%E6%B3%95%E3%80%81GC%E6%94%B6%E9%9B%86%E5%99%A8%EF%BC%89/" class="post-title-link" itemprop="url">JVM内存结构及Java垃圾收集（GC算法、GC收集器）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-08 18:22:24" itemprop="dateCreated datePublished" datetime="2021-06-08T18:22:24+08:00">2021-06-08</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-10-10 18:16:20" itemprop="dateModified" datetime="2021-10-10T18:16:20+08:00">2021-10-10</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Java/" itemprop="url" rel="index"><span itemprop="name">Java</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>最近正在复习Java相关的知识，总结一下JVM相关的知识点。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/06/08/JVM%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84%E5%8F%8AJava%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%EF%BC%88GC%E7%AE%97%E6%B3%95%E3%80%81GC%E6%94%B6%E9%9B%86%E5%99%A8%EF%BC%89/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiangzhiqi4551.github.io/2021/06/08/Spark%E7%AE%97%E5%AD%90%E4%B9%8BfoldByKey/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo1.jpeg">
      <meta itemprop="name" content="Master Jiang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiang's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/08/Spark%E7%AE%97%E5%AD%90%E4%B9%8BfoldByKey/" class="post-title-link" itemprop="url">Spark算子之foldByKey</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-08 18:19:25" itemprop="dateCreated datePublished" datetime="2021-06-08T18:19:25+08:00">2021-06-08</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-10-10 18:16:20" itemprop="dateModified" datetime="2021-10-10T18:16:20+08:00">2021-10-10</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <span id="more"></span>

<p>在学习foldByKey这个算子的时候，发现网上好多文章的内容相互冲突，于是决定自己实践一边，以理解这个算子是怎么运行的。</p>
<h2 id="foldByKey"><a href="#foldByKey" class="headerlink" title="foldByKey"></a>foldByKey</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">foldByKey</span><span class="params">(zeroValue: V, numPartitions: Int)</span><span class="params">(func: (V, V)</span> </span>=&gt; V): RDD[(K, V)] = self.withScope &#123;</span><br><span class="line">    foldByKey(zeroValue, <span class="keyword">new</span> HashPartitioner(numPartitions))(func)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">foldByKey</span><span class="params">(zeroValue: V)</span><span class="params">(func: (V, V)</span> </span>=&gt; V): RDD[(K, V)] = self.withScope &#123;</span><br><span class="line">    foldByKey(zeroValue, defaultPartitioner(self))(func)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>这是源码中对foldByKey的定义，主要是zeroValue这个点容易造成误解：</p>
<p>我们先初始化一个RDD</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd = sc.makeRDD(Array((<span class="string">&quot;A&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;A&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;B&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;B&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;C&quot;</span>,<span class="number">1</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[<span class="number">6</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br></pre></td></tr></table></figure>
<p>查看这个RDD的分区及分区中的元素</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd.mapPartitionsWithIndex((index, iter) =&gt; &#123;</span><br><span class="line">     |       <span class="keyword">var</span> rddmap = scala.collection.mutable.Map[String, List[(String, Int)]]()</span><br><span class="line">     |       <span class="keyword">while</span> (iter.hasNext) &#123;</span><br><span class="line">     |         <span class="keyword">var</span> elem = iter.next()</span><br><span class="line">     |         <span class="keyword">var</span> partNum = index + <span class="string">&quot;_&quot;</span></span><br><span class="line">     |         <span class="keyword">if</span> (rddmap.contains(partNum)) &#123;</span><br><span class="line">     |           <span class="keyword">var</span> elems = rddmap(partNum)</span><br><span class="line">     |           elems ::= elem</span><br><span class="line">     |           rddmap(partNum) = elems</span><br><span class="line">     |         &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     |           <span class="keyword">var</span> newlist = List[(String,Int)]()</span><br><span class="line">     |           newlist ::= elem</span><br><span class="line">     |           rddmap(partNum) = newlist</span><br><span class="line">     |         &#125;</span><br><span class="line">     |       &#125;</span><br><span class="line">     |       rddmap.toIterator</span><br><span class="line">     |     &#125;).collect()</span><br><span class="line">res11: Array[(String, List[(String, Int)])] = Array((0_,List((A,<span class="number">2</span>), (A,<span class="number">1</span>))), (1_,List((C,<span class="number">1</span>), (B,<span class="number">2</span>), (B,<span class="number">1</span>))))</span><br></pre></td></tr></table></figure>
<p>可以看到，目前是有两个分区：<br>第一个分区中包含(A,2), (A,1)；<br>第二个分区中包含(C,1), (B,2), (B,1)</p>
<p>这时候我们用foldByKey算子，zeroValue设为2</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  rdd.foldByKey(<span class="number">2</span>) &#123; (x1, x2) =&gt; x1 + x2 &#125;.collect()</span><br><span class="line">res12: Array[(String, Int)] = Array((B,<span class="number">5</span>), (A,<span class="number">5</span>), (C,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<h4 id="再来看看另一种情况"><a href="#再来看看另一种情况" class="headerlink" title="再来看看另一种情况"></a>再来看看另一种情况</h4><p>在初始化RDD的过程中，将分区数设为3</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd = sc.makeRDD(Array((<span class="string">&quot;A&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;A&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;B&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;B&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;C&quot;</span>, <span class="number">1</span>)),<span class="number">3</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[<span class="number">11</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br></pre></td></tr></table></figure>
<p>再查看下每个分区中的元素</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd.mapPartitionsWithIndex((index, iter) =&gt; &#123;</span><br><span class="line">     |       <span class="keyword">var</span> rddmap = scala.collection.mutable.Map[String, List[(String, Int)]]()</span><br><span class="line">     |       <span class="keyword">while</span> (iter.hasNext) &#123;</span><br><span class="line">     |         <span class="keyword">var</span> elem = iter.next()</span><br><span class="line">     |         <span class="keyword">var</span> partNum = index + <span class="string">&quot;_&quot;</span></span><br><span class="line">     |         <span class="keyword">if</span> (rddmap.contains(partNum)) &#123;</span><br><span class="line">     |           <span class="keyword">var</span> elems = rddmap(partNum)</span><br><span class="line">     |           elems ::= elem</span><br><span class="line">     |           rddmap(partNum) = elems</span><br><span class="line">     |         &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     |           <span class="keyword">var</span> newlist = List[(String,Int)]()</span><br><span class="line">     |           newlist ::= elem</span><br><span class="line">     |           rddmap(partNum) = newlist</span><br><span class="line">     |         &#125;</span><br><span class="line">     |       &#125;</span><br><span class="line">     |       rddmap.toIterator</span><br><span class="line">     |     &#125;).collect()</span><br><span class="line">res13: Array[(String, List[(String, Int)])] = Array((0_,List((A,<span class="number">1</span>))), (1_,List((B,<span class="number">1</span>), (A,<span class="number">2</span>))), (2_,List((C,<span class="number">1</span>), (B,<span class="number">2</span>))))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>第一个分区中只有 （A，1）<br>第二个分区中有 （B，1）（A，2）<br>第三个分区中有 （C，1）（B，2）<br>这时我们再使用foldByKey算子，zeroValue还是设为2</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd.foldByKey(<span class="number">2</span>) &#123; (x1, x2) =&gt; x1 + x2 &#125;.collect()</span><br><span class="line">res14: Array[(String, Int)] = Array((B,<span class="number">7</span>), (C,<span class="number">3</span>), (A,<span class="number">7</span>))</span><br></pre></td></tr></table></figure>


<h6 id="出现了和第一次不一样的结果"><a href="#出现了和第一次不一样的结果" class="headerlink" title="出现了和第一次不一样的结果"></a>出现了和第一次不一样的结果</h6><p>由此我们已经可以推断出foldByKey的机制了，我的理解是：<br>取RDD的每一个分片，在每一个分片中，先根据你定义的映射，用zeroValue对不同key对应的value做<font color='yellow'>一次</font>初始化，再对剩下的value值做映射。</p>
<p>在第一次操作中，第一个分区中包含(A,2), (A,1)，先做初始化 2+2 = 4 ，再对剩下的值做累加， 最后得到 （A，4+1 = 5）</p>
<p>在第二次操作中，第一个分区中只有 （A，1），初始化为（A，2+1 = 3），第二个分区中有 （B，1）（A，2），因为是不同分区，所以也对A进行初始化（A，2+2 = 4），最后两个分区的结果统计到一起就是（A，4+3 = 7），BC同理。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiangzhiqi4551.github.io/2021/06/08/Hive%E4%B8%AD%E7%9A%84Skew-Join/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo1.jpeg">
      <meta itemprop="name" content="Master Jiang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiang's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/08/Hive%E4%B8%AD%E7%9A%84Skew-Join/" class="post-title-link" itemprop="url">Hive中的Skew Join</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-08 18:16:45" itemprop="dateCreated datePublished" datetime="2021-06-08T18:16:45+08:00">2021-06-08</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-10-10 18:16:20" itemprop="dateModified" datetime="2021-10-10T18:16:20+08:00">2021-10-10</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Hive/" itemprop="url" rel="index"><span itemprop="name">Hive</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>看文档的时候突然发现Skew Join，之前只知道有内外连接，半开连接，全外连接，笛卡尔积，于是赶紧学习了下Skew Join，在这里做个总结。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/06/08/Hive%E4%B8%AD%E7%9A%84Skew-Join/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiangzhiqi4551.github.io/2021/06/08/%E8%AF%A6%E8%A7%A3TCP-IP%E5%8D%8F%E8%AE%AE%E6%A0%88/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo1.jpeg">
      <meta itemprop="name" content="Master Jiang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiang's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/08/%E8%AF%A6%E8%A7%A3TCP-IP%E5%8D%8F%E8%AE%AE%E6%A0%88/" class="post-title-link" itemprop="url">详解TCP/IP协议栈</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-08 17:21:05" itemprop="dateCreated datePublished" datetime="2021-06-08T17:21:05+08:00">2021-06-08</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-10-10 18:16:20" itemprop="dateModified" datetime="2021-10-10T18:16:20+08:00">2021-10-10</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index"><span itemprop="name">网络</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>之前对网络各层作用的了解一直都比较模糊，对各个协议的作用也不甚清楚，最近看到了一篇对TCP/IP协议栈讲解比较清晰的博文，特地转载过来。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/06/08/%E8%AF%A6%E8%A7%A3TCP-IP%E5%8D%8F%E8%AE%AE%E6%A0%88/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiangzhiqi4551.github.io/2021/06/08/git-commit-%E6%97%B6%E6%8F%90%E7%A4%BAplease-tell-me-who-you-are%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo1.jpeg">
      <meta itemprop="name" content="Master Jiang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiang's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/08/git-commit-%E6%97%B6%E6%8F%90%E7%A4%BAplease-tell-me-who-you-are%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/" class="post-title-link" itemprop="url">git commit 时提示please tell me who you are解决方法</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-08 17:15:25" itemprop="dateCreated datePublished" datetime="2021-06-08T17:15:25+08:00">2021-06-08</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-10-10 18:16:20" itemprop="dateModified" datetime="2021-10-10T18:16:20+08:00">2021-10-10</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/" itemprop="url" rel="index"><span itemprop="name">常见问题</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <span id="more"></span>

<p>先是按照git的提示</p>
<p>git config –global user.email “<a href="mailto:&#x79;&#111;&#x75;&#64;&#101;&#120;&#x61;&#109;&#112;&#x6c;&#101;&#46;&#x63;&#111;&#x6d;">&#x79;&#111;&#x75;&#64;&#101;&#120;&#x61;&#109;&#112;&#x6c;&#101;&#46;&#x63;&#111;&#x6d;</a>“<br>git config –global user.name “Your Name”</p>
<p>进行设置，结果发现还是报错，最后找到解决方案如下：</p>
<p>git init<br>git config user.name “Your Name”<br>git config user.email “<a href="mailto:&#121;&#111;&#x75;&#64;&#101;&#120;&#x61;&#109;&#112;&#x6c;&#x65;&#46;&#99;&#111;&#x6d;">&#121;&#111;&#x75;&#64;&#101;&#120;&#x61;&#109;&#112;&#x6c;&#x65;&#46;&#99;&#111;&#x6d;</a>“<br>git add *<br>git commit -m “commit message”</p>
<p>按如上步骤操作，最后成功commit</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://jiangzhiqi4551.github.io/2021/06/08/Hadoop-archive%E5%BD%92%E6%A1%A3%E5%91%BD%E4%BB%A4%E7%9A%84%E4%BD%BF%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo1.jpeg">
      <meta itemprop="name" content="Master Jiang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiang's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/08/Hadoop-archive%E5%BD%92%E6%A1%A3%E5%91%BD%E4%BB%A4%E7%9A%84%E4%BD%BF%E7%94%A8/" class="post-title-link" itemprop="url">Hadoop archive归档命令的使用</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-08 16:58:03" itemprop="dateCreated datePublished" datetime="2021-06-08T16:58:03+08:00">2021-06-08</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-10-10 18:16:20" itemprop="dateModified" datetime="2021-10-10T18:16:20+08:00">2021-10-10</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <span id="more"></span>

<h2 id="archive-命令有什么用"><a href="#archive-命令有什么用" class="headerlink" title="archive 命令有什么用"></a>archive 命令有什么用</h2><p>archive 可以用来解决 Hadoop 中的<em><strong>小文件</strong></em>问题，当存在大量小文件时，会产生如下影响：</p>
<ol>
<li>HDFS 中，小文件过多会占用大量内存，NameNode 内存容量最终会成为限制集群扩展的瓶颈。</li>
<li>HDFS 读写小文件更加耗时，因为每次都需要从 NameNode 获取元信息，并与对应的 DataNode 建立连接。</li>
<li>小文件过多会开很多 map，一个 map 启动一个 JVM 去执行，所以这些任务的初始化，启动，执行会浪费大量的资源，严重影响性能。</li>
</ol>
<h2 id="如何使用-archive-进行归档"><a href="#如何使用-archive-进行归档" class="headerlink" title="如何使用 archive 进行归档"></a>如何使用 archive 进行归档</h2><p><img src="https://img-blog.csdnimg.cn/20200428164050783.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>其实已经用法已经很明白了，name指定名字，将路径上的内容创建到 archive 文件中。</p>
<h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p>首先我们创建一个小文件 text.txt ，上传四份到 HDFS 上</p>
<p><img src="https://img-blog.csdnimg.cn/20200428164901114.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>用官方自带的 wordcount 跑一下：</p>
<p><img src="https://img-blog.csdnimg.cn/20200428165351971.png" alt="在这里插入图片描述"></p>
<p>很明显，4个输入，接下来执行归档命令，将 small 路径下的所有文件归档到 archiveResult 中，可以看到 archive 启动了一个 MapReduce：</p>
<p><img src="https://img-blog.csdnimg.cn/20200428165806873.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>得到：</p>
<p><img src="https://img-blog.csdnimg.cn/20200428170355564.png" alt="在这里插入图片描述"></p>
<p>但是进去之后会发现里面并不是我们之前的几个 text.txt 文件：</p>
<p><img src="https://img-blog.csdnimg.cn/20200428170538921.png" alt="在这里插入图片描述"></p>
<p>使用带 har 的命令即可解决：</p>
<p><img src="https://img-blog.csdnimg.cn/20200428170942383.png" alt="在这里插入图片描述"></p>
<p>使用归档之后的文件进行 wordcount：</p>
<p><img src="https://img-blog.csdnimg.cn/20200428171245198.png" alt="在这里插入图片描述"></p>
<p>最后仍然可以得到正确的结果，但是需要注意的是：<br><strong>使用 archive 归档后，减轻的是 HDFS 的压力，对于 MapReduce 来说，输入还是四个文件。</strong></p>
<p>如果想要控制 map 数，Hive 可以通过几个参数来实现：</p>
<ol>
<li>set mapred.max.split.size = **;（决定每个map处理的最大的文件大小）</li>
<li>set mapred.min.split.size.per.node = **;（一个节点上 split 的至少的大小，这个值决定了多个DataNode 上的文件是否需要合并）</li>
<li>set mapred.min.split.size.per.rack = **;（一个交换机下 split 的至少的大小，这个值决定了多个交换机上的文件是否需要合并）</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Master Jiang</span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
