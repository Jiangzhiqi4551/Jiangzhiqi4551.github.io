<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;example.com&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Pisces&quot;,&quot;version&quot;:&quot;8.5.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:false,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;搜索...&quot;,&quot;empty&quot;:&quot;没有找到任何搜索结果：${query}&quot;,&quot;hits_time&quot;:&quot;找到 ${hits} 个搜索结果（用时 ${time} 毫秒）&quot;,&quot;hits&quot;:&quot;找到 ${hits} 个搜索结果&quot;}}</script><script src="/js/config.js"></script>
<meta property="og:type" content="website">
<meta property="og:title" content="Jiang&#39;s blog">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Jiang&#39;s blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Master Jiang">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:true,&quot;isPost&quot;:false,&quot;lang&quot;:&quot;zh-CN&quot;,&quot;comments&quot;:&quot;&quot;,&quot;permalink&quot;:&quot;&quot;,&quot;path&quot;:&quot;index.html&quot;,&quot;title&quot;:&quot;&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>Jiang's blog</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Jiang's blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Master Jiang"
      src="/images/logo1.jpeg">
  <p class="site-author-name" itemprop="name">Master Jiang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">22</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Jiangzhiqi4551" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Jiangzhiqi4551" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:Jiangzhiqi4551@outlook.com" title="E-Mail → mailto:Jiangzhiqi4551@outlook.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/Jiangzhiqi4551?spm=1000.2115.3001.5343" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;Jiangzhiqi4551?spm&#x3D;1000.2115.3001.5343" rel="noopener" target="_blank"><i class="fas fa-blog fa-fw"></i>CSDN</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/08/%E5%9F%BA%E4%BA%8EGeohash%E5%AE%9E%E7%8E%B0%E6%A0%B9%E6%8D%AE%E7%BB%8F%E7%BA%AC%E5%BA%A6%E7%9A%84%E5%BF%AB%E9%80%9F%E5%AE%9A%E4%BD%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo1.jpeg">
      <meta itemprop="name" content="Master Jiang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiang's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/08/%E5%9F%BA%E4%BA%8EGeohash%E5%AE%9E%E7%8E%B0%E6%A0%B9%E6%8D%AE%E7%BB%8F%E7%BA%AC%E5%BA%A6%E7%9A%84%E5%BF%AB%E9%80%9F%E5%AE%9A%E4%BD%8D/" class="post-title-link" itemprop="url">基于Geohash实现根据经纬度的快速定位</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2021-06-08 18:47:31 / 修改时间：18:49:12" itemprop="dateCreated datePublished" datetime="2021-06-08T18:47:31+08:00">2021-06-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%95%99%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">教程</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>在项目中，SDK会上报包含用户经纬度信息的一系列数据，我们需要根据经纬度信息定位出此条数据上报时用户所在的位置（包括国家、省、市、区），并和其他信息写入宽表中。</p>
<h4 id="旧方案"><a href="#旧方案" class="headerlink" title="旧方案"></a>旧方案</h4><p>旧方案中，主要使用GeoSpark对数据进行定位，考虑到同一个经纬度下会有多条数据，所以我们先对数据做分组，同一个用户同一个会话下相同经纬度的数据分为一组，从每组数据中抽取第一条生成一张临时表，再在临时表上调用GeoSpark算出district_id，city_id，province_id，country_id，之后将临时表与原表关联，用临时表的四个ID填充相同经纬度的其他数据的ID。<br>在测试过程中，我们发现有很小一部分数据只有district_id或city_id等细粒度数据，却没有与之相对的province_id、country_id等数据，这是老大所不能接受的（在哪个城市都知道了，国家你给我个null？？？ =。=），所以在计算四个ID之后，会有一个反推的步骤，即：判断是否有下层ID不为空上层ID却为空的情况，如果有，通过下层ID进行反推，得到上层ID，并填充。</p>
<p>中间还有一些其他的过滤排序逻辑不做具体介绍，最后当整个定位逻辑完成后，需要做6~7次的shuffle，我们发现其性能远低于预期，在我们每天将近40亿的数据量下，较大的拖慢了整个流程的运行速度，影响了数据产出，因此需要对这一部分进行优化。经过调研后，本菜鸡决定采用GeoHash的方式进行优化。</p>
<h2 id="什么是Geohash"><a href="#什么是Geohash" class="headerlink" title="什么是Geohash"></a>什么是Geohash</h2><p>简单介绍下GeoHash，我们可以用一个经纬度的点（例如点A： 37.788422,-122.391907 ）计算出一个GeoHash字符串（9q8yyzh），这个字符串代表一个矩形面，点A以及点A附近的点B（37.787933,-122.392887）虽然经纬度不同，但通过经纬度计算出的GeoHash字符串相同，也就是说AB两个点都在（9q8yyzh）这个面内。这样就将二维的经纬度坐标转换成了一维的字符串表示。</p>
<p>但A附近的多少点会跟A共享相同的字符串呢？也就是这个面的大小是怎么确定的呢？这就取决于GeoHash字符串的长度了，GeoHash的字符串长度越长，意味着这个面也就越小，会有更少的点跟A共享同样的GeoHash值。</p>
<p>具体GeoHash的计算方式，以及字符串长度对应的面大小。请参考如下这篇文章：<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/35940647">GeoHash算法学习讲解、解析及原理分析</a></p>
<p>以上就是我们实现基于GeoHash进行定位的基础。</p>
<h2 id="如何用Geohash实现快速定位"><a href="#如何用Geohash实现快速定位" class="headerlink" title="如何用Geohash实现快速定位"></a>如何用Geohash实现快速定位</h2><p>既然可以用经纬度代表的一个点得到一个面，那如果我们的历史数据足够多，映射出足够多的的面，这些面就会像拼图一样，慢慢把我们的世界拼出来。</p>
<p>例如我们可以用21个GeoHash字符串将整个北京欢乐谷拼出来：</p>
<p><img src="https://img-blog.csdnimg.cn/20191206171409523.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="wx4ffc"></p>
<p>有了这个完整的拼图之后，当欢乐谷范围内有一条新数据上报时，我们只需要根据经纬度算出对应的GeoHash值，再用这个值去和这21个字符串匹配，如果和其中任意一个相同，就说明此条数据的位置信息为中国北京的朝阳区（不具体定位到欢乐谷是因为我们最细粒度只划分到行政区)。</p>
<p>采用这个思路，最后我们将世界地图构建好之后，当有新数据需要定位时，我们只需要做一次GeoHash字符串计算，再到数据库中进行匹配即可。速度大大提高。</p>
<h2 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h2><h4 id="Geohash字符串的计算："><a href="#Geohash字符串的计算：" class="headerlink" title="Geohash字符串的计算："></a>Geohash字符串的计算：</h4><p>此处采用的方法是写一个UDF，UDF的功能是输入经纬度及想要的GeoHash字符串长度，输出对应的GeoHash字符串。再将其打成jar包，上传之后在hive中创建临时函数，再进行调用。</p>
<h5 id="首先导入依赖"><a href="#首先导入依赖" class="headerlink" title="首先导入依赖"></a>首先导入依赖</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;ch.hsr&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;geohash&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<h5 id="继承UDF并重写evaluate方法"><a href="#继承UDF并重写evaluate方法" class="headerlink" title="继承UDF并重写evaluate方法"></a>继承UDF并重写evaluate方法</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">public class getGeoHashString extends UDF &#123;</span><br><span class="line"></span><br><span class="line">    private static int precision = 7;</span><br><span class="line"></span><br><span class="line">    public String evaluate(double latitude, double longtitude, int precisionParam) &#123;</span><br><span class="line"></span><br><span class="line">        GeoHash geoHash = GeoHash.withCharacterPrecision(latitude, longtitude, precisionParam);</span><br><span class="line">        return geoHash.toBase32();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String evaluate(double latitude, double longtitude) &#123;</span><br><span class="line"></span><br><span class="line">        GeoHash geoHash = GeoHash.withCharacterPrecision(latitude, longtitude, precision);</span><br><span class="line">        return geoHash.toBase32();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>默认采用7位长度，当然也支持传入参数自定义</p>
<h5 id="Maven打包并上传"><a href="#Maven打包并上传" class="headerlink" title="Maven打包并上传"></a>Maven打包并上传</h5><p>略。。</p>
<h5 id="使用UDF"><a href="#使用UDF" class="headerlink" title="使用UDF"></a>使用UDF</h5><p>目前是在hive命令行中运行的，具体方法如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add jar /data/home/geoHashUDF-1.0-SNAPSHOT-jar-with-dependencies.jar;</span><br></pre></td></tr></table></figure>
<p>先把jar包添加进来，在创建临时函数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE TEMPORARY FUNCTION get_geohash_string as &#x27;getGeoHashString&#x27;;</span><br></pre></td></tr></table></figure>
<p>其中get_geohash_string为函数名，getGeoHashString为你的主类。</p>
<h5 id="接下来写SQL就可以了"><a href="#接下来写SQL就可以了" class="headerlink" title="接下来写SQL就可以了"></a>接下来写SQL就可以了</h5><p>将每一天的新数据计算后写入分区</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE geohash_a_d</span><br><span class="line">PARTITION(dt)</span><br><span class="line">SELECT</span><br><span class="line">get_geohash_string(latitude,longitude),</span><br><span class="line"></span><br><span class="line">geo_district_id,</span><br><span class="line">geo_district_name_en,</span><br><span class="line">geo_district_name_zh,</span><br><span class="line"></span><br><span class="line">geo_city_id,</span><br><span class="line">geo_city_name_en,</span><br><span class="line">geo_city_name_zh,</span><br><span class="line"></span><br><span class="line">geo_province_id,</span><br><span class="line">geo_province_name_en,</span><br><span class="line">geo_province_name_zh,</span><br><span class="line"></span><br><span class="line">geo_country_id,</span><br><span class="line">geo_country_name_en,</span><br><span class="line">geo_country_name_zh,</span><br><span class="line">dt</span><br><span class="line">from </span><br><span class="line">report_i_h</span><br><span class="line">WHERE </span><br><span class="line">dt BETWEEN &#x27;2019-11-01&#x27; AND &#x27;2019-11-30&#x27;</span><br></pre></td></tr></table></figure>
<p>对每一天的数据进行去重合并</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE geohash_summary</span><br><span class="line">SELECT </span><br><span class="line">geohash,</span><br><span class="line">geo_district_id,</span><br><span class="line">geo_district_name_en,</span><br><span class="line">geo_district_name_zh,</span><br><span class="line">geo_city_id,</span><br><span class="line">geo_city_name_en,</span><br><span class="line">geo_city_name_zh,</span><br><span class="line">geo_province_id,</span><br><span class="line">geo_province_name_en,</span><br><span class="line">geo_province_name_zh,</span><br><span class="line">geo_country_id,</span><br><span class="line">geo_country_name_en,</span><br><span class="line">geo_country_name_zh</span><br><span class="line">from geohash_a_d</span><br><span class="line">WHERE dt=&#x27;2019-11-01&#x27;</span><br><span class="line">UNION DISTINCT</span><br><span class="line">SELECT </span><br><span class="line">geohash,</span><br><span class="line">geo_district_id,</span><br><span class="line">geo_district_name_en,</span><br><span class="line">geo_district_name_zh,</span><br><span class="line">geo_city_id,</span><br><span class="line">geo_city_name_en,</span><br><span class="line">geo_city_name_zh,</span><br><span class="line">geo_province_id,</span><br><span class="line">geo_province_name_en,</span><br><span class="line">geo_province_name_zh,</span><br><span class="line">geo_country_id,</span><br><span class="line">geo_country_name_en,</span><br><span class="line">geo_country_name_zh</span><br><span class="line">from geohash_a_d</span><br><span class="line">WHERE dt BETWEEN &#x27;2019-11-02&#x27; AND &#x27;2019-11-30&#x27;</span><br></pre></td></tr></table></figure>
<p>对hash值进行去重，确保一个hash值只对应一条记录（<strong>此处有大坑，之后讲</strong>）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">with tmp as (</span><br><span class="line">SELECT </span><br><span class="line">geohash,</span><br><span class="line">geo_district_id,</span><br><span class="line">geo_district_name_en,</span><br><span class="line">geo_district_name_zh,</span><br><span class="line">geo_city_id,</span><br><span class="line">geo_city_name_en,</span><br><span class="line">geo_city_name_zh,</span><br><span class="line">geo_province_id,</span><br><span class="line">geo_province_name_en,</span><br><span class="line">geo_province_name_zh,</span><br><span class="line">geo_country_id,</span><br><span class="line">geo_country_name_en,</span><br><span class="line">geo_country_name_zh,</span><br><span class="line">row_number() OVER(PARTITION BY geohash ORDER BY </span><br><span class="line">geo_country_id,geo_province_id,geo_city_id,geo_district_id</span><br><span class="line">desc) as rank</span><br><span class="line">from geohash_summary</span><br><span class="line">where geo_country_id is not null</span><br><span class="line">)</span><br><span class="line">insert overwrite table geohash_distinct</span><br><span class="line">select</span><br><span class="line">geohash,</span><br><span class="line">geo_district_id,</span><br><span class="line">geo_district_name_en,</span><br><span class="line">geo_district_name_zh,</span><br><span class="line">geo_city_id,</span><br><span class="line">geo_city_name_en,</span><br><span class="line">geo_city_name_zh,</span><br><span class="line">geo_province_id,</span><br><span class="line">geo_province_name_en,</span><br><span class="line">geo_province_name_zh,</span><br><span class="line">geo_country_id,</span><br><span class="line">geo_country_name_en,</span><br><span class="line">geo_country_name_zh</span><br><span class="line">from tmp</span><br><span class="line">where rank=1</span><br></pre></td></tr></table></figure>
<p>这几个SQL跑完后，我们的GeoHash维度表就初步构建完成了。</p>
<h2 id="效果测试"><a href="#效果测试" class="headerlink" title="效果测试"></a>效果测试</h2><p>构建完成后，便可以进行定位的效果测试了，我们采用的测试方案是：<br>取不在回溯日期内的几天的数据，通过GeoHash的方式获取其位置信息，在和用geoSpark获取的位置信息作对比，校验其准确性。</p>
<p>结果：99.5%的数据可以成功获取定位信息，但是其中千分之七的数据存在distinct级的误差，千分之一的数据存在city级的误差。此外，还意外的实现了千分之一的数据优化。</p>
<p>数据优化：有一些数据可能geoSpark定位不到，或定位的信息不全，通过geoHash可以获取到定位或将定位信息补全。随便举个例子：<br>geoSpark:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">latitude:45.12345</span><br><span class="line">longtitude:110.12345</span><br><span class="line">dim_geohash_distinct.geo_district_id	null</span><br><span class="line">dim_geohash_distinct.geo_city_id	null</span><br><span class="line">dim_geohash_distinct.geo_province_id	3117</span><br><span class="line">dim_geohash_distinct.geo_country_id	3142</span><br></pre></td></tr></table></figure>
<p>geoHash:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">latitude:45.12345</span><br><span class="line">longtitude:110.12345</span><br><span class="line">dim_geohash_distinct.geo_district_id	132</span><br><span class="line">dim_geohash_distinct.geo_city_id	3022</span><br><span class="line">dim_geohash_distinct.geo_province_id	3117</span><br><span class="line">dim_geohash_distinct.geo_country_id	3142</span><br></pre></td></tr></table></figure>
<p>通过GeoHash，可以将缺失的district_id及city_id补全。</p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>采用GeoHash实现定位的前提是有足够的数据量支持，为了达到本文实现的效果，我们回溯了三个月的数据，每天的数据量在35亿左右。最后生成的维度表结构如下所示：</p>
<p><img src="https://img-blog.csdnimg.cn/20191209104852237.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>其中district代表行政区（如东城区、朝阳区），geohash为生成的GeoHash字符串。<br>随便抽取其中一条记录如下：</p>
<p><img src="https://img-blog.csdnimg.cn/20191209105347452.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>（话说以前一直以为西藏的英文是Xizang。。orz)</p>
<p>GeoHash这种方式虽然较快的实现了定位，但仍有一些问题丞待解决，下一篇文章将讨论这些坑以及可能的解决方案。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/08/catalog%E5%92%8Cschema%E7%9A%84%E5%8C%BA%E5%88%AB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo1.jpeg">
      <meta itemprop="name" content="Master Jiang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiang's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/08/catalog%E5%92%8Cschema%E7%9A%84%E5%8C%BA%E5%88%AB/" class="post-title-link" itemprop="url">catalog和schema的区别</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2021-06-08 18:45:56 / 修改时间：18:47:00" itemprop="dateCreated datePublished" datetime="2021-06-08T18:45:56+08:00">2021-06-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">数据仓库</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>直接上图，直观一点：</p>
<p><img src="https://img-blog.csdnimg.cn/20191210125223204.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="什么是catalog"><a href="#什么是catalog" class="headerlink" title="什么是catalog"></a>什么是catalog</h3><p>从概念上说，一个catalog包含多个schema，一个schema下可以包含多个数据库对象（表，视图，字段），catalog可以理解为数据库实例的元数据集合。</p>
<p>常用数据库对catalog和schema的支持如下：</p>
<p><img src="https://img-blog.csdnimg.cn/2019121012584136.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="什么是schema"><a href="#什么是schema" class="headerlink" title="什么是schema"></a>什么是schema</h3><p>一般来说，schema是指数据库表的组织和定义，定义了表、字段以及表和字段间的关系。可以理解为表的命名空间。</p>
<p>推荐下stack overflow上的优秀回答：<br> <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/7022755/whats-the-difference-between-a-catalog-and-a-schema-in-a-relational-database">What’s the difference between a catalog and a schema in a relational database?
</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/08/Hive%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8B%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo1.jpeg">
      <meta itemprop="name" content="Master Jiang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiang's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/08/Hive%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8B%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/" class="post-title-link" itemprop="url">Hive入门之基础知识（一）之杂七杂八</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2021-06-08 18:41:57 / 修改时间：18:42:21" itemprop="dateCreated datePublished" datetime="2021-06-08T18:41:57+08:00">2021-06-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Hive/" itemprop="url" rel="index"><span itemprop="name">Hive</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="随笔总结一些关于Hive的杂七杂八的知识点"><a href="#随笔总结一些关于Hive的杂七杂八的知识点" class="headerlink" title="随笔总结一些关于Hive的杂七杂八的知识点"></a>随笔总结一些关于Hive的杂七杂八的知识点</h1><h2 id="Hive出现的原因"><a href="#Hive出现的原因" class="headerlink" title="Hive出现的原因"></a>Hive出现的原因</h2><p>从一个基于传统关系型数据库和结构化查询语言的数据基础架构转移到Hadoop上，使用HQL查询Hadoop中的数据。</p>
<h2 id="Hive与传统关系型数据库的区别"><a href="#Hive与传统关系型数据库的区别" class="headerlink" title="Hive与传统关系型数据库的区别"></a>Hive与传统关系型数据库的区别</h2><p>Hive不支持记录级别的更新、插入和删除操作。执行延迟大，不支持事务。</p>
<h2 id="Hive组成模块"><a href="#Hive组成模块" class="headerlink" title="Hive组成模块"></a>Hive组成模块</h2><p><img src="https://img-blog.csdnimg.cn/20191213140841632.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>所有的命令和查询都会进入到驱动模块driver中，driver对输入进行解析和编译，以及对需求的计算进行优化，然后启动MR来执行job。Hive本身不会生成MR的程序，而是通过一个表示”job执行计划“的XML文件驱动执行内置的原生的MR模块。<br>Hive通过和jobtracker进行通信来初始化MR任务。</p>
<h2 id="Hive安装目录"><a href="#Hive安装目录" class="headerlink" title="Hive安装目录"></a>Hive安装目录</h2><p><img src="https://img-blog.csdnimg.cn/20191213142812701.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="Hive常用的命令行执行参数"><a href="#Hive常用的命令行执行参数" class="headerlink" title="Hive常用的命令行执行参数"></a>Hive常用的命令行执行参数</h2><p>Hive -e执行完一条命令后立刻退出CLI<br>Hive -S开启静默模式，去掉OK等无关紧要的输出信息<br>Hive -f文件名，指定文件中的一个或多个查询语句。</p>
<h2 id="Hive的基本数据类型"><a href="#Hive的基本数据类型" class="headerlink" title="Hive的基本数据类型"></a>Hive的基本数据类型</h2><p><img src="https://img-blog.csdnimg.cn/2019121314530063.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191213145333149.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>其中时间戳可以是整数、浮点数、字符串，其表示的是UTC时间。</p>
<h4 id="类型转换的用法："><a href="#类型转换的用法：" class="headerlink" title="类型转换的用法："></a>类型转换的用法：</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cast(user_id AS STRING) as user_id_string</span><br></pre></td></tr></table></figure>

<h4 id="Hive中的集合数据类型"><a href="#Hive中的集合数据类型" class="headerlink" title="Hive中的集合数据类型"></a>Hive中的集合数据类型</h4><p>Hive中的集合数据类型：map、struct、array<br>为什么Hive支持集合数据类型，而大多数传统关系型数据库不支持：关系型数据库通过外键将多个表进行关联，当数据量很大时，根据外键进行关联造成的磁盘间的寻址操作将产生很高的代价。</p>
<h4 id="Hive的读时模式"><a href="#Hive的读时模式" class="headerlink" title="Hive的读时模式"></a>Hive的读时模式</h4><p>传统的关系型数据库是写时模式，在数据写入时对模式进行检查。<br>对于Hive要查询的数据，有很多种方式对其进行创建，修改，损坏，因此Hive采用读时模式，在查询时进行验证，尽量修复错误或者赋空值。</p>
<h4 id="Hive查看表信息"><a href="#Hive查看表信息" class="headerlink" title="Hive查看表信息"></a>Hive查看表信息</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DESCRIBE  表名</span><br><span class="line">DESCRIBE EXTENDED 表名</span><br><span class="line">DESCRIBE FORMATTED 表名</span><br></pre></td></tr></table></figure>
<p>都可以查询表的详细信息，信息完整程度由上到下逐渐增加，DESCRIBE FORMATTED的数据最完整。</p>
<h2 id="外部表和内部表"><a href="#外部表和内部表" class="headerlink" title="外部表和内部表"></a>外部表和内部表</h2><p>内部表：也叫管理表，Hive会控制数据的生命周期，删除一个内部表时，Hive也会删除表中的数据。内部表也不便于和其他工作共享数据。<br>外部表：删除表并不会删除表中的数据，但是描述表的元数据信息会被删除。有些HQL的语法结构也不适用于外部表。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE 目标表 LIKE 源表   -- 生成的表是外部表</span><br><span class="line">CREATE TABLE 目标表 LIKE 源表  -- 如果源表是外部表，则目标表也是外部表；如果源表是内部表，目标表也是内部表</span><br></pre></td></tr></table></figure>

<h2 id="Hive表的存储格式"><a href="#Hive表的存储格式" class="headerlink" title="Hive表的存储格式"></a>Hive表的存储格式</h2><p>默认情况下，Hive采用的存储格式为文本文件格式（TEXTFILE），在这种格式下，每一行被认为是一个单独的记录。<br>记录的解析由序列化器，反序列化器（SerDe）来控制。<br>Hive使用一个inputformat对象将输入流分割成记录，使用outputformat对象将记录格式化为输出流，使用SerDe在读数据时将记录解析成列，写数据时将列编码成记录。</p>
<h2 id="Hive防止表被删除或查询"><a href="#Hive防止表被删除或查询" class="headerlink" title="Hive防止表被删除或查询"></a>Hive防止表被删除或查询</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE test ENABLE NO_DROP  // 防止被删除</span><br><span class="line">ALTER TABLE test ENABLE OFFLINE  // 防止被查询</span><br></pre></td></tr></table></figure>
<p>若要允许被删除或查询，只需把 enable 改为 disable </p>
<h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><p>Hive没有关系型数据库中键的概念，只有有限的索引功能。一张表的索引数据存放在另一张表中。<br>建立索引可以帮助剪裁掉一张表的一些数据块，减少MR任务的数据输入量。<br>通过explain+SQL语句可以查看是否使用了索引。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/08/Spark%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%80%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo1.jpeg">
      <meta itemprop="name" content="Master Jiang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiang's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/08/Spark%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%80%EF%BC%89/" class="post-title-link" itemprop="url">Spark入门之基础知识（一）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2021-06-08 18:41:13 / 修改时间：18:41:39" itemprop="dateCreated datePublished" datetime="2021-06-08T18:41:13+08:00">2021-06-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h4 id="什么是Spark"><a href="#什么是Spark" class="headerlink" title="什么是Spark"></a>什么是Spark</h4><p>Spark 是一个用来实现快速而通用的集群计算的平台。</p>
<h4 id="Spark的核心"><a href="#Spark的核心" class="headerlink" title="Spark的核心"></a>Spark的核心</h4><p>Spark 的核心是一个对由很多计算任务组成的、运行在多个工作机器或者一个计算集群上的应用  进行调度、分发、监控的计算引擎。</p>
<h4 id="Spark软件栈设计的优点"><a href="#Spark软件栈设计的优点" class="headerlink" title="Spark软件栈设计的优点"></a>Spark软件栈设计的优点</h4><p>1）软件栈中所有程序库和高级组件都可以从下层的改进中受益。<br>2）运行整个软件栈的代价变小了。<br>3）可以构建出无缝整合处理不同模型的应用。</p>
<h4 id="Spark-的各个组件"><a href="#Spark-的各个组件" class="headerlink" title="Spark 的各个组件"></a>Spark 的各个组件</h4><p>Spark的各个组件如图所示：<br><img src="https://img-blog.csdnimg.cn/20191216125136843.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>Spark Core：实现了Spark的基本功能，报包含任务调度，内存管理，错误恢复，与存储系统交互等模块，还包含了对RDD的API定义。</p>
<p>Spark SQL：Spark用来操作结构化数据的程序包。</p>
<p>SparkStreaming：提供了用来操作数据流的API。</p>
<p>MLib与GraghX目前不在学习计划内暂时不介绍。</p>
<h4 id="Spark核心概念简介"><a href="#Spark核心概念简介" class="headerlink" title="Spark核心概念简介"></a>Spark核心概念简介</h4><p>每个Spark应用都由一个驱动器程序发起集群上的各种并行操作。驱动器程序通过SparkContext对象访问Spark，这个对象代表对计算集群的一个连接。驱动器程序还要管理多个执行器节点。<br><img src="https://img-blog.csdnimg.cn/20191216141248477.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>下篇文章介绍Spark的RDD编程。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/08/Spark%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%BA%8C%EF%BC%89RDD%E7%BC%96%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo1.jpeg">
      <meta itemprop="name" content="Master Jiang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiang's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/08/Spark%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%BA%8C%EF%BC%89RDD%E7%BC%96%E7%A8%8B/" class="post-title-link" itemprop="url">Spark入门之基础知识（二）RDD编程</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2021-06-08 18:40:29 / 修改时间：18:40:55" itemprop="dateCreated datePublished" datetime="2021-06-08T18:40:29+08:00">2021-06-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="什么是RDD"><a href="#什么是RDD" class="headerlink" title="什么是RDD"></a>什么是RDD</h2><p>弹性分布式数据集RDD是Spark的核心抽象。RDD其实就是分布式的元素集合，Spark中的操作创建、转化、或对RDD进行求值，Spark会自动将RDD的数据分发到集群上并并行执行。<br>RDD是一个不可变的分布式对象集合，每个RDD都被分为多个分区，这些分区在集群中不同的节点上运行。<br>”弹性“的解读：弹性意味着在任何时候都能进行重算，当某一部分数据丢失时，可以根据血缘关系将丢失的部分计算出来，而不是从头开始计算全部数据。</p>
<h2 id="RDD的属性"><a href="#RDD的属性" class="headerlink" title="RDD的属性"></a>RDD的属性</h2><p>（1）一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</p>
<p>（2）一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。</p>
<p>（3）RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</p>
<p>（4）一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</p>
<p>（5）一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。</p>
<h2 id="创建RDD的方法"><a href="#创建RDD的方法" class="headerlink" title="创建RDD的方法"></a>创建RDD的方法</h2><p>1）读取外部数据及</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val lines = sc.textFile(&quot;test.txt&quot;) // 从HDFS读取数据</span><br></pre></td></tr></table></figure>
<p>读取数据的操作同样也是惰性的，执行上面这行代码时，数据并没有真正被读取进来。<br>2）在Driver Program中分发驱动器程序中的对象集合。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val data = context.parallelize(List(1,2,3,4,5)) // 创建RDD最简单的方式</span><br></pre></td></tr></table></figure>
<h2 id="RDD的操作方法"><a href="#RDD的操作方法" class="headerlink" title="RDD的操作方法"></a>RDD的操作方法</h2><p>1）转化操作：transformation 会由一个RDD生成一个<em>新的</em>RDD。<br>Spark使用血缘关系图来记录不同RDD之间的依赖关系：<br><img src="https://img-blog.csdnimg.cn/201912161540010.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>2）行动操作：action会将RDD计算出一个结果，返回到Driver Program中或存到HDFS等外部存储中。</p>
<h5 id="转化操作和行动操作的区别"><a href="#转化操作和行动操作的区别" class="headerlink" title="转化操作和行动操作的区别"></a>转化操作和行动操作的区别</h5><p>transformation是惰性的，一个transformation操作并不会被立即计算，只有在action中用到时，才会去真正的计算。<br>转化操作返回的是一个RDD，行动操作返回的是其他的数据类型。</p>
<h5 id="为什么要区分转化和行动操作"><a href="#为什么要区分转化和行动操作" class="headerlink" title="为什么要区分转化和行动操作"></a>为什么要区分转化和行动操作</h5><p>主要是出于对资源和效率上的考虑，如果我们有1000行文本，先把前800行读出来记为RDD1，再将RDD1的前200行读出来记为RDD2，再读取RDD2的第一行记为RDD3，最后把RDD3存到HDFS上。<br>如果每一步都立刻计算的话，就会造成很大的开销。然而事实上我们只需要第一行的数据，在采取惰性计算后，在真正的action被执行时，Spark已经获知了整个转化操作链，只会计算我们真正需要的数据，大大加快了执行速度。</p>
<h2 id="关于RDD缓存"><a href="#关于RDD缓存" class="headerlink" title="关于RDD缓存"></a>关于RDD缓存</h2><h5 id="为什么要缓存"><a href="#为什么要缓存" class="headerlink" title="为什么要缓存"></a>为什么要缓存</h5><p>RDD会在每次对其进行action操作时重新计算，实际生产中很多情况下需要多次重用一个RDD（例如我们读取了一个大宽表，之后在这个宽表上产出各种维度数据），此时最好对RDD进行缓存。</p>
<h2 id="常见transformation与action操作讲解"><a href="#常见transformation与action操作讲解" class="headerlink" title="常见transformation与action操作讲解"></a>常见transformation与action操作讲解</h2><h4 id="transformation"><a href="#transformation" class="headerlink" title="transformation"></a>transformation</h4><p>1）map()：接受一个函数，将其应用于RDD中的每个元素，将函数返回值作为结果RDD中元素的对应值。返回值类型不需要和输入值类型一样。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val data = context.parallelize(List(1,2,3,4,5))</span><br><span class="line">    val dataAdd = data.map(x =&gt; x+1)</span><br><span class="line">    dataAdd.take(6)</span><br></pre></td></tr></table></figure>
<p>对每一个元素加一，结果为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res11: Array[Int] = Array(2, 3, 4, 5, 6)</span><br></pre></td></tr></table></figure>
<p>2）filter()：接受一个函数，将RDD中满足函数要求的元素放入结果RDD中返回。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val dataFilter = data.filter(x =&gt; x &gt; 3)</span><br><span class="line">   dataFilter.take(5)</span><br></pre></td></tr></table></figure>
<p>筛选出大于3的元素，结果为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res12: Array[Int] = Array(4, 5)</span><br></pre></td></tr></table></figure>
<p>3）flatMap()：对每个输入元素生成多个输出元素，可以理解为将输入的RDD拍扁。<br><img src="https://img-blog.csdnimg.cn/20191216161042645.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>经过flatmap拍扁之后，得到的是一个由各列表中元素组成的RDD，而不是一个由多列表组成的RDD。</p>
<h5 id="RDD集合操作"><a href="#RDD集合操作" class="headerlink" title="RDD集合操作"></a>RDD集合操作</h5><p>集合操作要求RDD之间的数据类型相同。<br>1）union操作：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = context.parallelize(List(1, 3, 5, 7, 10))</span><br><span class="line">val rdd2 = context.parallelize(List(2, 4, 6, 8, 10))</span><br><span class="line">val union = rdd1.union(rdd2)</span><br><span class="line">union.take(10)</span><br><span class="line">res14: Array[Int] = Array(1, 3, 5, 7, 10, 2, 4, 6, 8, 10)</span><br></pre></td></tr></table></figure>
<p>返回包含两个RDD中全部元素的新RDD，重复元素也会保留。</p>
<p>2）intersection操作：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = context.parallelize(List(1, 3, 5, 7, 10))</span><br><span class="line">val rdd2 = context.parallelize(List(2, 4, 6, 8, 10))</span><br><span class="line">val intersection = rdd1.intersection(rdd2)</span><br><span class="line">intersection.take(10)</span><br><span class="line">res15: Array[Int] = Array(10)   </span><br></pre></td></tr></table></figure>
<p>结果为两个RDD的交集，会对结果去重。</p>
<p>3）subtract操作：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = context.parallelize(List(1, 3, 5, 7, 10))</span><br><span class="line">val rdd2 = context.parallelize(List(2, 4, 6, 8, 10))</span><br><span class="line">val sub = rdd1.subtract(rdd2)</span><br><span class="line">sub.take(10)</span><br><span class="line">res16: Array[Int] = Array(1, 3, 5, 7)         </span><br></pre></td></tr></table></figure>
<p>返回存在第一个RDD中，不存在第二个RDD中的元素</p>
<p>4）cartesian操作：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.parallelize(List(1, 3, 5))</span><br><span class="line">val rdd2 = sc.parallelize(List(2, 4, 6))</span><br><span class="line">val cartesian = rdd1.cartesian(rdd2)</span><br><span class="line">cartesian.take(9)</span><br><span class="line">res17: Array[(Int, Int)] = Array((1,2), (1,4), (1,6), (3,2), (5,2), (3,4), (3,6), (5,4), (5,6))</span><br></pre></td></tr></table></figure>
<p>对两个RDD求笛卡尔积</p>
<h4 id="action"><a href="#action" class="headerlink" title="action"></a>action</h4><p>1）reduce：<br>操作两个RDD元素类型的数据并返回一个同样类型的新元素</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; list</span><br><span class="line">res0: List[Int] = List(1, 2, 3, 4, 5)</span><br><span class="line">scala&gt; list.reduce((x,y) =&gt; x+y)</span><br><span class="line">res2: Int = 15</span><br></pre></td></tr></table></figure>

<p>2）fold：<br>与reduce相同，只不过加了个初始值的设定<br>初始值为100，最后求和结果为115</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; list</span><br><span class="line">res0: List[Int] = List(1, 2, 3, 4, 5)</span><br><span class="line">scala&gt; list.fold(100)((x,y) =&gt; x+y)</span><br><span class="line">res3: Int = 115</span><br></pre></td></tr></table></figure>
<p>3）aggregate：<br>reduce和fold都只能返回和RDD中元素类型相同的值，例如前面的例子中。list中的是int，那么我们最后得到的返回值也是int，但是aggregate却没有这个限制，可以有不同类型的返回值。<br>举个经典的求平均值例子：<br>aggregate采用柯里化的方式接受三个参数，在本例中(0,0)代表初始值，第一个函数(x, y) =&gt; (x._1 + y, x._2 + 1)代表（目前已统计的数值之和，目前已统计的元素个数），第二个函数(x, y) =&gt; (x._1 + y._1, x._2 + y._2)代表对分布式计算的结果进行累加计算，例如从两个RDD统计的结果为（10，2），（10，3），对这两部分进行合并得到（20，5），最后用20/5即可得到平均值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val data = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">data: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line">scala&gt; val result = data.aggregate(0, 0)((x, y) =&gt; (x._1 + y, x._2 + 1), (x, y) =&gt; (x._1 + y._1, x._2 + y._2))</span><br><span class="line">result: (Int, Int) = (15,5)</span><br><span class="line">scala&gt; val avg = result._1/result._2.toDouble</span><br><span class="line">avg: Double = 3.0                                                     </span><br></pre></td></tr></table></figure>
<p>4）collect：<br>会将整个RDD的内容返回，要求所有数据都必须能一同放入单台机器的内存中。所以不建议在数据量很大时使用，一般用来测试代码。<br>5）take：<br>返回RDD中的n个元素</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; list</span><br><span class="line">res0: List[Int] = List(1, 2, 3, 4, 5)</span><br><span class="line">scala&gt; list.take(3)</span><br><span class="line">res4: List[Int] = List(1, 2, 3)</span><br></pre></td></tr></table></figure>
<p>6）top：<br>从RDD中提取前n个元素</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val data = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">data: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line">scala&gt; data.top(2)</span><br><span class="line">res10: Array[Int] = Array(5, 4)</span><br></pre></td></tr></table></figure>
<p>7）takeSample：<br>从数据采样<br>该方法仅在预期结果数组很小的情况下使用，因为所有数据都被加载到driver的内存中。<br>第一个参数true or false 表示是否可以重复抽样，在选为false的情况下，抽取的元素不会发生重复。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; data.takeSample(false,2)</span><br><span class="line">res11: Array[Int] = Array(4, 1)       </span><br><span class="line">scala&gt; data.takeSample(false,2)</span><br><span class="line">res12: Array[Int] = Array(2, 4) </span><br></pre></td></tr></table></figure>

<h4 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h4><p>当我们对RDD进行缓存时，计算出RDD的节点会分别保存他们计算出的数据，如果某个节点的数据丢失，当我们用到RDD时，Spark会重算这部分的数据。<br>在Scala中，默认情况下persist()会把数据以序列化的形式缓存在JVM的堆空间中。</p>
<p>各个缓存级别的差异如图所示：<br><img src="https://img-blog.csdnimg.cn/20191217153950699.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>persist()的调用不会触发求值，当缓存的数据太多内存放不下时，Spark会使用LRU删除最近最少使用的那部分数据，如果缓存级别为只放在内存中时，再次用到这部分的数据就需要重新计算。</p>
<p>下篇文章将介绍基本的键值对操作。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/08/Hive%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%BA%8C%EF%BC%89%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C%E4%B8%8E%E6%9F%A5%E8%AF%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo1.jpeg">
      <meta itemprop="name" content="Master Jiang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiang's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/08/Hive%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%BA%8C%EF%BC%89%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C%E4%B8%8E%E6%9F%A5%E8%AF%A2/" class="post-title-link" itemprop="url">Hive入门之基础知识（二）之数据操作与查询</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2021-06-08 18:38:57 / 修改时间：18:39:58" itemprop="dateCreated datePublished" datetime="2021-06-08T18:38:57+08:00">2021-06-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Hive/" itemprop="url" rel="index"><span itemprop="name">Hive</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="向Hive表中装载数据"><a href="#向Hive表中装载数据" class="headerlink" title="向Hive表中装载数据"></a>向Hive表中装载数据</h2><p>Hive不会验证向表中装载的数据和表的模式是否匹配（需要自己检查确认），但是会检查文件的格式是否和表结构定义的一致（创建表时指定的结构若为SEQUENCEFILE，则装载进去的文件也应该为sequencefile格式）。</p>
<p>从本地文件系统向表中装载数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA LOCAL INPATH &#x27;path&#x27; INTO TABLE &#x27;table&#x27;  </span><br></pre></td></tr></table></figure>

<p>从本地文件系统向表中装载数据，使用overwrite覆盖原表数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA LOCAL INPATH &#x27;path&#x27; OVERWRITE INTO TABLE &#x27;table&#x27;</span><br></pre></td></tr></table></figure>


<p>从本地文件系统向表中装载数据，使用overwrite覆盖原表数据并指定时间分区</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA LOCAL INPATH &#x27;path&#x27; OVERWRITE INTO TABLE &#x27;table&#x27; PARTITION (dt=&#x27;2019-11-11&#x27;)</span><br></pre></td></tr></table></figure>

<p>从HDFS向表中装载数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA INPATH &#x27;path&#x27; INTO TABLE &#x27;table&#x27;  </span><br></pre></td></tr></table></figure>

<p>从HDFS向表中装载数据，使用overwrite覆盖原表数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA INPATH &#x27;path&#x27; OVERWRITE INTO TABLE &#x27;table&#x27;</span><br></pre></td></tr></table></figure>



<p>从HDFS向表中装载数据，使用overwrite覆盖原表数据并指定时间分区</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA LOCAL INPATH &#x27;path&#x27; OVERWRITE INTO TABLE &#x27;table&#x27; PARTITION (dt=&#x27;2019-11-11&#x27;)</span><br></pre></td></tr></table></figure>

<p>另外需要注意的是，如果使用了local关键字，数据将会被<strong>拷贝</strong>到目标位置，<br>如果不使用local关键字，数据会被<strong>转移</strong>到目标位置。因为Hive默认在分布式文件系统中用户不需要一份文件的多份重复拷贝。</p>
<h2 id="通过查询语句向表中装载数据"><a href="#通过查询语句向表中装载数据" class="headerlink" title="通过查询语句向表中装载数据"></a>通过查询语句向表中装载数据</h2><p>PARTITION关键字可以指定要创建的分区</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE &#x27;table1&#x27;</span><br><span class="line">PARTITION(dt=&#x27;2019-11-11&#x27;)</span><br><span class="line">SELECT * FROM &#x27;table2&#x27;</span><br><span class="line">WHERE dt=&#x27;2019-11-11&#x27;</span><br></pre></td></tr></table></figure>
<h4 id="动态分区插入"><a href="#动态分区插入" class="headerlink" title="动态分区插入"></a>动态分区插入</h4><p>当分区很多时，一个一个指定很麻烦，可以使用动态分区插入<br>需要先开启动态分区</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SET hive.exec.dynamic.partition = true       </span><br><span class="line">SET hive.exec.dynamic.partition.mode = nostrict</span><br></pre></td></tr></table></figure>
<p>使用动态分区插入：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">将表2中11-01号到11-11号的user_id按时间分区插入到表1中</span><br><span class="line">INSERT OVERWRITE TABLE &#x27;table1&#x27;</span><br><span class="line">PARTITION(dt)</span><br><span class="line">SELECT </span><br><span class="line">user_id,</span><br><span class="line">dt</span><br><span class="line">from &#x27;table2&#x27;</span><br><span class="line">WHERE dt BETWEEN &#x27;2019-11-01&#x27; AND &#x27;2019-11-11&#x27;</span><br></pre></td></tr></table></figure>
<p>还可以通过一个查询语句直接创建出表，在实际工作中长使用此功能创建临时表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE tmp AS</span><br><span class="line">SELECT</span><br><span class="line">user_id,</span><br><span class="line">dt,</span><br><span class="line">hour</span><br><span class="line">from table1</span><br></pre></td></tr></table></figure>
<h2 id="导出数据"><a href="#导出数据" class="headerlink" title="导出数据"></a>导出数据</h2><p>如果数据恰好是所需要的格式，直接从HDFS上拷贝文件即可。<br>如果不是需要的格式，可以参考如下示例，Hive会将所有字段序列化成字符串写入到文件中。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE LOCAL DIRECTORY &#x27;yourPath&#x27;</span><br><span class="line">SELECT</span><br><span class="line">user_id,</span><br><span class="line">name,</span><br><span class="line">dt,</span><br><span class="line">hour</span><br><span class="line">from yourTable</span><br></pre></td></tr></table></figure>
<h2 id="如何引用集合类型中的元素"><a href="#如何引用集合类型中的元素" class="headerlink" title="如何引用集合类型中的元素"></a>如何引用集合类型中的元素</h2><h4 id="array"><a href="#array" class="headerlink" title="array"></a>array</h4><p>数组的索引从0开始，使用array[索引]的语法，引用一个不存在的元素将返回null</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT user_info[0] from user_detail</span><br></pre></td></tr></table></figure>
<h4 id="map"><a href="#map" class="headerlink" title="map"></a>map</h4><p>与array相同，使用array[…]的语法，不过使用对应的key值而不是索引</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT user_info[&quot;location&quot;] from user_detail</span><br></pre></td></tr></table></figure>
<h4 id="struct"><a href="#struct" class="headerlink" title="struct"></a>struct</h4><p>使用 点 <strong><font color='yellow'>.</font></strong> 符号</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT address.city from user_detail</span><br></pre></td></tr></table></figure>

<h2 id="如何解决算术计算中可能的上溢或下溢问题"><a href="#如何解决算术计算中可能的上溢或下溢问题" class="headerlink" title="如何解决算术计算中可能的上溢或下溢问题"></a>如何解决算术计算中可能的上溢或下溢问题</h2><p>1）使用范围更广的数据类型，但会占用更多空间。<br>2）进行缩放，除以10、100、1000等，还可以取log值进行计算。</p>
<h2 id="可以进行数据类型转换的函数"><a href="#可以进行数据类型转换的函数" class="headerlink" title="可以进行数据类型转换的函数"></a>可以进行数据类型转换的函数</h2><p>floor、round、ceil，输入的是double类型，返回值为bigint类型。在进行数据类型转换时，这些函数是首选的处理方式。</p>
<h2 id="什么情况下Hive可以避免产生一个MR任务"><a href="#什么情况下Hive可以避免产生一个MR任务" class="headerlink" title="什么情况下Hive可以避免产生一个MR任务"></a>什么情况下Hive可以避免产生一个MR任务</h2><p>1）本地模式，如 select * from table ，不会产生MR，Hive会直接读取存储目录下的文件，输出格式化后的数据。<br>2）在where子句中只有分区字段时，也不会产生MR。 </p>
<h2 id="Hive的join优化"><a href="#Hive的join优化" class="headerlink" title="Hive的join优化"></a>Hive的join优化</h2><p>大多数情况下，Hive会对每对join对象启动一个MR任务，但如果对3个或3个以上的表进行join时，on条件使用了相同的连接键，只会产生一个MR任务。</p>
<h2 id="order-by与sort-by"><a href="#order-by与sort-by" class="headerlink" title="order by与sort by"></a>order by与sort by</h2><p>order by：对结果执行全局排序，所有数据全部放在一个reducer中执行，当数据量很大时，会执行很长时间。<br>sort by：只会在每个reducer中进行排序，即局部排序。可以保证每个reducer输出的结果是有序的，但是不同reducer输出的结果可能会有重复的。</p>
<h2 id="distribute-by语句的使用"><a href="#distribute-by语句的使用" class="headerlink" title="distribute by语句的使用"></a>distribute by语句的使用</h2><p>distribute by控制map的输出在reduce中是如何划分的，可以指定distribute by的值，将相同值得数据分发到一个reducer中去，类似于group by。在分发后的数据中可以调用sort by 进行reducer内部的排序。<br>按用户ID做distribute，再按客户端时间做排序</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SELECT</span><br><span class="line">* </span><br><span class="line">from your_table</span><br><span class="line">DISTRIBUTE BY user_id</span><br><span class="line">SORT BY client_event_time</span><br></pre></td></tr></table></figure>
<p>当distribute by和sort by中的字段相同时，可以使用<strong>cluster by</strong>做替代，达成相同的效果，但是使用cluster by会剥夺sort by的并行性，而且cluster by也不能指定ASC或者desc，只能按降序排列，但是可以实现数据的全局有序。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/08/Hive%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%89%EF%BC%89%E4%B9%8B%E5%88%86%E5%8C%BA%E4%B8%8E%E4%BC%98%E5%8C%96%E7%9A%84%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo1.jpeg">
      <meta itemprop="name" content="Master Jiang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiang's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/08/Hive%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%89%EF%BC%89%E4%B9%8B%E5%88%86%E5%8C%BA%E4%B8%8E%E4%BC%98%E5%8C%96%E7%9A%84%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/" class="post-title-link" itemprop="url">Hive入门之基础知识（三）之分区与优化的简单介绍</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2021-06-08 18:38:19 / 修改时间：18:38:42" itemprop="dateCreated datePublished" datetime="2021-06-08T18:38:19+08:00">2021-06-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Hive/" itemprop="url" rel="index"><span itemprop="name">Hive</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="为什么要对数据进行分区"><a href="#为什么要对数据进行分区" class="headerlink" title="为什么要对数据进行分区"></a>为什么要对数据进行分区</h2><p>在实际生产中，每天的数据量都是以亿为单位的，如果我们不对数据进行分区，直接对全部数据进行统计，则会大大增加时间开销，浪费大量资源。当我们做了合理分区后，例如按天进行分区，当查找某一天的数据时，Hive不会读取全部文件，只会读取HDFS中该天对应的目录，大大提高了执行效率。</p>
<h2 id="分区是不是越多越好"><a href="#分区是不是越多越好" class="headerlink" title="分区是不是越多越好"></a>分区是不是越多越好</h2><p>多数情况下，对数据可以按天进行分区，如果数据量还是太大，可以考虑再按小时进行分区，或者取另一个维度作为分区条件，在此基础上，还可以按分钟，秒进行分区，但是进行过多的分区并不一定会加快查询执<br>行效率。</p>
<p>使用过多的分区可能导致创建了很多非必须的Hadoop文件和文件夹，一个分区对应着一个包含了多个文件的文件夹。如果对表进行过多分区，又存储了跨度很久的数据，最后就会超出NameNode对系统的管理能力，因为NameNode必须保存文件系统的元数据信息，当小文件过多时，会限制HDFS所能管理的文件总数上限。</p>
<p>另外，MR会将一个job转化为多个task，默认情况下，每个task都是一个新的JVM实例，而JVM的开启和销毁都是需要时间开销的，对于每个小文件产生的task，JVM创建和销毁的开销可能会大于对文件内容本身进行处理的开销。</p>
<p>此外，默认情况下，Hive还会限制动态分区可以创建的最大分区数。</p>
<h4 id="对数据进行分桶"><a href="#对数据进行分桶" class="headerlink" title="对数据进行分桶"></a>对数据进行分桶</h4><p>分桶是将数据集分解为更容易管理的若干部分的另一种方法。</p>
<p>以下将创建一个按时间分区，使用user_id作为分桶字段的测试表，user_id会根据我们指定的值（此处为233，分桶数）进行哈希，分发到桶中。根据哈希算法的特性，同一个user_id的数据会分发到一个桶中，而享有相同哈希值的不同user_id的数据也会分发到一个桶中。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE IF NOT EXISTS test(</span><br><span class="line">user_id STRING COMMENT &#x27;用户ID&#x27;,</span><br><span class="line">user_age SMALLINT COMMENT &#x27;用户年龄&#x27;,</span><br><span class="line">user_identify STRING COMMENT &#x27;用户身份证号&#x27;,</span><br><span class="line">url STRING COMMENT &#x27;用户访问的URL&#x27;</span><br><span class="line">)</span><br><span class="line">PARTITIONED BY (dt STRING COMMENT &#x27;时间&#x27;)</span><br><span class="line">CLUSTERED BY (user_id) INTO 233 BUCKETS</span><br></pre></td></tr></table></figure>

<p>向分桶表中插入数据也和其他表有些区别，我们要设置一个属性强制为分桶表设置正确的reducer个数，然后执行SQL语句。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.enforce.bucketing = true</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE test</span><br><span class="line">PARTITION(dt=&#x27;2019-11-11&#x27;)</span><br><span class="line">SELECT</span><br><span class="line">user_id,</span><br><span class="line">user_age,</span><br><span class="line">user_identify,</span><br><span class="line">url</span><br><span class="line">FROM your_table</span><br><span class="line">WHERE dt = &#x27;2019-11-11&#x27;</span><br></pre></td></tr></table></figure>

<p>使用分桶的优点：桶的数量是固定的，所以分桶没有数据波动，适合于进行抽样。分桶还有利于执行高效的map-side join。</p>
<h2 id="JVM重用"><a href="#JVM重用" class="headerlink" title="JVM重用"></a>JVM重用</h2><p>Hadoop的默认配置是采用派生JVM来执行map和reduce的，当小文件比较多时JVM的创建和销毁会造成很大的开销。而JVM重用可以使同一个JVM在一个job中重用多次，减小频繁创建销毁JVM的开销。</p>
<h6 id="JVM重用的缺点："><a href="#JVM重用的缺点：" class="headerlink" title="JVM重用的缺点："></a>JVM重用的缺点：</h6><p>开启JVM重用后，会一直占用使用到的task插槽，以便进行重用，直到任务完成。如果某个reduce过程非常长的话，其他插槽虽然是空着的，但是必须等到该reduce执行完才能释放，无法被其他的job使用，造成资源浪费。</p>
<h2 id="limit语句的优化"><a href="#limit语句的优化" class="headerlink" title="limit语句的优化"></a>limit语句的优化</h2><p>Hive中，很多情况下虽然使用了limit，但还是要将这个语句执行完，再返回一部分。为了避免这个情况，可以考虑开启limit优化</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.limit.optimize.enable = true // 默认为false</span><br></pre></td></tr></table></figure>
<p>不过，将这个功能开启后，可能导致输入中有用的数据永远不会被处理到。</p>
<h2 id="Join优化"><a href="#Join优化" class="headerlink" title="Join优化"></a>Join优化</h2><p>在我们平时写SQL的过程中，可以将最大的表放在最右边，Hive会自动帮我们进行优化。<br>如果所有表中有一个表足够小可以存入内存中，Hive此时可以执行一个map-side join，减少reduce过程。</p>
<h2 id="并行执行"><a href="#并行执行" class="headerlink" title="并行执行"></a>并行执行</h2><p>Hive会将一个任务转化成一个或者多个stage，不同阶段有可能不存在依赖关系，这是开启并行执行就会加快整个任务的执行过程。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.parallel = true</span><br></pre></td></tr></table></figure>


<h2 id="关于Hive的严格模式"><a href="#关于Hive的严格模式" class="headerlink" title="关于Hive的严格模式"></a>关于Hive的严格模式</h2><p>在Hive的严格模式下，一些类型的查询语句是被禁止执行的，因为这些语句可能会产生一个巨大的MR任务（例如两个十亿级数据量的表做join）。</p>
<p>在严格模式下，三种类型的操作将被禁止：<br>1）对分区表的查询中，where条件没有对分区字段做限制。<br>2）使用order by子句时，没有用limit做限制。当order by时，会将所有结果分发到一个reducer中进行排序，如果不进行限制，可能会执行很长时间。<br>3）限制笛卡尔积 （join）</p>
<p>不过在确实需要执行某些语句的情况下，可以关闭严格模式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.mapred.mode = nostrict;</span><br></pre></td></tr></table></figure>



<h2 id="reducer个数的调整"><a href="#reducer个数的调整" class="headerlink" title="reducer个数的调整"></a>reducer个数的调整</h2><p>reducer设置的过多会导致job在运行过程中产生过多开销，还可能会消耗集群中过多的插槽，降低其他任务的运行效率，设置的过少会降低并行性。<br>HIve默认的reducer个数为3，可以通过</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapred.reduce.tasks = number</span><br></pre></td></tr></table></figure>
<p>中number值的设置调整reducer个数。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/08/Hive%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B9%8B%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F%E5%92%8C%E5%8E%8B%E7%BC%A9%E6%96%B9%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo1.jpeg">
      <meta itemprop="name" content="Master Jiang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiang's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/08/Hive%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B9%8B%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F%E5%92%8C%E5%8E%8B%E7%BC%A9%E6%96%B9%E6%B3%95/" class="post-title-link" itemprop="url">Hive入门之基础知识（四）之文件格式和压缩方法</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2021-06-08 18:37:29 / 修改时间：18:38:04" itemprop="dateCreated datePublished" datetime="2021-06-08T18:37:29+08:00">2021-06-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Hive/" itemprop="url" rel="index"><span itemprop="name">Hive</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="为什么使用压缩"><a href="#为什么使用压缩" class="headerlink" title="为什么使用压缩"></a>为什么使用压缩</h2><p>使用压缩可以减小所需的磁盘空间，减小磁盘和网络的IO操作，减小了载入内存的数据量提高了IO吞吐量，提升了网络性能（因为MapReduce大多是文件读写操作，属于IO密集型）。</p>
<h2 id="几种不同的压缩方案"><a href="#几种不同的压缩方案" class="headerlink" title="几种不同的压缩方案"></a>几种不同的压缩方案</h2><p>如果磁盘利用率和IO开销都需要考虑的话，可以选择以下两种</p>
<h6 id="BZip2"><a href="#BZip2" class="headerlink" title="BZip2"></a>BZip2</h6><p>压缩率最高，同时需要消耗最多的CPU开销。</p>
<h6 id="GZip"><a href="#GZip" class="headerlink" title="GZip"></a>GZip</h6><p>压缩率 / 压缩、解压速度的最佳选择</p>
<h6 id="LZO-和-Snappy"><a href="#LZO-和-Snappy" class="headerlink" title="LZO 和 Snappy"></a>LZO 和 Snappy</h6><p>压缩率不及上面的两种，但是压缩和解压的速度更快，特别是在解压过程。<br>如果需要频繁读取数据进行解压缩，可以使用LZO或者Snappy。</p>
<p>此外，需要注意的一点是GZip和Snappy的压缩文件不可划分。即MapReduce无法将输入的文件划分为多个部分，如果文件特别大，就需要一个单独task来进行处理。</p>
<h2 id="中间压缩"><a href="#中间压缩" class="headerlink" title="中间压缩"></a>中间压缩</h2><p>开启中间压缩可以减少map和reduce之间的数据传输量。<br>对于中间压缩，低CPU开销比最终的压缩效率更为重要。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.compress.intermediate = true // 开启中间压缩，默认为false</span><br></pre></td></tr></table></figure>



<h2 id="sequence-file存储格式"><a href="#sequence-file存储格式" class="headerlink" title="sequence file存储格式"></a>sequence file存储格式</h2><p>sequence file是含有键值对的二进制文件。</p>
<p>对于不可分割的压缩文件，最严重的的缺点就是只能从头读到尾，无法由多个mapper并行执行。</p>
<p>而sequence file存储格式可以将一个文件分为多块，然后采取可分割的方式对块进行压缩。<br>创建表时可以直接指定sequence file格式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE tmp</span><br><span class="line">(</span><br><span class="line">user_id STRING</span><br><span class="line">)</span><br><span class="line">STORED AS SEQUENCEFILE</span><br></pre></td></tr></table></figure>

<p>sequence file提供了三种压缩方式：NONE， RECORD， BLOCK 。<br>默认为RECORD级，不过通常来说BLOCK级压缩性能最好。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6%E4%B8%AD%E7%9A%84%E5%B0%8F%E6%96%87%E4%BB%B6%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo1.jpeg">
      <meta itemprop="name" content="Master Jiang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiang's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6%E4%B8%AD%E7%9A%84%E5%B0%8F%E6%96%87%E4%BB%B6%E9%97%AE%E9%A2%98/" class="post-title-link" itemprop="url">大数据框架中的小文件问题</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2021-06-08 18:34:38 / 修改时间：18:37:03" itemprop="dateCreated datePublished" datetime="2021-06-08T18:34:38+08:00">2021-06-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/" itemprop="url" rel="index"><span itemprop="name">常见问题</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><a target="_blank" rel="noopener" href="http://xcx1024.com/ArtInfo/997661.html">原文链接</a></p>
<h1 id="Hadoop里面的小文件问题"><a href="#Hadoop里面的小文件问题" class="headerlink" title="Hadoop里面的小文件问题"></a>Hadoop里面的小文件问题</h1><p>小文件指的是那些size比HDFS的block size(默认64M)小的多的文件。如果在HDFS中存储小文件，那么在HDFS中肯定会含有许许多多这样的小文件(不然就不会用hadoop了)。而HDFS的问题在于无法很有效的处理大量小文件。</p>
<p>任何一个文件，目录和block，在HDFS中都会被表示为一个object存储在namenode的内存中，没一个object占用150 bytes的内存空间。所以，如果有10million个文件，没一个文件对应一个block，那么就将要消耗namenode 3G的内存来保存这些block的信息。如果规模再大一些，那么将会超出现阶段计算机硬件所能满足的极限。</p>
<p>不仅如此，HDFS并不是为了有效的处理大量小文件而存在的。它主要是为了流式的访问大文件而设计的。对小文件的读取通常会造成大量从datanode到datanode的seeks和hopping来retrieve文件，而这样是非常的低效的一种访问方式。</p>
<h3 id="大量小文件在mapreduce中的问题"><a href="#大量小文件在mapreduce中的问题" class="headerlink" title="大量小文件在mapreduce中的问题"></a>大量小文件在mapreduce中的问题</h3><p>Map tasks通常是每次处理一个block的input(默认使用FileInputFormat)。如果文件非常的小，并且拥有大量的这种小文件，那么每一个map task都仅仅处理了非常小的input数据，并且会产生大量的map tasks，每一个map task都会消耗一定量的bookkeeping的资源。比较一个1GB的文件，默认block size为64M，和1Gb的文件，没一个文件100KB，那么后者没一个小文件使用一个map task，那么job的时间将会十倍甚至百倍慢于前者。</p>
<p>hadoop中有一些特性可以用来减轻这种问题：可以在一个JVM中允许task reuse，以支持在一个JVM中运行多个map task，以此来减少一些JVM的启动消耗(通过设置mapred.job.reuse.jvm.num.tasks属性，默认为1，－1为无限制)。另一种方法为使用MultiFileInputSplit，它可以使得一个map中能够处理多个split。</p>
<h5 id="为什么会产生大量的小文件？"><a href="#为什么会产生大量的小文件？" class="headerlink" title="为什么会产生大量的小文件？"></a>为什么会产生大量的小文件？</h5><p>至少有两种情况下会产生大量的小文件</p>
<p>1.这些小文件都是一个大的逻辑文件的pieces。由于HDFS仅仅在不久前才刚刚支持对文件的append，因此以前用来向unbounde files(例如log文件)添加内容的方式都是通过将这些数据用许多chunks的方式写入HDFS中。</p>
<p>2.文件本身就是很小。例如许许多多的小图片文件。每一个图片都是一个独立的文件。并且没有一种很有效的方法来将这些文件合并为一个大的文件<br>这两种情况需要有不同的解决方式。对于第一种情况，文件是由许许多多的records组成的，那么可以通过件邪行的调用HDFS的sync()方法(和append方法结合使用)来解决。或者，可以通过些一个程序来专门合并这些小文件(see Nathan Marz’s post about a tool called the Consolidator which does exactly this)。<br>对于第二种情况，就需要某种形式的容器来通过某种方式来group这些file。hadoop提供了一些选择：</p>
<h5 id="HAR-files"><a href="#HAR-files" class="headerlink" title="HAR files"></a>HAR files</h5><p>Hadoop Archives (HAR files)是在0.18.0版本中引入的，它的出现就是为了缓解大量小文件消耗namenode内存的问题。HAR文件是通过在HDFS上构建一个层次化的文件系统来工作。一个HAR文件是通过hadoop的archive命令来创建，而这个命令实 际上也是运行了一个MapReduce任务来将小文件打包成HAR。对于client端来说，使用HAR文件没有任何影响。所有的原始文件都 visible &amp;&amp; accessible（using har://URL）。但在HDFS端它内部的文件数减少了。</p>
<p>通过HAR来读取一个文件并不会比直接从HDFS中读取文件高效，而且实际上可能还会稍微低效一点，因为对每一个HAR文件的访问都需要完成两层index文件的读取和文件本身数据的读取(见上图)。并且尽管HAR文件可以被用来作为MapReduce job的input，但是并没有特殊的方法来使maps将HAR文件中打包的文件当作一个HDFS文件处理。可以考虑通过创建一种input format，利用HAR文件的优势来提高MapReduce的效率，但是目前还没有人作这种input format。需要注意的是：MultiFileInputSplit，即使在HADOOP-4565的改进(choose files in a split that are node local)，但始终还是需要seek per small file。</p>
<h5 id="Sequence-Files"><a href="#Sequence-Files" class="headerlink" title="Sequence Files"></a>Sequence Files</h5><p>通常对于“the small files problem”的回应会是：使用SequenceFile。这种方法是说，使用filename作为key，并且file contents作为value。实践中这种方式非常管用。回到10000个100KB的文件，可以写一个程序来将这些小文件写入到一个单独的SequenceFile中去，然后就可以在一个streaming fashion(directly or using mapreduce)中来使用这个sequenceFile。不仅如此，SequenceFiles也是splittable的，所以mapreduce可以break them into chunks，并且分别的被独立的处理。和HAR不同的是，这种方式还支持压缩。block的压缩在许多情况下都是最好的选择，因为它将多个records压缩到一起，而不是一个record一个压缩。</p>
<p>将已有的许多小文件转换成一个SequenceFiles可能会比较慢。但是，完全有可能通过并行的方式来创建一个一系列的SequenceFiles。(Stuart Sierra has written a very useful post about converting a tar file into a SequenceFile—tools like this are very useful)。更进一步，如果有可能最好设计自己的数据pipeline来将数据直接写入一个SequenceFile。</p>
<h1 id="hive中的小文件问题"><a href="#hive中的小文件问题" class="headerlink" title="hive中的小文件问题"></a>hive中的小文件问题</h1><h3 id="小文件是如何产生的"><a href="#小文件是如何产生的" class="headerlink" title="小文件是如何产生的"></a>小文件是如何产生的</h3><p>1.动态分区插入数据，产生大量的小文件，从而导致map数量剧增。<br>2.reduce数量越多，小文件也越多(reduce的个数和输出文件是对应的)。<br>3.数据源本身就包含大量的小文件。</p>
<h3 id="小文件问题的影响"><a href="#小文件问题的影响" class="headerlink" title="小文件问题的影响"></a>小文件问题的影响</h3><p>1.从Hive的角度看，小文件会开很多map，一个map开一个JVM去执行，所以这些任务的初始化，启动，执行会浪费大量的资源，严重影响性能。<br>2.在HDFS中，每个小文件对象约占150byte，如果小文件过多会占用大量内存。这样NameNode内存容量严重制约了集群的扩展。</p>
<h3 id="小文件问题的解决方案"><a href="#小文件问题的解决方案" class="headerlink" title="小文件问题的解决方案"></a>小文件问题的解决方案</h3><h5 id="从小文件产生的途经就可以从源头上控制小文件数量，方法如下："><a href="#从小文件产生的途经就可以从源头上控制小文件数量，方法如下：" class="headerlink" title="从小文件产生的途经就可以从源头上控制小文件数量，方法如下："></a>从小文件产生的途经就可以从源头上控制小文件数量，方法如下：</h5><p>1.使用Sequencefile作为表存储格式，不要用textfile，在一定程度上可以减少小文件。<br>2.减少reduce的数量(可以使用参数进行控制)。<br>3.少用动态分区，用时记得按distribute by分区。</p>
<h5 id="对于已有的小文件，我们可以通过以下几种方案解决："><a href="#对于已有的小文件，我们可以通过以下几种方案解决：" class="headerlink" title="对于已有的小文件，我们可以通过以下几种方案解决："></a>对于已有的小文件，我们可以通过以下几种方案解决：</h5><p>1.使用hadoop archive命令把小文件进行归档。<br>2.重建表，建表时减少reduce数量。<br>3.通过参数进行调节，设置map/reduce端的相关参数，如下：</p>
<h6 id="设置map输入合并小文件的相关参数："><a href="#设置map输入合并小文件的相关参数：" class="headerlink" title="设置map输入合并小文件的相关参数："></a>设置map输入合并小文件的相关参数：</h6><p>//每个Map最大输入大小(这个值决定了合并后文件的数量)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapred.max.split.size=256000000;</span><br></pre></td></tr></table></figure>
<p>//一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapred.min.split.size.per.node=100000000;</span><br></pre></td></tr></table></figure>
<p>//一个交换机下split的至少的大小(这个值决定了多个交换机上的文件是否需要合并)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapred.min.split.size.per.rack=100000000;</span><br></pre></td></tr></table></figure>
<p>//执行Map前进行小文件合并</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure>



<h6 id="设置map输出和reduce输出进行合并的相关参数："><a href="#设置map输出和reduce输出进行合并的相关参数：" class="headerlink" title="设置map输出和reduce输出进行合并的相关参数："></a>设置map输出和reduce输出进行合并的相关参数：</h6><p>//设置map端输出进行合并，默认为true</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.merge.mapfiles = true</span><br></pre></td></tr></table></figure>
<p>//设置reduce端输出进行合并，默认为false</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.merge.mapredfiles = true</span><br></pre></td></tr></table></figure>
<p>//设置合并文件的大小</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.merge.size.per.task = 25610001000</span><br></pre></td></tr></table></figure>
<p>//当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件merge。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.merge.smallfiles.avgsize=16000000</span><br></pre></td></tr></table></figure>

<h1 id="spark中的小文件问题"><a href="#spark中的小文件问题" class="headerlink" title="spark中的小文件问题"></a>spark中的小文件问题</h1><h3 id="SparkStreaming如何解决小文件问题"><a href="#SparkStreaming如何解决小文件问题" class="headerlink" title="SparkStreaming如何解决小文件问题"></a>SparkStreaming如何解决小文件问题</h3><p>使用sparkstreaming时，如果实时计算结果要写入到HDFS，那么不可避免的会遇到一个问题，那就是在默认情况下会产生非常多的小文件，这是由sparkstreaming的微批处理模式和DStream(RDD)的分布式(partition)特性导致的，sparkstreaming为每个partition启动一个独立的线程来处理数据，一旦文件输出到HDFS，那么这个文件流就关闭了，再来一个batch的parttition任务，就再使用一个新的文件流，那么假设，一个batch为10s，每个输出的DStream有32个partition，那么一个小时产生的文件数将会达到(3600/10)*32=11520个之多。众多小文件带来的结果是有大量的文件元信息，比如文件的location、文件大小、block number等需要NameNode来维护，NameNode会因此鸭梨山大。不管是什么格式的文件，parquet、text,、JSON或者 Avro，都会遇到这种小文件问题，这里讨论几种处理Sparkstreaming小文件的典型方法。</p>
<h5 id="1-增加batch大小"><a href="#1-增加batch大小" class="headerlink" title="1 增加batch大小"></a>1 增加batch大小</h5><p>这种方法很容易理解，batch越大，从外部接收的event就越多，内存积累的数据也就越多，那么输出的文件数也就回变少，比如上边的时间从10s增加为100s，那么一个小时的文件数量就会减少到1152个。但别高兴太早，实时业务能等那么久吗，本来人家10s看到结果更新一次，现在要等快两分钟，是人都会骂娘。所以这种方法适用的场景是消息实时到达，但不想挤压在一起处理，因为挤压在一起处理的话，批处理任务在干等，这时就可以采用这种方法(是不是很像spark内部的pipeline模式，但是要注意区别哦)。</p>
<h5 id="2-Coalesce大法好？"><a href="#2-Coalesce大法好？" class="headerlink" title="2 Coalesce大法好？"></a>2 Coalesce大法好？</h5><p>文章开头讲了，小文件的基数是：batch_number*partition_number，而第一种方法是减少batch_number，那么这种方法就是减少partition_number了，这个api不细说，就是减少初始的分区个数。看过spark源码的童鞋都知道，对于窄依赖，一个子RDD的partition规则继承父RDD，对于宽依赖(就是那些个叉叉叉ByKey操作)，如果没有特殊指定分区个数，也继承自父rdd。那么初始的SourceDstream是几个partiion，最终的输出就是几个partition。所以Coalesce大法的好处就是，可以在最终要输出的时候，来减少一把partition个数。但是这个方法的缺点也很明显，本来是32个线程在写256M数据，现在可能变成了4个线程在写256M数据，而没有写完成这256M数据，这个batch是不算做结束的。那么一个batch的处理时延必定增长，batch挤压会逐渐增大。这种方法也要慎用，切鸡切鸡啊！</p>
<h5 id="3-SparkStreaming外部来处理"><a href="#3-SparkStreaming外部来处理" class="headerlink" title="3 SparkStreaming外部来处理"></a>3 SparkStreaming外部来处理</h5><p>我们既然把数据输出到hdfs，那么说明肯定是要用hive或者sparksql这样的“sql on hadoop”系统类进一步进行数据分析，而这些表一般都是按照半小时或者一小时、一天，这样来分区的(注意不要和sparkStreaming的分区混淆，这里的分区，是用来做分区裁剪优化的)，那么我们可以考虑在SparkStreaming外再启动定时的批处理任务来合并SparkStreaming产生的小文件。这种方法不是很直接，但是却比较有用，“性价比”较高，唯一要注意的是，批处理的合并任务在时间切割上要把握好，搞不好就可能回去合并一个还在写入的SparkStreaming小文件。</p>
<h5 id="自己调用foreach去append"><a href="#自己调用foreach去append" class="headerlink" title="自己调用foreach去append"></a>自己调用foreach去append</h5><p>SparkStreaming提供的foreach这个outout类api，可以让我们自定义输出计算结果的方法。那么我们其实也可以利用这个特性，那就是每个batch在要写文件时，并不是去生成一个新的文件流，而是把之前的文件打开。考虑这种方法的可行性，首先，HDFS上的文件不支持修改，但是很多都支持追加，那么每个batch的每个partition就对应一个输出文件，每次都去追加这个partition对应的输出文件，这样也可以实现减少文件数量的目的。这种方法要注意的就是不能无限制的追加，当判断一个文件已经达到某一个阈值时，就要产生一个新的文件进行追加了。</p>
<h1 id="如何避免Spark-SQL做数据导入时产生大量小文件"><a href="#如何避免Spark-SQL做数据导入时产生大量小文件" class="headerlink" title="如何避免Spark SQL做数据导入时产生大量小文件"></a>如何避免Spark SQL做数据导入时产生大量小文件</h1><h3 id="什么是小文件？"><a href="#什么是小文件？" class="headerlink" title="什么是小文件？"></a>什么是小文件？</h3><p>生产上，我们往往将Spark SQL作为Hive的替代方案，来获得SQL on Hadoop更出色的性能。因此，本文所讲的是指存储于HDFS中小文件，即指文件的大小远小于HDFS上块（dfs.block.size）大小的文件。</p>
<h3 id="小文件问题的影响-1"><a href="#小文件问题的影响-1" class="headerlink" title="小文件问题的影响"></a>小文件问题的影响</h3><p>一方面，大量的小文件会给Hadoop集群的扩展性和性能带来严重的影响。NameNode在内存中维护整个文件系统的元数据镜像，用户HDFS的管理；其中每个HDFS文件元信息（位置，大小，分块等）对象约占150字节，如果小文件过多，会占用大量内存，直接影响NameNode的性能。相对的，HDFS读写小文件也会更加耗时，因为每次都需要从NameNode获取元信息，并与对应的DataNode建立连接。如果NameNode在宕机中恢复，也需要更多的时间从元数据文件中加载。</p>
<p>另一方面，也会给Spark SQL等查询引擎造成查询性能的损耗，大量的数据分片信息以及对应产生的Task元信息也会给Spark Driver的内存造成压力，带来单点问题。此外，入库操作最后的commit job操作，在Spark Driver端单点做，很容易出现单点的性能问题。</p>
<h3 id="Spark小文件产生的过程"><a href="#Spark小文件产生的过程" class="headerlink" title="Spark小文件产生的过程"></a>Spark小文件产生的过程</h3><p>数据源本身就是就含大量小文件<br>动态分区插入数据，没有Shuffle的情况下，输入端有多少个逻辑分片，对应的HadoopRDD就会产生多少个HadoopPartition，每个Partition对应于Spark作业的Task（个数为M），分区数为N。最好的情况就是（M=N） &amp;&amp; （M中的数据也是根据N来预先打散的），那就刚好写N个文件；最差的情况下，每个Task中都有各个分区的记录，那文件数最终文件数将达到M * N个。这种情况下是极易产生小文件的。</p>
<p>比如我们拿TPCDS测试集中的store_sales进行举例， sql如下所示<br>use tpcds_1t_parquet;</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">INSERT overwrite table store_sales partition </span><br><span class="line">       ( </span><br><span class="line">              ss_sold_date_sk </span><br><span class="line">       ) </span><br><span class="line">SELECT ss_sold_time_sk, </span><br><span class="line">       ss_item_sk, </span><br><span class="line">       ss_customer_sk, </span><br><span class="line">       ss_cdemo_sk, </span><br><span class="line">       ss_hdemo_sk, </span><br><span class="line">       ss_addr_sk, </span><br><span class="line">       ss_store_sk, </span><br><span class="line">       ss_promo_sk, </span><br><span class="line">       ss_ticket_number, </span><br><span class="line">       ss_quantity, </span><br><span class="line">       ss_wholesale_cost, </span><br><span class="line">       ss_list_price, </span><br><span class="line">       ss_sales_price, </span><br><span class="line">       ss_ext_discount_amt, </span><br><span class="line">       ss_ext_sales_price, </span><br><span class="line">       ss_ext_wholesale_cost, </span><br><span class="line">       ss_ext_list_price, </span><br><span class="line">       ss_ext_tax, </span><br><span class="line">       ss_coupon_amt, </span><br><span class="line">       ss_net_paid, </span><br><span class="line">       ss_net_paid_inc_tax, </span><br><span class="line">       ss_net_profit, </span><br><span class="line">       ss_sold_date_sk </span><br><span class="line">FROM   tpcds_1t_ext.et_store_sales;</span><br></pre></td></tr></table></figure>

<p>首先我们得到其执行计划，如下所示，<br><font color='yellow'> Physical Plan </font></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">InsertIntoHiveTable MetastoreRelation tpcds_1t_parquet, store_sales, Map(ss_sold_date_sk -&gt; None), true, false</span><br><span class="line">+- HiveTableScan [ss_sold_time_sk#4L, ss_item_sk#5L, ss_customer_sk#6L, ss_cdemo_sk#7L, ss_hdemo_sk#8L, ss_addr_sk#9L, ss_store_sk#10L, ss_promo_sk#11L, ss_ticket_number#12L, ss_quantity#13, ss_wholesale_cost#14, ss_list_price#15, ss_sales_price#16, ss_ext_discount_amt#17, ss_ext_sales_price#18, ss_ext_wholesale_cost#19, ss_ext_list_price#20, ss_ext_tax#21, ss_coupon_amt#22, ss_net_paid#23, ss_net_paid_inc_tax#24, ss_net_profit#25, ss_sold_date_sk#3L], MetastoreRelation tpcds_1t_ext, et_store_sales</span><br></pre></td></tr></table></figure>

<p>store_sales的原生文件包含1616逻辑分片，对应生成1616 个Spark Task，插入动态分区表之后生成1824个数据分区加一个NULL值的分区，每个分区下都有可能生成1616个文件，这种情况下，最终的文件数量极有可能达到2949200。1T的测试集store_sales也就大概300g，这种情况每个文件可能就零点几M。</p>
<p>动态分区插入数据，有Shuffle的情况下，上面的M值就变成了spark.sql.shuffle.partitions(默认值200)这个参数值，文件数的算法和范围和2中基本一致。</p>
<p>比如，为了防止Shuffle阶段的数据倾斜我们可以在上面的sql中加上 distribute by rand()，这样我们的执行计划就变成了，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">InsertIntoHiveTable MetastoreRelation tpcds_1t_parquet, store_sales, Map(ss_sold_date_sk -&gt; None), true, false</span><br><span class="line">+- *Project [ss_sold_time_sk#4L, ss_item_sk#5L, ss_customer_sk#6L, ss_cdemo_sk#7L, ss_hdemo_sk#8L, ss_addr_sk#9L, ss_store_sk#10L, ss_promo_sk#11L, ss_ticket_number#12L, ss_quantity#13, ss_wholesale_cost#14, ss_list_price#15, ss_sales_price#16, ss_ext_discount_amt#17, ss_ext_sales_price#18, ss_ext_wholesale_cost#19, ss_ext_list_price#20, ss_ext_tax#21, ss_coupon_amt#22, ss_net_paid#23, ss_net_paid_inc_tax#24, ss_net_profit#25, ss_sold_date_sk#3L]</span><br><span class="line">   +- Exchange(coordinator id: 1080882047) hashpartitioning(_nondeterministic#49, 2048), coordinator[target post-shuffle partition size: 67108864]</span><br><span class="line">      +- *Project [ss_sold_time_sk#4L, ss_item_sk#5L, ss_customer_sk#6L, ss_cdemo_sk#7L, ss_hdemo_sk#8L, ss_addr_sk#9L, ss_store_sk#10L, ss_promo_sk#11L, ss_ticket_number#12L, ss_quantity#13, ss_wholesale_cost#14, ss_list_price#15, ss_sales_price#16, ss_ext_discount_amt#17, ss_ext_sales_price#18, ss_ext_wholesale_cost#19, ss_ext_list_price#20, ss_ext_tax#21, ss_coupon_amt#22, ss_net_paid#23, ss_net_paid_inc_tax#24, ss_net_profit#25, ss_sold_date_sk#3L, rand(4184439864130379921) AS _nondeterministic#49]</span><br><span class="line">         +- HiveTableScan [ss_sold_date_sk#3L, ss_sold_time_sk#4L, ss_item_sk#5L, ss_customer_sk#6L, ss_cdemo_sk#7L, ss_hdemo_sk#8L, ss_addr_sk#9L, ss_store_sk#10L, ss_promo_sk#11L, ss_ticket_number#12L, ss_quantity#13, ss_wholesale_cost#14, ss_list_price#15, ss_sales_price#16, ss_ext_discount_amt#17, ss_ext_sales_price#18, ss_ext_wholesale_cost#19, ss_ext_list_price#20, ss_ext_tax#21, ss_coupon_amt#22, ss_net_paid#23, ss_net_paid_inc_tax#24, ss_net_profit#25], MetastoreRelation tpcds_1t_ext, et_store_sales</span><br></pre></td></tr></table></figure>
<p>这种情况下，这样我们的文件数妥妥的就是spark.sql.shuffle.partitions * N，因为rand函数一般会把数据打散的非常均匀。当spark.sql.shuffle.partitions设置过大时，小文件问题就产生了；当spark.sql.shuffle.partitions设置过小时，任务的并行度就下降了，性能随之受到影响。<br>最理想的情况，当然是根据分区字段进行shuffle，在上面的sql中加上distribute by ss_sold_date_sk。 把同一分区的记录都哈希到同一个分区中去，由一个Spark的Task进行写入，这样的话只会产生N个文件，在我们的case中store_sales，在1825个分区下各种生成了一个数据文件。<br>但是这种情况下也容易出现数据倾斜的问题，比如双11的销售数据就很容易在这种情况下发生倾斜。</p>
<p>基于分区字段Shuffle可能出现数据倾斜</p>
<p>如上图所示，在我们插入store_sales时，就发生了null值的倾斜，大大的拖慢的数据入库的时间。</p>
<p>如何解决Spark SQL产生小文件问题</p>
<p>前面已经提到根据分区字段进行分区，除非每个分区下本身的数据较少，分区字段选择不合理，那么小文件问题基本上就不存在了，但是也有可能由于shuffle引入新的数据倾斜问题。<br>我们首先可以尝试是否可以将两者结合使用， 在之前的sql上加上distribute by ss_sold_date_sk，cast(rand() * 5 as int)， 这个类似于我们处理数据倾斜问题时候给字段加上后缀的形式。如，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">use tpcds_1t_parquet;</span><br><span class="line"></span><br><span class="line">INSERT overwrite table store_sales partition </span><br><span class="line">       ( </span><br><span class="line">              ss_sold_date_sk </span><br><span class="line">       ) </span><br><span class="line">SELECT ss_sold_time_sk, </span><br><span class="line">       ss_item_sk, </span><br><span class="line">       ss_customer_sk, </span><br><span class="line">       ss_cdemo_sk, </span><br><span class="line">       ss_hdemo_sk, </span><br><span class="line">       ss_addr_sk, </span><br><span class="line">       ss_store_sk, </span><br><span class="line">       ss_promo_sk, </span><br><span class="line">       ss_ticket_number, </span><br><span class="line">       ss_quantity, </span><br><span class="line">       ss_wholesale_cost, </span><br><span class="line">       ss_list_price, </span><br><span class="line">       ss_sales_price, </span><br><span class="line">       ss_ext_discount_amt, </span><br><span class="line">       ss_ext_sales_price, </span><br><span class="line">       ss_ext_wholesale_cost, </span><br><span class="line">       ss_ext_list_price, </span><br><span class="line">       ss_ext_tax, </span><br><span class="line">       ss_coupon_amt, </span><br><span class="line">       ss_net_paid, </span><br><span class="line">       ss_net_paid_inc_tax, </span><br><span class="line">       ss_net_profit, </span><br><span class="line">       ss_sold_date_sk </span><br><span class="line">FROM   tpcds_1t_ext.et_store_sales</span><br><span class="line">distribute by ss_sold_date_sk, cast(rand() * 5 as int);</span><br></pre></td></tr></table></figure>

<p>按照之前的推算，每个分区下将产生5个文件，同时null值倾斜部分的数据也被打散成五份进行计算，缓解了数据倾斜的问题 ，我们最终将得到1825 *5=9105个文件，如下所示<br>1825 9105 247111074494 /user/kyuubi/hive_db/tpcds_1t_parquet.db/store_sales</p>
<p>如果我们将5改得更小，文件数也会越少，但相应的倾斜key的计算时间也会上去。<br>在我们知道那个分区键倾斜的情况下，我们也可以将入库的SQL拆成几个部分，比如我们store_sales是因为null值倾斜，我们就可以通过where ss_sold_date_sk is not null 和 where ss_sold_date_sk is null 将原始数据分成两个部分。前者可以基于分区字段进行分区，如distribute by ss_sold_date_sk;后者可以基于随机值进行分区，distribute by cast(rand() * 5 as int), 这样可以静态的将null值部分分成五个文件。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">FROM   tpcds_1t_ext.et_store_sales </span><br><span class="line">where ss_sold_date_sk is not null</span><br><span class="line">distribute by ss_sold_date_sk;</span><br><span class="line"></span><br><span class="line">FROM   tpcds_1t_ext.et_store_sales </span><br><span class="line">where ss_sold_date_sk is null</span><br><span class="line">distribute by distribute by cast(rand() * 5 as int);</span><br></pre></td></tr></table></figure>

<p>对于倾斜部分的数据，我们可以开启Spark SQL的自适应功能，spark.sql.adaptive.enabled=true来动态调整每个相当于Spark的reduce端task处理的数据量，这样我们就不需要认为的感知随机值的规模了，我们可以直接</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FROM   tpcds_1t_ext.et_store_sales </span><br><span class="line">where ss_sold_date_sk is null</span><br><span class="line">distribute by distribute by rand() ;</span><br></pre></td></tr></table></figure>

<p>然后Spark在Shuffle 阶段会自动的帮我们将数据尽量的合并成spark.sql.adaptive.shuffle.targetPostShuffleInputSize（默认64m）的大小，以减少输出端写文件线程的总量，最后减少个数。<br>对于spark.sql.adaptive.shuffle.targetPostShuffleInputSize参数而言，我们也可以设置成为dfs.block.size的大小，这样可以做到和块对齐，文件大小可以设置的最为合理。</p>
<h1 id="sparkstreaming实时写入hive后合并小文件问题"><a href="#sparkstreaming实时写入hive后合并小文件问题" class="headerlink" title="sparkstreaming实时写入hive后合并小文件问题"></a>sparkstreaming实时写入hive后合并小文件问题</h1><p>今天主要来说一下sparksql写入hive后小文件太多,影响查询性能的问题.在另外一篇博客里面也稍微提到了一下,但还是感觉要单独说一下,首先我们要知道hive里面文件的数量=executor-coresnum-executorsjob数,所以如果我们batchDuration的设置的比较小的话,每天在一个分区里面就会生成很多的小文件,我们在hive里面查询的时候就会非常的影响性能,下面介绍两种方法优化小文件:</p>
<p>第一种,可以在创建的DataFrame的时候,cache一下,然后对DataFrame进行重新分区,可以把分区设置为1,可以用reparation,当然也可以用coalesce,这两个的区别,可以看我的另外一篇博客,这个时候就会一个job产生一个文件.但是这么做就降低了写入的性能,所以数据量不是特别大的时候,还是可以用的,但是如果数据量很大,就需谨慎使用,</p>
<p>第二种方法是利用sql定时执行一下,insert overwrite table a select * from a;这个时候会覆盖表的数据达到合并小文件的目的,具体的sql下面会有.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val df = spark.createDataFrame(rowRDD, schema).cache()</span><br><span class="line">          df.coalesce(1).createOrReplaceTempView(&quot;tempTable&quot;)</span><br><span class="line">          val sq = &quot;insert into combine_data partition(day_time=&#x27;&quot; + day_time + &quot;&#x27;) select * from tempTable&quot;</span><br><span class="line">          sql(sq)</span><br><span class="line">          println(&quot;插入hive成功了&quot;)</span><br><span class="line">          df.unpersist(true)</span><br><span class="line">insert overwrite table combine_data partition (day_time=&#x27;2018-08-01&#x27;) select data,enter_time from combine_data where day_time = &#x27;2018-08-01&#x27;</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/08/Spark%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%89%EF%BC%89%E9%94%AE%E5%80%BC%E5%AF%B9%E6%93%8D%E4%BD%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo1.jpeg">
      <meta itemprop="name" content="Master Jiang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiang's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/08/Spark%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%89%EF%BC%89%E9%94%AE%E5%80%BC%E5%AF%B9%E6%93%8D%E4%BD%9C/" class="post-title-link" itemprop="url">Spark入门之基础知识（三）键值对操作</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2021-06-08 18:33:21 / 修改时间：18:34:14" itemprop="dateCreated datePublished" datetime="2021-06-08T18:33:21+08:00">2021-06-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="键值对RDD在实际生产中很常用，通常用来进行聚合计算，并且Spark对键值对RDD也提供了新的操作接口可以做更多操作，本文简单介绍一些键值对RDD的基础操作。"><a href="#键值对RDD在实际生产中很常用，通常用来进行聚合计算，并且Spark对键值对RDD也提供了新的操作接口可以做更多操作，本文简单介绍一些键值对RDD的基础操作。" class="headerlink" title="键值对RDD在实际生产中很常用，通常用来进行聚合计算，并且Spark对键值对RDD也提供了新的操作接口可以做更多操作，本文简单介绍一些键值对RDD的基础操作。"></a>键值对RDD在实际生产中很常用，通常用来进行聚合计算，并且Spark对键值对RDD也提供了新的操作接口可以做更多操作，本文简单介绍一些键值对RDD的基础操作。</h2><h3 id="如何创建Pair-RDD"><a href="#如何创建Pair-RDD" class="headerlink" title="如何创建Pair RDD"></a>如何创建Pair RDD</h3><p>1）键值对格式的数据可以直接读入，返回Pair RDD<br>2）使用map()把一个普通的RDD转化为Pair RDD<br>读取text文件，取每行文本的第一个单词做key，该行文本做value</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val lines = context.textFile(&quot;text&quot;)</span><br><span class="line">lines.map(x =&gt; (x.split(&quot; &quot;)(0), x))</span><br></pre></td></tr></table></figure>

<h3 id="Pair-RDD的转化操作"><a href="#Pair-RDD的转化操作" class="headerlink" title="Pair RDD的转化操作"></a>Pair RDD的转化操作</h3><p>Pair RDD也是RDD，对RDD可用的操作对于Pair RDD也可用。</p>
<p>首先创建出一个Pair RDD</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  val data = sc.parallelize(List(1,2,3,4,4))</span><br><span class="line">data: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[7] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val data2 = data.map(x =&gt; (x,1))</span><br><span class="line">data2: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[8] at map at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line">scala&gt; data2.take(10)</span><br><span class="line">res16: Array[(Int, Int)] = Array((1,1), (2,1), (3,1), (4,1), (4,1))</span><br></pre></td></tr></table></figure>

<p>1)reduceByKey<br>合并具有相同键的值</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;     val result1 = data2.reduceByKey((x, y) =&gt; x + y)</span><br><span class="line">result1: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[10] at reduceByKey at &lt;console&gt;:28</span><br><span class="line">scala&gt; result1.collect()</span><br><span class="line">res19: Array[(Int, Int)] = Array((4,2), (2,1), (1,1), (3,1))     </span><br></pre></td></tr></table></figure>

<p>2）groupByKey<br>对相同键的值进行分组</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;     data2.groupByKey().collect()</span><br><span class="line">res20: Array[(Int, Iterable[Int])] = Array((4,CompactBuffer(1, 1)), (2,CompactBuffer(1)), (1,CompactBuffer(1)), (3,CompactBuffer(1)))</span><br></pre></td></tr></table></figure>

<p>3）mapValues<br>对Pair RDD中的每个值应用函数而不改变键</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;     data2.mapValues(x =&gt; x * 10).collect()</span><br><span class="line">res21: Array[(Int, Int)] = Array((1,10), (2,10), (3,10), (4,10), (4,10))  </span><br></pre></td></tr></table></figure>

<p>4）flatMapValues<br>应用函数到键值对中的值上，每一个KV对的Value都会被映射成一系列的值，这些值再和K重新组合成多个KV对</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;     data2.flatMapValues(x =&gt; x to (4)).collect()</span><br><span class="line">res22: Array[(Int, Int)] = Array((1,1), (1,2), (1,3), (1,4), (2,1), (2,2), (2,3), (2,4), (3,1), (3,2), (3,3), (3,4), (4,1), (4,2), (4,3), (4,4), (4,1), (4,2), (4,3), (4,4))</span><br></pre></td></tr></table></figure>

<p>5）keys<br>返回一个仅包含键值得RDD</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;     data2.keys.collect()</span><br><span class="line">res23: Array[Int] = Array(1, 2, 3, 4, 4)      </span><br></pre></td></tr></table></figure>

<p>6）values<br>返回一个仅包含值得RDD</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;     data2.values.collect()</span><br><span class="line">res24: Array[Int] = Array(1, 1, 1, 1, 1)</span><br></pre></td></tr></table></figure>

<p>7）sortByKey<br>返回一个根据键值排序的RDD（范围分区）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;     data2.sortByKey().collect()</span><br><span class="line">res25: Array[(Int, Int)] = Array((1,1), (2,1), (3,1), (4,1), (4,1))</span><br></pre></td></tr></table></figure>

<p>8）combineByKey<br>基于键进行聚合的函数，对于分区中的每一个键值，要么是已经遍历过的，要么是还没遍历过的。</p>
<p>如果是已经遍历过的：<br>使用 mergeValue 进行处理</p>
<p>如果是还没遍历过的：<br>使用createCombiner进行处理</p>
<p>如果多个分区中都有同一个键值：<br>使用mergeCombiner进行处理</p>
<p>计算平均值的例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">val data = sc.parallelize(List(&quot;a&quot;, &quot;aa&quot;, &quot;aaa&quot;, &quot;aaa&quot;, &quot;aaaa&quot;))</span><br><span class="line">val pair = data.map(x =&gt; (x, x.length))</span><br><span class="line">val result = pair.combineByKey(</span><br><span class="line">// 对于每一个新出现的key ，保存对应的value值  同时将个数初始化为1</span><br><span class="line">  valueOfKey =&gt; (valueOfKey, 1),   </span><br><span class="line">//  对于已经出现过的key值，将新出现的key对应的value进行累加，同时个数加一</span><br><span class="line">  (tmp: (Int, Int), newValue) =&gt; (tmp._1 + newValue, tmp._2 + 1),  </span><br><span class="line">//  多个分区进行合并时，如果两个分区有相同的key值，则把两个分区统计完的总value进行相加，同时计算值的个数</span><br><span class="line">  (tmp1: (Int, Int), tmp2: (Int, Int)) =&gt; (tmp1._1 + tmp2._1, tmp1._2 + tmp2._2)  </span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; result.take(10)</span><br><span class="line">res5: Array[(String, (Int, Int))] = Array((aa,(2,1)), (aaaa,(4,1)), (a,(1,1)), (aaa,(6,2)))</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// 计算平均值</span><br><span class="line">    val avg = result.map&#123; case (key, value) =&gt; (key, value._1 / value._2.toFloat) &#125;</span><br><span class="line"></span><br><span class="line">scala&gt; avg.take(10)</span><br><span class="line">res6: Array[(String, Float)] = Array((aa,2.0), (aaaa,4.0), (a,1.0), (aaa,3.0))</span><br></pre></td></tr></table></figure>


<p>9）查看分区数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;     val partitionSize = result.partitions.size</span><br><span class="line">partitionSize: Int = 2</span><br></pre></td></tr></table></figure>
<p>可以使用repartition或者coalesce进行重分区。</p>
<p>repartition和coalesce的区别：<br>repartition的底层实现为简单调用了coalesce，并将shuffle 设置为 true。<br>如果我们目前有1000个分区，想要重分区成100个，最好调用coalesce，因为整个过程不会发生shuffle。<br>如果我们有100个分区，想要重分区成1000个，这时候需要调用repartition，调用coalesce是无效的，因为不经过shuffle无法增加分区。</p>
<p>10）groupBy<br>对RDD中每个元素应用函数，函数的结果作为该元素的key，再根据key进行分组</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val data3 = sc.parallelize(List(1,2,3,4,5,6))</span><br><span class="line">val data4 = data3.groupBy( x =&gt; (x%2))</span><br><span class="line">data4.take(10)</span><br><span class="line"></span><br><span class="line">res7: Array[(Int, Iterable[Int])] = Array((0,CompactBuffer(2, 4, 6)), (1,CompactBuffer(1, 3, 5)))</span><br></pre></td></tr></table></figure>
<p>在上面的代码中我们将奇数分为一组，偶数分为一组。</p>
<p>11）join连接操作</p>
<p>内连接</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">	val ori1 = sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;))</span><br><span class="line">    val first = ori1.map(x =&gt; (x,1))</span><br><span class="line">    val ori2 = sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;e&quot;))</span><br><span class="line">    val second = ori2.map(x =&gt; (x,2))</span><br><span class="line">    val joinResult = first.join(second)</span><br><span class="line">    joinResult.take(10)</span><br><span class="line"></span><br><span class="line">res8: Array[(String, (Int, Int))] = Array((b,(1,2)), (a,(1,2)), (c,(1,2)))      </span><br></pre></td></tr></table></figure>

<p>左外连接（右外连接同理）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">	val leftResult = first.leftOuterJoin(second)</span><br><span class="line">    leftResult.take(10)</span><br><span class="line"></span><br><span class="line">scala&gt;     leftResult.take(10)</span><br><span class="line">res9: Array[(String, (Int, Option[Int]))] = Array((d,(1,None)), (b,(1,Some(2))), (a,(1,Some(2))), (c,(1,Some(2))))</span><br></pre></td></tr></table></figure>


<p>join操作的执行过程：<br>默认情况下，连接操作会将两个数据集中所有键的哈希值都求出来，将哈希值相同的记录通过网络传输到同一台机器上，在该机器上对所有键相同的记录执行连接操作。</p>
<h3 id="Pair-RDD的行动操作"><a href="#Pair-RDD的行动操作" class="headerlink" title="Pair RDD的行动操作"></a>Pair RDD的行动操作</h3><p><img src="https://img-blog.csdnimg.cn/20191219171254168.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h6 id="Spark会为生成RDD设定好分区方式的操作"><a href="#Spark会为生成RDD设定好分区方式的操作" class="headerlink" title="Spark会为生成RDD设定好分区方式的操作"></a>Spark会为生成RDD设定好分区方式的操作</h6><p>cogroup<br>groupWith<br>join<br>leftOuterJoin<br>rightOuterJoin<br>groupByKey<br>reduceByKey<br>combineByKey<br>partitionBy<br>sort<br>mapValues<br>flatMapValues<br>filter</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Master Jiang</span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
