<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jiang&#39;s blog</title>
  
  
  <link href="https://jiangzhiqi4551.github.io/atom.xml" rel="self"/>
  
  <link href="https://jiangzhiqi4551.github.io/"/>
  <updated>2022-03-23T06:33:31.549Z</updated>
  <id>https://jiangzhiqi4551.github.io/</id>
  
  <author>
    <name>Master Jiang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>数仓中相似模型定义的几种方法</title>
    <link href="https://jiangzhiqi4551.github.io/2022/03/23/%E6%95%B0%E4%BB%93%E4%B8%AD%E7%9B%B8%E4%BC%BC%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95/"/>
    <id>https://jiangzhiqi4551.github.io/2022/03/23/%E6%95%B0%E4%BB%93%E4%B8%AD%E7%9B%B8%E4%BC%BC%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95/</id>
    <published>2022-03-23T02:43:28.000Z</published>
    <updated>2022-03-23T06:33:31.549Z</updated>
    
    <content type="html"><![CDATA[<p>随着业务的不断发展，在数仓中会慢慢产生一些相似模型，在数据治理中，我们需要找到并推动相似和可替换模型的下线，节约成本资源，优化数仓模型质量，提高模型开发效率。</p><p>本文将介绍判断相似模型的几种方法。</p><span id="more"></span><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h4 id="烟囱建设"><a href="#烟囱建设" class="headerlink" title="烟囱建设"></a>烟囱建设</h4><p>不同的项目组进行数据建设的同时，可能存在烟囱式建设，相同功能或者粒度的表可能会被重复建设。这样会造成相似模型的开发。</p><h4 id="不遵循维度建模规范"><a href="#不遵循维度建模规范" class="headerlink" title="不遵循维度建模规范"></a>不遵循维度建模规范</h4><p>在模型建设的时候，不遵循维度建模的规范，在同一个项目或者业务线，为了某些快速满足需求，往往会不考虑模型的可扩展，相同的维度的表会多次建设，造成相似模型的开发。</p><h4 id="模型评审"><a href="#模型评审" class="headerlink" title="模型评审"></a>模型评审</h4><p>没有通过规范的模型评审就开始创建新表，不遵守评审规范，可能会造成相似模型的开发。</p><h2 id="相似模型定义"><a href="#相似模型定义" class="headerlink" title="相似模型定义"></a>相似模型定义</h2><h3 id="字段包含"><a href="#字段包含" class="headerlink" title="字段包含"></a>字段包含</h3><p><strong>定义：</strong></p><p>​    有A、B两张表，A表的所有字段都包含在B表中，则认为B表是A表的可替换表。同时限制两张表存储大小差距在一定范围内。</p><p><img src="/img/similar_model/graf1.png"></p><p>如上图，如果A表的字段都在B中，假设A表B表的字段都是向量（见2.4），相似度可以采用余弦计算，B表越大则相似度越小，化简后公式如下:<br>$$<br>相似度 = \frac{\vec{A}*\vec{B}}{\Vert A \Vert\Vert B \Vert} =  \frac{size(A\bigcap B)}{\sqrt{size(A)}\sqrt{size(B)}} =  \frac{\sqrt{size(A)}}{\sqrt{size(B)}}<br>$$<br><strong>注意事项：</strong></p><ul><li>如果A表有多张相似表，取字段最多的表。</li><li>匹配通过字段名，如果字段含义相同但命名不同则不统计在内。</li><li>B表包含A表所有字段，但可能存在B表和A表的粒度不同，具体是否可以替换需业务数仓评估。</li></ul><h3 id="同输入相似"><a href="#同输入相似" class="headerlink" title="同输入相似"></a>同输入相似</h3><p><strong>定义：</strong></p><p>​    有A、B两张表，A表B表的所有输入表都是相同，并且两个表的文件数和大小相近，则认为B表是A表的可替换表。相似度可以用两张表的存储大小的比值表示：<br>$$<br>相似度 = 1 - \frac{\Vert totalsizeB - totalsizeA\Vert}{max(totalsizeA,totalsizeN)}<br>$$<br><strong>注意事项：</strong></p><ul><li>如果A表有多张相似表，取字段最多的表作为其他表的相似表。</li><li>可能存在A、B输入表相同，大小文件相似，但字段区分很大，有判断错误风险，需具体评估。</li></ul><h3 id="向量相似"><a href="#向量相似" class="headerlink" title="向量相似"></a>向量相似</h3><p><strong>定义：</strong></p><p>​    该方法融合<strong>字段包含</strong>与<strong>同输入相似</strong>的方法，并进行范围的扩大，从来源表、字段词根、表存储大小三个方面筛选其他的相似的表。</p><p><img src="/img/similar_model/graf2.png"></p><ul><li><strong>字段词根</strong>：对表的字段进行词根分隔，将所有表的词根建设词典，获得每张表所有词根在词典中的向量，如果A表和B表词根向量余弦相似度较高则认为A表B表字段词根相近，如果余弦相似度完全为1，则认为其完全相似，如果余弦相似度为0，则认为两个表词根没有交叉。</li><li><strong>来源表</strong>：与字段词根方法相似，将所有的来源表建设词典，给每个表的来源表建设向量，并计算向量余弦相似度作为其来源表相似度。</li><li><strong>存储大小</strong>：可以用两张表的存储大小的比值表示相似度。</li><li><strong>相似度</strong>：最后相似度通过上面三种相似度的加权平均进行计算。</li></ul><p>$$<br>相似度（字段与表） = \frac{\vec{A}*\vec{B}}{\Vert A \Vert\Vert B \Vert} =  \frac{size(A\bigcap B)}{\sqrt{size(A)}\sqrt{size(B)}}<br>$$</p><p>$$<br>来源表相似度 = 1 - \frac{\Vert totalsizeB - totalsizeA\Vert}{max(totalsizeA,totalsizeN)}<br>$$</p><p>$$<br>相似度 =  0.4 * 字段相似度 + 0.3 * 来源表相似度 + 0.3 * 存储大小相似度<br>$$</p><p><strong>举例：</strong></p><p>​    A表字段：user_id，photo_id，show_cnt，上游血缘表：A，B，C，大小为0.8PB</p><p>​    B表字段：user_id，photo_id，click_cnt， 上游血缘表：A，B，       大小为0.4PB</p><p>词根词典顺序为为：user，id，photo，show，click，cnt，按照该词典(不统计词频)，AB的字段向量分别如下：</p><p>​        A（1，1，1，1，0，1）</p><p>​        B（1，1，1，0，1，1）</p><p>AB的来源表向量为：</p><p>​        A（1，1，1）</p><p>​        B（1，1，0）</p><p>计算得到：AB字段余弦相似度 = 0.8，AB来源表余弦相似度 = 0.81，AB大小相似度=0.5，向量相似度为=0.71。</p><p><strong>注意事项：</strong></p><ul><li>字段词根相近的条件相比上面其他定义更加宽泛，筛选出来的相似的表可能存在完全无法替换的情况。</li><li>匹配通过字段名，如果字段含义相同但词根不同则不统计在内。</li><li>如果A表有多张相似表，取字段最多的表作为其他表的相似表。</li></ul><h3 id="综合相似"><a href="#综合相似" class="headerlink" title="综合相似"></a>综合相似</h3><p><strong>定义：</strong></p><p>上述四种方法范围有一定的包含关系：</p><p><strong>向量相似定义范围 &gt; 同输入相似粒度定义范围 &gt; 字段包含定义范围</strong></p><p>综合相似度通过上述三种定义进行加权平均，权重按照三种相似度的包含关系进行确定，定义范围越大权重越小。<br>$$<br>相似度 =  0.25 * 向量相近相似度 + 0.35 * 同输入相似粒度相似度 + 0.4 * 字段包含相似度<br>$$<br><strong>注意事项：</strong></p><ul><li>同一张表出现在不同定义中，则其相似度进行累加，其相似的定义取范围最小的定义，比如AB两个表既是同输入相似，又符合字段匹配，则认为是字段匹配。</li><li>加权平均的方法更加突出多种方法中重复出现的表，其权重可根据不同情况进行修改。</li><li>综合相似度大于xxx，则认为是相似度较高，可推进替换下线。</li><li>部分表的开发过程中有中间表，与生产表相似度极高，可以过滤掉</li><li>考虑ods表与dwd等层表的关系，在相似模型计算过程中排除ods表。</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;随着业务的不断发展，在数仓中会慢慢产生一些相似模型，在数据治理中，我们需要找到并推动相似和可替换模型的下线，节约成本资源，优化数仓模型质量，提高模型开发效率。&lt;/p&gt;
&lt;p&gt;本文将介绍判断相似模型的几种方法。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据技术&amp;数据仓库" scheme="https://jiangzhiqi4551.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>数据开发中需要注意的一些规则</title>
    <link href="https://jiangzhiqi4551.github.io/2021/11/01/%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E4%B8%AD%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E4%B8%80%E4%BA%9B%E8%A7%84%E5%88%99/"/>
    <id>https://jiangzhiqi4551.github.io/2021/11/01/%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E4%B8%AD%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E4%B8%80%E4%BA%9B%E8%A7%84%E5%88%99/</id>
    <published>2021-11-01T07:52:45.000Z</published>
    <updated>2022-03-22T11:45:18.230Z</updated>
    
    <content type="html"><![CDATA[<p>在数据开发中有一些常见的点需要注意，这些<strong>点</strong>可以理解成是数据开发中的通用规范，如果我们能在开发中遵守这些规范，将大大提高我们模型、代码的质量，这里总结一些常见的规则以及为什么要遵守这些规则的原因。</p><span id="more"></span><h4 id="1、任务代码中不能包含DDL语句"><a href="#1、任务代码中不能包含DDL语句" class="headerlink" title="1、任务代码中不能包含DDL语句"></a>1、任务代码中不能包含DDL语句</h4><p>原因：DDL规范管理减少生产风险，且使用工具管理使得元数据可管控提升数据管理能力。</p><h4 id="2、任务代码中不能包含”select-”"><a href="#2、任务代码中不能包含”select-”" class="headerlink" title="2、任务代码中不能包含”select *”"></a>2、任务代码中不能包含”select *”</h4><p>原因：减少资源消耗、提升计算性能，避免源表字段变更导致和目标表字段不一致从而造成任务运行失败</p><h4 id="3、任务代码不能使用不带种子的rand"><a href="#3、任务代码不能使用不带种子的rand" class="headerlink" title="3、任务代码不能使用不带种子的rand()"></a>3、任务代码不能使用不带种子的rand()</h4><p>原因：避免部分task失败重试分配的随机数差异造成结果不一致，导致任务失败或者数据错误</p><h4 id="4、任务代码日期分区范围限制不合理"><a href="#4、任务代码日期分区范围限制不合理" class="headerlink" title="4、任务代码日期分区范围限制不合理"></a>4、任务代码日期分区范围限制不合理</h4><p>原因：避免大范围数据扫描，造成资源过度消耗</p><h4 id="5、任务命名、模型命名不规范"><a href="#5、任务命名、模型命名不规范" class="headerlink" title="5、任务命名、模型命名不规范"></a>5、任务命名、模型命名不规范</h4><p>原因：规范管理，降低使用和维护成本</p><h4 id="6、不允许依赖topic模型"><a href="#6、不允许依赖topic模型" class="headerlink" title="6、不允许依赖topic模型"></a>6、不允许依赖topic模型</h4><p>原因：topic模型不保证产出时效，应尽量使用dwd、dws层模型，避免数据产出延迟</p><h4 id="7、不允许过度（跨层）依赖ods模型"><a href="#7、不允许过度（跨层）依赖ods模型" class="headerlink" title="7、不允许过度（跨层）依赖ods模型"></a>7、不允许过度（跨层）依赖ods模型</h4><p>原因：避免过度使用ods表，从而引起资源浪费、减小上层指标不一致风险</p><h4 id="8、低层模型不允许依赖高层模型"><a href="#8、低层模型不允许依赖高层模型" class="headerlink" title="8、低层模型不允许依赖高层模型"></a>8、低层模型不允许依赖高层模型</h4><p>原因：数据从低到高依次为：ods-&gt;dwd-&gt;dws-&gt;app|topic，避免反向依赖可以减小数据产出延迟的风险</p><h4 id="9、配置完善的数据质量监控"><a href="#9、配置完善的数据质量监控" class="headerlink" title="9、配置完善的数据质量监控"></a>9、配置完善的数据质量监控</h4><p>原因：可以配置表行数波动、主键唯一、主键为空等监控，并根据实际场景配置阻断，及时发现质量问题</p><h4 id="10、避免出现过多的质检报警"><a href="#10、避免出现过多的质检报警" class="headerlink" title="10、避免出现过多的质检报警"></a>10、避免出现过多的质检报警</h4><p>原因：确保DQC问题都能被及时发现并解决，避免发生出现问题但无人处理的状况</p><h4 id="11、避免重要基线破线"><a href="#11、避免重要基线破线" class="headerlink" title="11、避免重要基线破线"></a>11、避免重要基线破线</h4><p>原因：核心数据链路需要高优保障</p><h4 id="12、减少产生大量小文件的任务数"><a href="#12、减少产生大量小文件的任务数" class="headerlink" title="12、减少产生大量小文件的任务数"></a>12、减少产生大量小文件的任务数</h4><p>原因：减少资源浪费</p><h4 id="13、减少频繁报错的任务数"><a href="#13、减少频繁报错的任务数" class="headerlink" title="13、减少频繁报错的任务数"></a>13、减少频繁报错的任务数</h4><p>原因：减少计算资源的浪费</p><h4 id="14、减少执行时间超长的任务数"><a href="#14、减少执行时间超长的任务数" class="headerlink" title="14、减少执行时间超长的任务数"></a>14、减少执行时间超长的任务数</h4><p>原因：避免基线破线，减少计算资源的浪费</p><h4 id="15、减少产出0热度表的任务数"><a href="#15、减少产出0热度表的任务数" class="headerlink" title="15、减少产出0热度表的任务数"></a>15、减少产出0热度表的任务数</h4><p>原因：减少计算资源的浪费</p><p>0热度表判断规则：近 n 天通过调度任务访问的次数为 0，且通过 adhoc 查询的次数为 0</p><h4 id="16、减少产出数据量为0的任务数"><a href="#16、减少产出数据量为0的任务数" class="headerlink" title="16、减少产出数据量为0的任务数"></a>16、减少产出数据量为0的任务数</h4><p>原因：减少计算资源的浪费</p><p>任务检索规则：近 n 天分区无数据</p><h4 id="17、各层表需要设置合理的生命周期"><a href="#17、各层表需要设置合理的生命周期" class="headerlink" title="17、各层表需要设置合理的生命周期"></a>17、各层表需要设置合理的生命周期</h4><p>原因：减少存储资源浪费</p><h4 id="18、模型和模型字段必须添加中文注释"><a href="#18、模型和模型字段必须添加中文注释" class="headerlink" title="18、模型和模型字段必须添加中文注释"></a>18、模型和模型字段必须添加中文注释</h4><p>原因：方便数据查找</p><h4 id="19、避免0热度HDFS目录的存储"><a href="#19、避免0热度HDFS目录的存储" class="headerlink" title="19、避免0热度HDFS目录的存储"></a>19、避免0热度HDFS目录的存储</h4><p>原因：减少存储浪费</p><h4 id="20、核心指标需要配置监控"><a href="#20、核心指标需要配置监控" class="headerlink" title="20、核心指标需要配置监控"></a>20、核心指标需要配置监控</h4><p>原因：重要指标需要配置求和值大于0监控以及波动监控</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在数据开发中有一些常见的点需要注意，这些&lt;strong&gt;点&lt;/strong&gt;可以理解成是数据开发中的通用规范，如果我们能在开发中遵守这些规范，将大大提高我们模型、代码的质量，这里总结一些常见的规则以及为什么要遵守这些规则的原因。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据技术&amp;数据仓库" scheme="https://jiangzhiqi4551.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>如何在IDEA中用sbt打包运行Spark程序</title>
    <link href="https://jiangzhiqi4551.github.io/2021/10/28/%E5%A6%82%E4%BD%95%E5%9C%A8IDEA%E4%B8%AD%E7%94%A8sbt%E6%89%93%E5%8C%85%E8%BF%90%E8%A1%8CSpark%E7%A8%8B%E5%BA%8F/"/>
    <id>https://jiangzhiqi4551.github.io/2021/10/28/%E5%A6%82%E4%BD%95%E5%9C%A8IDEA%E4%B8%AD%E7%94%A8sbt%E6%89%93%E5%8C%85%E8%BF%90%E8%A1%8CSpark%E7%A8%8B%E5%BA%8F/</id>
    <published>2021-10-28T08:22:56.000Z</published>
    <updated>2022-01-13T06:34:16.523Z</updated>
    
    <content type="html"><![CDATA[<p>记录一下如何在 IDEA 中通过 sbt 打包写好的 Spark 代码，并通过 spark-submit 提交运行</p><span id="more"></span><p>一开始尝试在 terminal 中直接用 sbt package 进行打包，结果并没有 jar 包被打出来。。第一次用 sbt 也不知道是哪里出了问题，之后用如下方法成功打包。</p><p>文件目录机构如下：</p><p><img src="/img/20211028_img1.png" alt="截屏2021-10-28 下午4.36.49"></p><p>SimpleApp 代码如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @author jiangzhiqi &lt;jiangzhiqi@kuaishou.com&gt;</span></span><br><span class="line"><span class="comment"> *         Created on 2021-10-28</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SimpleApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> logFile = <span class="string">&quot;/Users/jiangzhiqi/Project/spark-3.2.0-bin-hadoop3.2/README.md&quot;</span></span><br><span class="line">    <span class="keyword">val</span> session = <span class="type">SparkSession</span>.builder().appName(<span class="string">&quot;simple application&quot;</span>).getOrCreate()</span><br><span class="line">    <span class="keyword">val</span> logData = session.read.textFile(logFile).cache()</span><br><span class="line">    <span class="keyword">val</span> numAs = logData.filter(line =&gt; line.contains(<span class="string">&quot;a&quot;</span>)).count()</span><br><span class="line">    <span class="keyword">val</span> numBs = logData.filter(line =&gt; line.contains(<span class="string">&quot;b&quot;</span>)).count()</span><br><span class="line"></span><br><span class="line">    println(<span class="string">s&quot;Line with a : <span class="subst">$numAs</span> , b : <span class="subst">$numBs</span>&quot;</span>)</span><br><span class="line">    session.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>点击 File –&gt; Project Structure –&gt; Artifacts –&gt; 加号 –&gt; JAR –&gt; From modules with dependencies</p><p><img src="/img/20211028_img2.png" alt="截屏2021-10-28 下午4.40.11"></p><p>Module 处选择你的项目，Main class 选择入口，点击 OK</p><p><img src="/img/20211028_img3.png" alt="截屏2021-10-28 下午4.41.59"></p><p>选中所有外部依赖，点击 减号 删除，只保留项目 jar 包</p><p><img src="/img/20211028_img4.png" alt="截屏2021-10-28 下午4.42.35"></p><p>选择左上角 Build -&gt; Build Artifacts -&gt; Build ，即可在 out 文件夹中找到打包好的 jar 包。</p><p>最后在 terminal 中提交 jar 包即可。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /Users/jiangzhiqi/Project/spark-3.2.0-bin-hadoop3.2</span><br><span class="line"></span><br><span class="line">➜  spark-3.2.0-bin-hadoop3.2 ./bin/spark-submit \                                     </span><br><span class="line">--master local \</span><br><span class="line">--class &quot;SimpleApp&quot; \</span><br><span class="line">/Users/jiangzhiqi/IdeaProjects/SbtExampleProject/out/artifacts/SbtExampleProject_jar/SbtExampleProject.jar</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;记录一下如何在 IDEA 中通过 sbt 打包写好的 Spark 代码，并通过 spark-submit 提交运行&lt;/p&gt;</summary>
    
    
    
    <category term="Spark" scheme="https://jiangzhiqi4551.github.io/categories/Spark/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark在bin目录下执行spark-shell提示command not found: spark-shell解决方案</title>
    <link href="https://jiangzhiqi4551.github.io/2021/10/27/Spark%E5%9C%A8bin%E7%9B%AE%E5%BD%95%E4%B8%8B%E6%89%A7%E8%A1%8Cspark-shell%E6%8F%90%E7%A4%BAcommand-not-found-spark-shell%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <id>https://jiangzhiqi4551.github.io/2021/10/27/Spark%E5%9C%A8bin%E7%9B%AE%E5%BD%95%E4%B8%8B%E6%89%A7%E8%A1%8Cspark-shell%E6%8F%90%E7%A4%BAcommand-not-found-spark-shell%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</id>
    <published>2021-10-27T13:10:32.000Z</published>
    <updated>2021-10-27T13:36:08.620Z</updated>
    
    <content type="html"><![CDATA[<p>今天学习 Spark 时发现一个问题，在 bin 目录下执行 spark-shell 无法启动 Spark，但是在 spark 目录下执行 ./bin/spark-shell 却可以，记录下原因和解决方案。</p><span id="more"></span><p>bin 目录下启动失败</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  bin pwd</span><br><span class="line">/Users/xxx/Project/spark-3.2.0-bin-hadoop3.2/bin</span><br><span class="line">➜  bin spark-shell</span><br><span class="line">zsh: command not found: spark-shell</span><br></pre></td></tr></table></figure><p>在 spark 主目录下执行成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark-3.2.0-bin-hadoop3.2 pwd</span><br><span class="line">/Users/xxx/Project/spark-3.2.0-bin-hadoop3.2</span><br><span class="line">➜  spark-3.2.0-bin-hadoop3.2 ./bin/spark-shell</span><br><span class="line">WARNING: An illegal reflective access operation has occurred # 启动成功</span><br></pre></td></tr></table></figure><h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p><a href="https://stackoverflow.com/questions/35620687/unable-to-run-spark-shell-from-bin">Stack Overflow原文链接</a></p><p>简单来说，终端会到 <strong>$PATH</strong> 中寻找可执行文件，如果你要执行的程序不在 <strong>$PATH</strong> 中，就需要指明这个可执行文件在文件系统中的绝对路径。</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>如果不希望每次都 cd 到 spark 目录下再执行命令启动 spark，我们就需要把 SPARK_HOME 添加到 $PATH 中。</p><p>首先先在 spark 目录下执行 pwd 记下这个路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  spark-3.2.0-bin-hadoop3.2 pwd</span><br><span class="line">/Users/xxx/Project/spark-3.2.0-bin-hadoop3.2</span><br></pre></td></tr></table></figure><p>接下来到根目录下修改 <strong>.bash_profile</strong> 文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd ~</span><br><span class="line"># 如果还没有 .bash_profile ，则需要先创建文件</span><br><span class="line"># touch .bash_profile</span><br><span class="line">vim .bash_profile </span><br></pre></td></tr></table></figure><p>接下来复制粘贴如下内容，把第一条中的路径换成自己的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_HOME=/Users/xxx/Project/spark-3.2.0-bin-hadoop3.2</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure><p>最后执行 source 命令使修改生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure><p>再次执行 spark-shell 就会发现 spark 已经成功启动了~</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;今天学习 Spark 时发现一个问题，在 bin 目录下执行 spark-shell 无法启动 Spark，但是在 spark 目录下执行 ./bin/spark-shell 却可以，记录下原因和解决方案。&lt;/p&gt;</summary>
    
    
    
    <category term="排坑记录" scheme="https://jiangzhiqi4551.github.io/categories/%E6%8E%92%E5%9D%91%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>Git使用指南&amp;常用命令</title>
    <link href="https://jiangzhiqi4551.github.io/2021/10/25/Git%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    <id>https://jiangzhiqi4551.github.io/2021/10/25/Git%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</id>
    <published>2021-10-25T08:28:31.000Z</published>
    <updated>2021-10-26T10:33:41.781Z</updated>
    
    <content type="html"><![CDATA[<p>简单介绍如何利用 Git 进行多人合作开发。</p><span id="more"></span><p><a href="http://git-scm.com/docs">Git 完整命令手册地址</a></p><p><a href="%5Bgithub-git-cheat-sheet.pdf%5D(https://www.runoob.com/manual/github-git-cheat-sheet.pdf)">PDF 版命令手册</a></p><h1 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h1><h4 id="查看当前配置信息"><a href="#查看当前配置信息" class="headerlink" title="查看当前配置信息"></a>查看当前配置信息</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --list</span><br></pre></td></tr></table></figure><h4 id="创建仓库"><a href="#创建仓库" class="headerlink" title="创建仓库"></a>创建仓库</h4><p>1、cd 到创建好的目录下执行 git init</p><p>2、直接执行 git init targrt_directory</p><h4 id="克隆远程服务器上的仓库到本地"><a href="#克隆远程服务器上的仓库到本地" class="headerlink" title="克隆远程服务器上的仓库到本地"></a>克隆远程服务器上的仓库到本地</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone username@host:/path/to/repository</span><br></pre></td></tr></table></figure><h4 id="显示所有远程仓库"><a href="#显示所有远程仓库" class="headerlink" title="显示所有远程仓库"></a>显示所有远程仓库</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote -v</span><br></pre></td></tr></table></figure><p>修改远程仓库名：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote rename old_name new_name  </span><br></pre></td></tr></table></figure><h4 id="从远程仓库下载分支和数据"><a href="#从远程仓库下载分支和数据" class="headerlink" title="从远程仓库下载分支和数据"></a>从远程仓库下载分支和数据</h4><p><strong>origin</strong> 为<strong>远程地址</strong>的别名。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 从名为 origin 的远程上拉取名为 master 的分支到本地分支 origin/master 中</span></span><br><span class="line">git fetch origin master</span><br></pre></td></tr></table></figure><h4 id="从远端仓库提取数据并合并到当前分支"><a href="#从远端仓库提取数据并合并到当前分支" class="headerlink" title="从远端仓库提取数据并合并到当前分支"></a>从远端仓库提取数据并合并到当前分支</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 从 origin/master 分支提取数据，合并到当前分支</span></span><br><span class="line">git merge origin/master</span><br></pre></td></tr></table></figure><p><img src="https://www.runoob.com/wp-content/uploads/2015/03/main-qimg-00a6b5a8ec82400657444504c4d4d1a7.png"></p><h4 id="创建新分支并切换到新分支"><a href="#创建新分支并切换到新分支" class="headerlink" title="创建新分支并切换到新分支"></a>创建新分支并切换到新分支</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -b new_branch_name</span><br></pre></td></tr></table></figure><p>只创建分支：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch (branchname)</span><br></pre></td></tr></table></figure><h4 id="从当前分支切换回主分支"><a href="#从当前分支切换回主分支" class="headerlink" title="从当前分支切换回主分支"></a>从当前分支切换回主分支</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout master</span><br></pre></td></tr></table></figure><h4 id="删除分支"><a href="#删除分支" class="headerlink" title="删除分支"></a>删除分支</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -d  branch_name_you_want_to_delete</span><br></pre></td></tr></table></figure><h4 id="合并远端代码到当前分支"><a href="#合并远端代码到当前分支" class="headerlink" title="合并远端代码到当前分支"></a>合并远端代码到当前分支</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin mster</span><br></pre></td></tr></table></figure><h4 id="合并其他分支代码到当前分支"><a href="#合并其他分支代码到当前分支" class="headerlink" title="合并其他分支代码到当前分支"></a>合并其他分支代码到当前分支</h4><p>当前位于分支branch_A，执行如下命令后，会将branch_B中的改动合并到branch_A中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git merge branch_B</span><br></pre></td></tr></table></figure><h4 id="添加文件到暂存区"><a href="#添加文件到暂存区" class="headerlink" title="添加文件到暂存区"></a>添加文件到暂存区</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">添加一个或多个文件到暂存区</span></span><br><span class="line">git add [file1] [file2] </span><br><span class="line"><span class="meta">#</span><span class="bash">添加指定目录到暂存区，包括子目录</span></span><br><span class="line">git add [dir]</span><br><span class="line"><span class="meta">#</span><span class="bash">添加当前目录下的所有文件到暂存区</span></span><br><span class="line">git add .</span><br></pre></td></tr></table></figure><h4 id="比较文件不同"><a href="#比较文件不同" class="headerlink" title="比较文件不同"></a>比较文件不同</h4><p><strong>git diff</strong> 比较文件在暂存区和工作区的差异</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">显示暂存区和工作区的差异</span></span><br><span class="line">git diff [file]</span><br><span class="line"><span class="meta">#</span><span class="bash">显示暂存区和上一次提交(commit)的差异</span></span><br><span class="line">git diff --cached [file]</span><br><span class="line"><span class="meta">#</span><span class="bash">查看已缓存的与未缓存的所有改动</span></span><br><span class="line">git diff HEAD</span><br><span class="line"><span class="meta">#</span><span class="bash">显示两次提交之间的差异</span></span><br><span class="line">git diff [first-branch]...[second-branch]</span><br></pre></td></tr></table></figure><h4 id="操作失误时，替换掉本地改动"><a href="#操作失误时，替换掉本地改动" class="headerlink" title="操作失误时，替换掉本地改动"></a>操作失误时，替换掉本地改动</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -- &lt;filename&gt;</span><br></pre></td></tr></table></figure><h4 id="丢弃本地的所有改动与提交"><a href="#丢弃本地的所有改动与提交" class="headerlink" title="丢弃本地的所有改动与提交"></a>丢弃本地的所有改动与提交</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git fetch origin  #到服务器获取最新版本</span><br><span class="line">git reset --hard origin/master  #将本地主分支指向最新版本</span><br></pre></td></tr></table></figure><h4 id="设置提交代码时的用户信息"><a href="#设置提交代码时的用户信息" class="headerlink" title="设置提交代码时的用户信息"></a>设置提交代码时的用户信息</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;your_name&quot;</span><br><span class="line">git config --global user.email your_email_address</span><br></pre></td></tr></table></figure><h4 id="版本回退"><a href="#版本回退" class="headerlink" title="版本回退"></a>版本回退</h4><p>文件A、B都被git add后，git reset A 后再 git commit，只会将文件B 的改动commit到本地仓库。</p><p><strong>git reset</strong> 用于重置暂存区的文件与上一次的提交(commit)保持一致，工作区文件内容保持不变。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 回退所有内容到上一个版本</span></span><br><span class="line">git reset HEAD^</span><br><span class="line"><span class="meta">#</span><span class="bash"> 回退 hello.php 文件的版本到上一个版本</span></span><br><span class="line">git reset HEAD^ hello.php  </span><br><span class="line"><span class="meta">#</span><span class="bash"> 回退到指定版本</span></span><br><span class="line">git  reset  052e         </span><br></pre></td></tr></table></figure><p><strong>–soft</strong> 参数用于回退到某个版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 回退上上上一个版本</span></span><br><span class="line">git reset --soft HEAD~3</span><br></pre></td></tr></table></figure><p><strong>–hard</strong> 参数<strong>撤销工作区中所有未提交</strong>的修改内容，将<strong>暂存区与工作区</strong>都回到上一次版本，并删除之前的<strong>所有</strong>信息提交（<strong>即会删除回退点之前的所有信息</strong>）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 回退上上上一个版本</span></span><br><span class="line">git reset –hard HEAD~3  </span><br><span class="line"><span class="meta">#</span><span class="bash"> 回退到某个版本回退点之前的所有信息</span></span><br><span class="line">git reset –hard bae128  </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将本地的状态回退到和远程的一样</span> </span><br><span class="line">git reset --hard origin/master    </span><br></pre></td></tr></table></figure><ul><li>HEAD 、HEAD~0 表示当前版本</li><li>HEAD^ 、HEAD~1 表示上一个版本</li><li>HEAD^^ 、HEAD^2 表示上上一个版本</li><li>HEAD^3 表示上上上一个版本</li></ul><h4 id="删除文件"><a href="#删除文件" class="headerlink" title="删除文件"></a>删除文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 将文件从暂存区和工作区中删除</span></span><br><span class="line">git rm &lt;file&gt;  </span><br><span class="line">git rm runoob.txt </span><br><span class="line"><span class="meta">#</span><span class="bash"> 强行从 暂存区 和 工作区 中删除</span></span><br><span class="line">git rm -f runoob.txt</span><br><span class="line"><span class="meta">#</span><span class="bash"> 从 暂存区 移除，但仍保留在 工作目录 中</span></span><br><span class="line">git rm --cached &lt;file&gt;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除该目录下的所有文件和子目录</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 1、进入待删除的目录中</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2、执行：</span></span><br><span class="line">git rm –r * </span><br></pre></td></tr></table></figure><h4 id="删除远程仓库"><a href="#删除远程仓库" class="headerlink" title="删除远程仓库"></a>删除远程仓库</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote rm [仓库名]</span><br></pre></td></tr></table></figure><p>查看历史提交记录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">git log </span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看历史提交记录的简洁版</span></span><br><span class="line">git log --oneline</span><br><span class="line"><span class="meta">#</span><span class="bash"> 以拓扑图展示分支、合并记录</span></span><br><span class="line">git log --graph</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看指定用户的提交记录</span></span><br><span class="line">git log --author=username</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看指定文件的修改记录</span></span><br><span class="line">git blame &lt;file&gt;</span><br></pre></td></tr></table></figure><h1 id="Git-基本原理"><a href="#Git-基本原理" class="headerlink" title="Git 基本原理"></a>Git 基本原理</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ul><li><strong>工作区：</strong>就是电脑里能看到的目录。</li><li><strong>暂存区：</strong>英文叫 stage 或 index。一般存放在 <strong>.git</strong> 目录下的 index 文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）</li><li><strong>版本库：</strong>工作区有一个隐藏目录 <strong>.git</strong>，这个不算工作区，而是 Git 的版本库。</li></ul><p><img src="https://www.runoob.com/wp-content/uploads/2015/02/1352126739_7909.jpg"></p><p>图中左侧为工作区，右侧为版本库。在版本库中标记为 “index” 的区域是暂存区</p><ul><li>“HEAD” 实际是指向 master 分支的一个”游标”。所以图示的命令中出现 HEAD 的地方可以用 master 来替换。</li><li>图中的 objects 标识的区域为 Git 的对象库，里面包含了创建的各种对象及内容。</li><li>当对工作区修改（或新增）的文件执行 <strong>git add</strong> 命令时，暂存区的目录树被更新，同时工作区修改（或新增）的文件内容被写入到对象库中的一个新的对象中，而该对象的ID被记录在暂存区的文件索引中。</li><li>当执行提交操作（git commit）时，暂存区的目录树写到版本库（对象库）中，master 分支会做相应的更新。即 master 指向的目录树就是提交时暂存区的目录树。</li><li>当执行 <strong>git reset HEAD</strong> 命令时，暂存区的目录树会被重写，被 master 分支指向的目录树所替换，但是工作区不受影响。</li><li>当执行 git rm –cached <file> 命令时，会直接从暂存区删除文件，工作区则不做出改变。</li><li>当执行 git checkout . 或者 git checkout – file_name 命令时，会用<strong>暂存区</strong>全部或指定的文件替换<strong>工作区</strong>的文件，会清除工作区中未添加到暂存区中的改动。</li><li>当执行 <strong>git checkout HEAD .</strong> 或者 <strong>git checkout HEAD file_name</strong> 命令时，会用 HEAD 指向的 <strong>master 分支</strong>中的全部或者部分文件替换<strong>暂存区</strong>和以及<strong>工作区</strong>中的文件。不但会清除<strong>工作区</strong>中未提交的改动，也会清除<strong>暂存区</strong>中未提交的改动。</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;简单介绍如何利用 Git 进行多人合作开发。&lt;/p&gt;</summary>
    
    
    
    <category term="计算机基础知识" scheme="https://jiangzhiqi4551.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    
  </entry>
  
  <entry>
    <title>Python学习笔记(一)</title>
    <link href="https://jiangzhiqi4551.github.io/2021/10/12/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%80/"/>
    <id>https://jiangzhiqi4551.github.io/2021/10/12/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%80/</id>
    <published>2021-10-12T11:00:23.000Z</published>
    <updated>2021-10-25T06:47:55.177Z</updated>
    
    <content type="html"><![CDATA[<p>初学 Python，记录 Python 的基础知识。</p><span id="more"></span><p>1、使用三引号(<strong>‘’’</strong> 或 <strong>“””</strong>)可以指定一个多行字符串。</p><p>例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">paragraph = <span class="string">&quot;&quot;&quot;这是一个段落，</span></span><br><span class="line"><span class="string">可以由多行组成&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>2、字符串的截取的语法格式如下：<strong>变量[头下标:尾下标:步长]<strong>，遵循</strong>左闭右开</strong>原则，str[0:2] 是<strong>不包含</strong>第 3 个字符的。</p><p>例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">str</span>)                 <span class="comment"># 输出字符串</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">str</span>[<span class="number">0</span>:-<span class="number">1</span>])           <span class="comment"># 输出第一个到倒数第二个的所有字符</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">str</span>[<span class="number">0</span>])              <span class="comment"># 输出字符串第一个字符</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">str</span>[<span class="number">2</span>:<span class="number">5</span>])            <span class="comment"># 输出从第三个开始到第五个的字符</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">str</span>[<span class="number">2</span>:])             <span class="comment"># 输出从第三个开始后的所有字符</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">str</span>[<span class="number">1</span>:<span class="number">5</span>:<span class="number">2</span>])          <span class="comment"># 输出从第二个开始到第五个且每隔一个的字符（步长为2）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">str</span> * <span class="number">2</span>)             <span class="comment"># 输出字符串两次</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">str</span> + <span class="string">&#x27;你好&#x27;</span>)         <span class="comment"># 连接字符串</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;hello\nrunoob&#x27;</span>)      <span class="comment"># 换行符会被转义</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">r&#x27;hello\nrunoob&#x27;</span>)     <span class="comment"># 在字符串前面添加一个 r，表示原始字符串，不会发生转义</span></span><br></pre></td></tr></table></figure><p>3、</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 换行输出 </span></span><br><span class="line"><span class="built_in">print</span>( x )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不换行输出</span></span><br><span class="line"><span class="built_in">print</span>( x, end=<span class="string">&quot; &quot;</span> )</span><br></pre></td></tr></table></figure><p>4、</p><p>Python3 中有六个标准的数据类型：</p><ul><li><p>Number（数字）<strong>不可变数据</strong>，支持 <strong>int、float、bool、complex（复数）</strong></p><p><em>bool 是 int 的子类，True 和 False 可以和数字相加，</em> <strong>True==1、False==0</strong></p><p>数值的除法包含两个运算符：**/** 返回一个浮点数，**//** 返回一个整数。</p><p>在混合计算时，Python会把整型转换成为浮点数。 4.3 - 2 = 2.3</p></li><li><p>String（字符串）<strong>不可变数据</strong></p><p><img src="https://static.runoob.com/wp-content/uploads/123456-20200923-1.svg"></p></li><li><p>List（列表）<strong>可变数据</strong></p><p>列表中元素的类型可以不相同</p><p>列表同样可以被索引和截取，返回一个新列表，截取的语法格式：变量[头下标:尾下标]，索引值以 <strong>0</strong> 为开始值，**-1** 为从末尾的开始位置。如果第三个参数为负数表示逆向读取</p><p><img src="https://www.runoob.com/wp-content/uploads/2014/08/list_slicing1_new1.png"></p></li><li><p>Tuple（元组）<strong>不可变数据</strong></p><p>元组（tuple）与列表类似，不同之处在于元组的元素不能修改。元组写在小括号 <strong>()</strong> 里，元组中的元素类型也可以不相同</p><p>虽然tuple的元素不可改变，但它可以包含可变的对象，比如list列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tup1 = ()    <span class="comment"># 空元组</span></span><br><span class="line">tup2 = (<span class="number">20</span>,) <span class="comment"># 一个元素，需要在元素后添加逗号</span></span><br></pre></td></tr></table></figure><p>元组也可以使用+操作符进行拼接。</p></li><li><p>Set（集合）<strong>可变数据</strong></p><p>可以使用大括号 <strong>{ }</strong> 或者 <strong>set()</strong> 函数创建集合</p><p>创建一个空集合必须用 <strong>set()</strong> 而不是 <strong>{ }</strong></p><p>基本功能是进行成员关系测试和删除重复元素。</p></li><li><p>Dictionary（字典）<strong>可变数据</strong></p><p>字典是无序的对象集合，字典当中的元素是通过键来存取的，而不是通过偏移存取。</p><p>字典用 <strong>{ }</strong> 标识，它是一个无序的 <strong>键(key) : 值(value)</strong> 的集合，键(key)必须使用不可变类型。</p><p>创建空字典使用 **{ }**。</p></li></ul><p>5、</p><p><strong>逻辑运算符</strong></p><p>​            x and y    布尔”与” - 如果 x 为 False，x and y 返回 x 的值，否则返回 y 的计算值。</p><p>​            x or y    布尔”或” - 如果 x 是 True，它返回 x 的值，否则它返回 y 的计算值。</p><p>​            not x    布尔”非” - 如果 x 为 True，返回 False 。如果 x 为 False，它返回 True。</p><p>6、</p><p><strong>身份运算符（比较两个对象的存储单元）</strong></p><p>​            is：<strong>x is y</strong>, 类似 <strong>id(x) == id(y)</strong> , 如果引用的是同一个对象则返回 True，否则返回 False</p><p>​            is not：<strong>x is not y</strong> ， 类似 **id(a) != id(b)**。如果引用的不是同一个对象则返回结果 True，否则返回 False。</p><p>​             <a href="https://www.runoob.com/python/python-func-id.html">id()</a> 函数用于获取对象内存地址。</p><p><strong>is 与 == 区别：</strong></p><p>​            is 用于判断两个变量引用对象是否为同一个， == 用于判断引用变量的值是否相等。</p><p>7、</p><p><strong>字符串格式化</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> (<span class="string">&quot;我叫 %s 今年 %d 岁!&quot;</span> % (<span class="string">&#x27;小明&#x27;</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><p> %s     格式化字符串</p><p>%d     格式化整数</p><p>8、</p><p><strong>删除列表中的元素</strong></p><p>del list[2]</p><p>9、</p><p><strong>向集合添加元素：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s.add( x )</span><br><span class="line"></span><br><span class="line">s.update( x ) <span class="comment"># 参数可以是列表，元组，字典</span></span><br></pre></td></tr></table></figure><p><strong>移除集合元素</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s.remove( x ) <span class="comment"># x不存在会报错</span></span><br><span class="line"></span><br><span class="line">s.discard( x ) <span class="comment">#ｘ不存在不会报错</span></span><br></pre></td></tr></table></figure><p> <strong>随机删除集合元素</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s.pop() </span><br></pre></td></tr></table></figure><p>10、</p><p><strong>Python函数传参</strong></p><p>如果参数是<strong>不可变类型</strong>：传递的只是 a 的值，没有影响 a 对象本身。如果在 fun(a) 内部修改 a 的值，则是新生成一个 a 的对象。</p><p>如果参数是<strong>可变类型</strong>：则是将 la 真正的传过去，修改后 fun 外部的 la 也会受影响</p><p>11、</p><p><strong>遍历技巧</strong></p><p>在字典中遍历时，关键字和对应的值可以使用 items() 方法同时解读出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>knights = &#123;<span class="string">&#x27;gallahad&#x27;</span>: <span class="string">&#x27;the pure&#x27;</span>, <span class="string">&#x27;robin&#x27;</span>: <span class="string">&#x27;the brave&#x27;</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> k, v <span class="keyword">in</span> knights.items():</span><br><span class="line"><span class="meta">... </span>    <span class="built_in">print</span>(k, v)</span><br><span class="line">...</span><br><span class="line">gallahad the pure</span><br><span class="line">robin the brave</span><br></pre></td></tr></table></figure><p>在序列中遍历时，索引位置和对应值可以使用 enumerate() 函数同时得到：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> i, v <span class="keyword">in</span> <span class="built_in">enumerate</span>([<span class="string">&#x27;tic&#x27;</span>, <span class="string">&#x27;tac&#x27;</span>, <span class="string">&#x27;toe&#x27;</span>]):</span><br><span class="line"><span class="meta">... </span>    <span class="built_in">print</span>(i, v)</span><br><span class="line">...</span><br><span class="line"><span class="number">0</span> tic</span><br><span class="line"><span class="number">1</span> tac</span><br></pre></td></tr></table></figure><p>同时遍历两个或更多的序列，可以使用 zip() 组合：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>questions = [<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;quest&#x27;</span>, <span class="string">&#x27;favorite color&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>answers = [<span class="string">&#x27;lancelot&#x27;</span>, <span class="string">&#x27;the holy grail&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> q, a <span class="keyword">in</span> <span class="built_in">zip</span>(questions, answers):</span><br><span class="line"><span class="meta">... </span>    <span class="built_in">print</span>(<span class="string">&#x27;What is your &#123;0&#125;?  It is &#123;1&#125;.&#x27;</span>.<span class="built_in">format</span>(q, a))</span><br><span class="line">...</span><br><span class="line">What <span class="keyword">is</span> your name?  It <span class="keyword">is</span> lancelot.</span><br></pre></td></tr></table></figure><p>要反向遍历一个序列，首先指定这个序列，然后调用 reversed() 函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">10</span>, <span class="number">2</span>)):</span><br><span class="line"><span class="meta">... </span>    <span class="built_in">print</span>(i)</span><br></pre></td></tr></table></figure><p>要按顺序遍历一个序列，使用 sorted() 函数返回一个已排序的序列，并不修改原值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>basket = [<span class="string">&#x27;apple&#x27;</span>, <span class="string">&#x27;orange&#x27;</span>, <span class="string">&#x27;apple&#x27;</span>, <span class="string">&#x27;pear&#x27;</span>, <span class="string">&#x27;orange&#x27;</span>, <span class="string">&#x27;banana&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> f <span class="keyword">in</span> <span class="built_in">sorted</span>(<span class="built_in">set</span>(basket)):</span><br><span class="line"><span class="meta">... </span>    <span class="built_in">print</span>(f)</span><br></pre></td></tr></table></figure><p>12、</p><p><strong>文件读写</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">open</span>(filename, mode)</span><br></pre></td></tr></table></figure><p><img src="https://www.runoob.com/wp-content/uploads/2013/11/2112205-861c05b2bdbc9c28.png"></p><p>13、</p><p><strong>JSON</strong></p><ul><li><strong>json.dumps():</strong> 对数据进行编码。</li><li><strong>json.loads():</strong> 对数据进行解码。</li></ul><p><img src="https://www.runoob.com/wp-content/uploads/2016/04/json-dumps-loads.png"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;初学 Python，记录 Python 的基础知识。&lt;/p&gt;</summary>
    
    
    
    <category term="计算机基础知识" scheme="https://jiangzhiqi4551.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    
  </entry>
  
  <entry>
    <title>Hive中如何对数组合并去重</title>
    <link href="https://jiangzhiqi4551.github.io/2021/10/12/Hive%E4%B8%AD%E5%A6%82%E4%BD%95%E5%AF%B9%E6%95%B0%E7%BB%84%E5%90%88%E5%B9%B6%E5%8E%BB%E9%87%8D/"/>
    <id>https://jiangzhiqi4551.github.io/2021/10/12/Hive%E4%B8%AD%E5%A6%82%E4%BD%95%E5%AF%B9%E6%95%B0%E7%BB%84%E5%90%88%E5%B9%B6%E5%8E%BB%E9%87%8D/</id>
    <published>2021-10-12T08:38:05.000Z</published>
    <updated>2022-01-12T08:11:39.477Z</updated>
    
    <content type="html"><![CDATA[<p>如果需要对 Hive 中的数组进行合并，并且去重，比较通用的方式是先用侧视图把数组打开（lateral view explode），再用 collect_set 函数处理。除此之外，某些情况下可以直接使用 brickhouse 下的函数，更简单快捷。</p><span id="more"></span><p>直接使用 brickhouse 下的 array_union 函数即可。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> brickhouse.array_union(<span class="keyword">array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>), <span class="keyword">array</span>(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">_c0</span><br><span class="line">[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br></pre></td></tr></table></figure><h2 id="Brickhouse-简介"><a href="#Brickhouse-简介" class="headerlink" title="Brickhouse 简介"></a>Brickhouse 简介</h2><p><a href="https://github.com/klout/brickhouse">Brickhouse GitHub</a></p><h4 id="什么是-Brickhouse？"><a href="#什么是-Brickhouse？" class="headerlink" title="什么是 Brickhouse？"></a>什么是 Brickhouse？</h4><p>Brickhouse 是 GitHub 上的开源项目，包含了 Hive 下的一系列 UDF，可以帮助开发者提高 Hive 的开发效率。</p><p>Brickhouse 中主要包含了以下几类的 UDF，其中有些函数与 Hive 的自带函数功能重叠，目前笔者也只用到了collect下的函数。</p><ul><li><strong>bloom</strong>：Hadoop 布隆过滤器实现</li><li><strong>collect</strong>：处理 map 和 array</li><li><strong>date</strong>：时间相关函数</li><li> <strong>hll</strong>：即 HyperLogLog</li><li> <strong>json</strong>：在 Hive struct 和 JSON 间进行转换</li><li><strong>sanity</strong>：生产环境下进行 Hive 管理和健全性检查的工具</li><li> <strong>timeseries</strong>：时间序列</li></ul><h4 id="再介绍几个函数的用法"><a href="#再介绍几个函数的用法" class="headerlink" title="再介绍几个函数的用法"></a>再介绍几个函数的用法</h4><ul><li>brickhouse.append_array：向数组末尾添加新元素</li></ul><p>&ensp; &ensp; &ensp; &ensp; 用法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    visit_list_30d</span><br><span class="line">    ,brickhouse.append_array(visit_list_30d , <span class="string">&#x27;test&#x27;</span>)</span><br><span class="line"><span class="keyword">from</span> xxx</span><br><span class="line"><span class="keyword">where</span> xxx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">visit_list_30d</span><br><span class="line">[&quot;zhangsan&quot;]</span><br><span class="line">_c1</span><br><span class="line">[&quot;zhangsan&quot;,&quot;test&quot;]</span><br></pre></td></tr></table></figure><ul><li>brickhouse.add_days：日期增加函数，日期可以直接使用YYYYMMDD格式</li></ul><p>&ensp; &ensp; &ensp; &ensp; 用法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> brickhouse.add_days(<span class="string">&#x27;20210101&#x27;</span> , <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">_c0</span><br><span class="line"><span class="number">20210104</span></span><br></pre></td></tr></table></figure><ul><li>brickhouse.array_index：返回数组下标为n时的元素值</li></ul><p>&ensp; &ensp; &ensp; &ensp; 用法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> brickhouse.array_index(<span class="keyword">array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>), <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">_c0</span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure><ul><li>brickhouse.array_range：返回下标从n到m的子数组</li></ul><p>&ensp; &ensp; &ensp; &ensp; 用法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> brickhouse.array_range(<span class="keyword">array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>), <span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">_c0</span><br><span class="line">[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br></pre></td></tr></table></figure><ul><li>brickhouse.bloom：根据输入的key构建布隆过滤器</li></ul><ul><li>brickhouse.bloom_and：返回两个布隆过滤器的逻辑与，即两个布隆过滤器中值的交集</li></ul><ul><li><p>brickhouse.bloom_contains：判断布隆过滤器中是否可能包含输入元素</p></li><li><p>brickhouse.bloom_not：返回布隆过滤器的逻辑非，即不在bloom1中的value</p></li><li><p>brickhouse.bloom_or：返回两个布隆过滤器的逻辑或，即bloom1和bloom2的并集</p></li></ul><ul><li>brickhouse.cast_array：数据类型转化，可以理解为 Hive cast函数的优化</li></ul><p>&ensp; &ensp; &ensp; &ensp; 用法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> brickhouse.cast_array(<span class="keyword">array</span>(<span class="string">&#x27;1&#x27;</span>,<span class="string">&#x27;2&#x27;</span>,<span class="string">&#x27;3&#x27;</span>) , <span class="string">&#x27;int&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">_c0</span><br><span class="line">[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br></pre></td></tr></table></figure><ul><li>brickhouse.cast_map：将 map 的 key 和 value 转化为 string </li></ul><p>&ensp; &ensp; &ensp; &ensp; 用法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> brickhouse.cast_map(map(<span class="number">1</span> , <span class="number">2</span> , <span class="number">3</span> , <span class="number">4</span> ))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">_c0</span><br><span class="line">&#123;&quot;1&quot;:&quot;2&quot;,&quot;3&quot;:&quot;4&quot;&#125;</span><br></pre></td></tr></table></figure><ul><li>brickhouse.json_split：将 JSON 数组拆分成独立的 JSON 字符串</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;如果需要对 Hive 中的数组进行合并，并且去重，比较通用的方式是先用侧视图把数组打开（lateral view explode），再用 collect_set 函数处理。除此之外，某些情况下可以直接使用 brickhouse 下的函数，更简单快捷。&lt;/p&gt;</summary>
    
    
    
    <category term="Hive" scheme="https://jiangzhiqi4551.github.io/categories/Hive/"/>
    
    
  </entry>
  
  <entry>
    <title>将DB数据同步到数据仓库中的两种方法（简介）</title>
    <link href="https://jiangzhiqi4551.github.io/2021/10/12/%E5%B0%86DB%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E5%88%B0%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%AD%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%EF%BC%88%E7%AE%80%E4%BB%8B%EF%BC%89/"/>
    <id>https://jiangzhiqi4551.github.io/2021/10/12/%E5%B0%86DB%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E5%88%B0%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%AD%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%EF%BC%88%E7%AE%80%E4%BB%8B%EF%BC%89/</id>
    <published>2021-10-12T08:12:53.000Z</published>
    <updated>2021-10-12T08:18:20.429Z</updated>
    
    <content type="html"><![CDATA[<p>在数据仓库中，ODS 层数据一般来源于客户端的日志和业务数据库中的数据，对于 MYSQL 等关系型数据库，我们需要将库中数据同步到 Hive 中进行查询与分析。这里简单介绍两种同步的方法。</p><span id="more"></span><h2 id="低性能同步法"><a href="#低性能同步法" class="headerlink" title="低性能同步法"></a>低性能同步法</h2><ol><li>直连 MySQL 并 select 表中数据</li><li>将 select 的结果存储到本地作为中间存储</li><li>将中间存储的文件 load 进 Hive 表中</li></ol><p>这种方法实现起来比较简单，但也有较大的缺陷，主要有以下几点：</p><ol><li>随着业务规模不断扩大，每次 select 得到的数据量不断加大，将结果存本地再 load 进 Hive 这种方式会造成性能问题，无法满足下游数仓生产的时效性要求</li><li>从 MySQL 中 select 大量数据，会对 MySQL 造成过大压力，可能会影响线上服务</li><li>Hive 无法有效支持 MySQL 中的 update、delete 操作</li></ol><p>为了解决上述问题，大多数互联网公司都采用了<strong>实时Binlog采集 + 离线处理Binlog还原业务数据</strong>的方案，由于Binlog是流式产生的，因此将数据处理需求由每天同步一次均摊到实时流中，减小了对MySQL的访问压力，减小了对线上性能可能造成的影响。另外，因为Binlog是MySQL的二进制日志，其记录了MySQL中发生的所有数据变更，因此也能够记录数据的 insert、update、delete操作，通过一些语义方面的处理，能做到数据的精准还原。</p><h2 id="利用-MySQL-Binlog-的同步法"><a href="#利用-MySQL-Binlog-的同步法" class="headerlink" title="利用 MySQL Binlog 的同步法"></a>利用 MySQL Binlog 的同步法</h2><p>这一部分仅介绍大致思路</p><h4 id="Binlog实时采集"><a href="#Binlog实时采集" class="headerlink" title="Binlog实时采集"></a>Binlog实时采集</h4><p>采用开源项目Canal，从MySQL实时拉取Binlog，并进行适当解析，将采集到的数据暂存到kafka上供下游消费</p><h4 id="还原MySQL数据"><a href="#还原MySQL数据" class="headerlink" title="还原MySQL数据"></a>还原MySQL数据</h4><p>采用开源项目Camus，将kafka上的Binlog数据拉取到hive上。</p><p>对每张ODS表，先采取直接select的方式从MySQL获取一份存量数据，再基于存量数据和每天的增量产生的Binlog日志做合并，从而还原出业务数据。</p><h4 id="merge-流程举例"><a href="#merge-流程举例" class="headerlink" title="merge 流程举例"></a>merge 流程举例</h4><p>整个流程分为以下两个步骤：</p><ol><li>将Binlog中的数据存储到delta表中</li><li>将delta表中的数据和存量数据做基于主键的merge</li></ol><p>当一条数据一天发生多次变更时，delta表只保存最新的更新状态。</p><p>在merge的过程中，通过主键关联后，如果一条数据既存在于delta表中，又存在于存量表中，说明数据发生了更新，需要取delta表中的数据作为最终结果，如果只存在于存量表中，不在delta表中，则说明该数据没有被更新过，取存量表中的数据作为最终结果。</p><p><strong>示例1</strong>：</p><p><img src="https://img-blog.csdnimg.cn/0afd82f415544bb883f219ad5dadf300.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70#pic_center"></p><p><strong>示例2</strong>：</p><p><img src="https://img-blog.csdnimg.cn/2897bd4aec3b4b66b6247ad7025c20bb.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70#pic_center"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在数据仓库中，ODS 层数据一般来源于客户端的日志和业务数据库中的数据，对于 MYSQL 等关系型数据库，我们需要将库中数据同步到 Hive 中进行查询与分析。这里简单介绍两种同步的方法。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据技术&amp;数据仓库" scheme="https://jiangzhiqi4551.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>数据治理过程中的几种优化手段</title>
    <link href="https://jiangzhiqi4551.github.io/2021/10/12/%E6%95%B0%E6%8D%AE%E6%B2%BB%E7%90%86%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E5%87%A0%E7%A7%8D%E4%BC%98%E5%8C%96%E6%89%8B%E6%AE%B5/"/>
    <id>https://jiangzhiqi4551.github.io/2021/10/12/%E6%95%B0%E6%8D%AE%E6%B2%BB%E7%90%86%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E5%87%A0%E7%A7%8D%E4%BC%98%E5%8C%96%E6%89%8B%E6%AE%B5/</id>
    <published>2021-10-12T07:56:42.000Z</published>
    <updated>2021-10-12T08:03:52.923Z</updated>
    
    <content type="html"><![CDATA[<p>当企业数仓发展到一定规模后，就会出现模型迭代速度远慢于业务发展速度的现象，为了满足业务需要，DE 往往会面向需求进行快速开发，由此造成了烟囱表、数据安全、计算冗余、存储浪费等各种各样的问题。为了解决这些问题，保持离线数仓长期向好的健康状态，需要开展数据治理工作。</p><p>但在多数公司中，开展数据治理的原因往往是为了节约成本，减少计算开销和存储开销。。。下面总结一些可以优化计算和存储开销的常见方法。</p><span id="more"></span><h3 id="1-删除-TOPk-存储的无下游，且近-N-天无更新的-ODS-表"><a href="#1-删除-TOPk-存储的无下游，且近-N-天无更新的-ODS-表" class="headerlink" title="1. 删除 TOPk 存储的无下游，且近 N 天无更新的 ODS 表"></a>1. 删除 TOPk 存储的无下游，且近 N 天无更新的 ODS 表</h3><p>业务风险高，数据不可恢复，需要跟业务确认</p><h3 id="2-如果存在新旧模型（新旧链路）并行运行的现象，将旧模型（老链路下线）"><a href="#2-如果存在新旧模型（新旧链路）并行运行的现象，将旧模型（老链路下线）" class="headerlink" title="2. 如果存在新旧模型（新旧链路）并行运行的现象，将旧模型（老链路下线）"></a>2. 如果存在新旧模型（新旧链路）并行运行的现象，将旧模型（老链路下线）</h3><p>业务风险中，可能存在回溯数据的需求</p><h3 id="3-删除-TOPk-存储的-DWD-表-N-年前的数据"><a href="#3-删除-TOPk-存储的-DWD-表-N-年前的数据" class="headerlink" title="3. 删除 TOPk 存储的 DWD 表 N 年前的数据"></a>3. 删除 TOPk 存储的 DWD 表 N 年前的数据</h3><p>业务风险中，可能存在回溯数据的需求</p><h3 id="4-对未执行完-EC-的表进行-EC-操作"><a href="#4-对未执行完-EC-的表进行-EC-操作" class="headerlink" title="4. 对未执行完 EC 的表进行 EC 操作"></a>4. 对未执行完 EC 的表进行 EC 操作</h3><p>业务风险低</p><h3 id="5-缩短-Hive-表-EC-的时间窗口（如缩短为7天）"><a href="#5-缩短-Hive-表-EC-的时间窗口（如缩短为7天）" class="headerlink" title="5. 缩短 Hive 表 EC 的时间窗口（如缩短为7天）"></a>5. 缩短 Hive 表 EC 的时间窗口（如缩短为7天）</h3><p>业务风险低</p><h3 id="6-埋点模型优化治理"><a href="#6-埋点模型优化治理" class="headerlink" title="6. 埋点模型优化治理"></a>6. 埋点模型优化治理</h3><ul><li>对埋点模型本身进行优化，剪裁无效列，减少原始日志数据上报量，并去清理历史数据，将无效字段删除。</li><li>针对原始上报数据，上报数据量 TOP 的埋点，与业务侧确认后，以埋点合并、埋点下线等方式进行优化，减少每天新增的数据量。</li><li>将埋点平台未注册的埋点逐步下线。</li><li>开展埋点生命周期管理。对于功能迭代、活动结束、埋点重新设计场景，可以主动下线埋点，同步客户端删除代码。对于长时间无访问且无人维护的埋点，可以由埋点平台主动识别并下线。</li></ul><p>业务风险低，清除历史数据会消耗一定的计算资源。</p><h3 id="7-分离业务日志与技术日志"><a href="#7-分离业务日志与技术日志" class="headerlink" title="7. 分离业务日志与技术日志"></a>7. 分离业务日志与技术日志</h3><p>业务风险低</p><h3 id="8-MySQL-同步到-HIve-的快照表只保留近-N-天数据"><a href="#8-MySQL-同步到-HIve-的快照表只保留近-N-天数据" class="headerlink" title="8. MySQL 同步到 HIve 的快照表只保留近 N 天数据"></a>8. MySQL 同步到 HIve 的快照表只保留近 N 天数据</h3><p>业务风险中，可能存在回溯数据的需求</p><h3 id="9-TOP存储的中间表，生命周期设为-N-天"><a href="#9-TOP存储的中间表，生命周期设为-N-天" class="headerlink" title="9. TOP存储的中间表，生命周期设为 N 天"></a>9. TOP存储的中间表，生命周期设为 N 天</h3><p>业务风险低</p><h3 id="10-删除不持续产出的-TOP-表"><a href="#10-删除不持续产出的-TOP-表" class="headerlink" title="10. 删除不持续产出的 TOP 表"></a>10. 删除不持续产出的 TOP 表</h3><p>业务风险低</p><h3 id="11-TOP-存储的-ODS-表冷存"><a href="#11-TOP-存储的-ODS-表冷存" class="headerlink" title="11. TOP 存储的 ODS 表冷存"></a>11. TOP 存储的 ODS 表冷存</h3><p>业务风险低</p><h3 id="12-DWD-表技术埋点裁剪"><a href="#12-DWD-表技术埋点裁剪" class="headerlink" title="12. DWD 表技术埋点裁剪"></a>12. DWD 表技术埋点裁剪</h3><p>业务风险高，可能需要切换使用方式</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;当企业数仓发展到一定规模后，就会出现模型迭代速度远慢于业务发展速度的现象，为了满足业务需要，DE 往往会面向需求进行快速开发，由此造成了烟囱表、数据安全、计算冗余、存储浪费等各种各样的问题。为了解决这些问题，保持离线数仓长期向好的健康状态，需要开展数据治理工作。&lt;/p&gt;
&lt;p&gt;但在多数公司中，开展数据治理的原因往往是为了节约成本，减少计算开销和存储开销。。。下面总结一些可以优化计算和存储开销的常见方法。&lt;/p&gt;</summary>
    
    
    
    <category term="大数据技术&amp;数据仓库" scheme="https://jiangzhiqi4551.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
    
    <category term="数据治理" scheme="https://jiangzhiqi4551.github.io/tags/%E6%95%B0%E6%8D%AE%E6%B2%BB%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>Hive宏(Macro)简介</title>
    <link href="https://jiangzhiqi4551.github.io/2021/10/12/Hive%E5%AE%8F-Macro-%E7%AE%80%E4%BB%8B/"/>
    <id>https://jiangzhiqi4551.github.io/2021/10/12/Hive%E5%AE%8F-Macro-%E7%AE%80%E4%BB%8B/</id>
    <published>2021-10-12T07:48:31.000Z</published>
    <updated>2021-10-12T07:54:09.852Z</updated>
    
    <content type="html"><![CDATA[<p>最近在开发中用到了宏，用起来的确很方便，大大简化了代码，提高了可读性和可维护性，在此记录下宏的用法，以后有机会多更新一些宏的复杂用法。</p><span id="more"></span><h2 id="什么是-Macro"><a href="#什么是-Macro" class="headerlink" title="什么是 Macro"></a>什么是 Macro</h2><p>在进行 Hive SQL 开发的时候，有一些逻辑需要反复使用，如果代码中每次都把所需逻辑复制粘贴一遍，不仅会造成代码冗余，还会增加维护难度，这时使用 Hive宏 对逻辑进行提炼，可以起到简化代码，提高开发效率，提升程序可读性的效果。</p><h2 id="如何使用-Macro"><a href="#如何使用-Macro" class="headerlink" title="如何使用 Macro"></a>如何使用 Macro</h2><p>宏的使用主要分为：创建、使用、销毁这三步，其中最重要的是创建部分，定义好后可以将宏像函数一样使用，最后的销毁是出于开发规范的需要。下面举一个简单例子：</p><h4 id="1、创建"><a href="#1、创建" class="headerlink" title="1、创建"></a><strong>1、创建</strong></h4><p>创建宏的标准语法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> TEMPORARY MACRO macro_name([col_name col_type, ...]) expression;</span><br></pre></td></tr></table></figure><p>创建一个简单的字符串拼接宏：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> temporary macro test_macro(input_string string)</span><br><span class="line">concat(<span class="string">&#x27;hello&#x27;</span> , <span class="string">&#x27; &#x27;</span> , input_string);</span><br></pre></td></tr></table></figure><p>创建一个两个参数的求和宏：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> TEMPORARY MACRO simple_add (x <span class="type">int</span>, y <span class="type">int</span>) x <span class="operator">+</span> y;</span><br></pre></td></tr></table></figure><h4 id="2、使用"><a href="#2、使用" class="headerlink" title="2、使用"></a><strong>2、使用</strong></h4><p>类似于UDF，直接传参使用：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> test_macro(<span class="string">&#x27;test&#x27;</span>);</span><br></pre></td></tr></table></figure><h4 id="3、销毁"><a href="#3、销毁" class="headerlink" title="3、销毁"></a><strong>3、销毁</strong></h4><p> 标准语法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> TEMPORARY MACRO [IF <span class="keyword">EXISTS</span>] macro_name;</span><br></pre></td></tr></table></figure><h2 id="为什么要使用-Macro"><a href="#为什么要使用-Macro" class="headerlink" title="为什么要使用 Macro"></a>为什么要使用 Macro</h2><ol><li>Macro 的开发过程比 UDF 更简单</li><li>与 UDF 相比，因为 UDF 是用 Java 代码开发的，堆变量的内存回收不受开发者控制，由虚拟机决定，并且实际生产中需要将 UDF 嵌套到 HQL 中，因此如果表的数据量比较大，存在发生 OOM 的风险。</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近在开发中用到了宏，用起来的确很方便，大大简化了代码，提高了可读性和可维护性，在此记录下宏的用法，以后有机会多更新一些宏的复杂用法。&lt;/p&gt;</summary>
    
    
    
    <category term="Hive" scheme="https://jiangzhiqi4551.github.io/categories/Hive/"/>
    
    
  </entry>
  
  <entry>
    <title>Hadoop纠删码（Erasure Coding）简介</title>
    <link href="https://jiangzhiqi4551.github.io/2021/10/12/Hadoop%E7%BA%A0%E5%88%A0%E7%A0%81%EF%BC%88Erasure-Coding%EF%BC%89%E7%AE%80%E4%BB%8B/"/>
    <id>https://jiangzhiqi4551.github.io/2021/10/12/Hadoop%E7%BA%A0%E5%88%A0%E7%A0%81%EF%BC%88Erasure-Coding%EF%BC%89%E7%AE%80%E4%BB%8B/</id>
    <published>2021-10-12T06:28:32.000Z</published>
    <updated>2021-10-12T06:58:03.855Z</updated>
    
    <content type="html"><![CDATA[<p>摘要：相比于三副本机制，纠删码可以大大提高存储效率，本文将简介纠删码的原理。</p><span id="more"></span><h1 id="三副本机制"><a href="#三副本机制" class="headerlink" title="三副本机制"></a>三副本机制</h1><h3 id="1、什么是三副本"><a href="#1、什么是三副本" class="headerlink" title="1、什么是三副本"></a>1、什么是三副本</h3><p>默认情况下，HDFS会使用三副本机制来保证数据可用性，第一个副本存放在本地机架节点上，另一个副本存放在同一机架的另一个节点上，第三个副本存放在在不同机架的节点上。三副本机制可以减少机架间的数据传输，提高写操作的效率，保证数据的可用性，因为机架错误的概率要小于节点出错的概率</p><h3 id="2、三副本带来的问题"><a href="#2、三副本带来的问题" class="headerlink" title="2、三副本带来的问题"></a>2、三副本带来的问题</h3><p>三副本会造成存储空间200%的额外开销，还会在其他资源上造成浪费，比如在写数据时会造成额外的带宽消耗。尤其对于冷数据，因为查询的频率很低，第二个和第三个副本很少被访问，却依然占用了同样大小的存储空间，在数据治理的过程中，我们需要着重关注这部分数据，能删掉的要及时下线，但对于一些必须保留却很少使用的冷数据（如某个已结束的促销活动），最好能有一种办法替代三副本，达到既能保证数据可用性，又能降低存储开销的效果。由此我们引入了 Erasure Coding（简称EC）技术。</p><h1 id="Erasure-Coding"><a href="#Erasure-Coding" class="headerlink" title="Erasure Coding"></a>Erasure Coding</h1><h3 id="1、什么是-Erasure-Coding"><a href="#1、什么是-Erasure-Coding" class="headerlink" title="1、什么是 Erasure Coding"></a>1、什么是 Erasure Coding</h3><p>纠删码技术（Erasure coding）简称 EC，是一种编码容错技术。EC 最早用于通信行业，用于数据传输中的数据恢复。它通过对数据进行分块，然后计算出校验数据，使得各个部分的数据产生关联性。当一部分数据块丢失时，可以通过剩余的数据块和校验块计算出丢失的数据块。<br>在 Hadoop 3.0 中，Cloudera 与 Intel 的工程师在 HDFS-7285 下启动了 EC 项目，引入了纠删码技术，它可以提高 50% 以上的存储利用率，并且保证数据的可靠性。</p><h3 id="2、Erasure-Coding-的原理"><a href="#2、Erasure-Coding-的原理" class="headerlink" title="2、Erasure Coding 的原理"></a>2、Erasure Coding 的原理</h3><p>EC 的基本思想是将ｋ块原始的数据元素通过一定的编码计算，得到 ｍ 块校验元素。对于这 ｋ+ｍ 块元素，当其中任意的 ｍ 块元素出错（包括数据和校验出错），均可以通过对应的重构算法恢复出原来的ｋ块数据。生成校验的过程被成为编码（encoding），恢复丢失数据块的过程被称为解码（decoding）。</p><h4 id=""><a href="#" class="headerlink" title=""></a></h4><h5 id="目前-HDFS-中内置的-EC-编解码算法有以下三种："><a href="#目前-HDFS-中内置的-EC-编解码算法有以下三种：" class="headerlink" title="目前 HDFS 中内置的 EC 编解码算法有以下三种："></a>目前 HDFS 中内置的 EC 编解码算法有以下三种：</h5><ul><li>XOR</li><li>RS-LEGACY（旧版Reed-Solomon，性能较差）</li><li>RS（新版Reed-Solomon，性能是RS-LEGACY的5倍）</li></ul><h5 id="HDFS-的内置-EC-策略有以下几种："><a href="#HDFS-的内置-EC-策略有以下几种：" class="headerlink" title="HDFS 的内置 EC 策略有以下几种："></a>HDFS 的内置 EC 策略有以下几种：</h5><ul><li>RS-3-2-1024k</li><li>RS-6-3-1024k</li><li>RS-10-4-1024k</li><li>RS-LEGACY-6-3-1024k</li><li>XOR-2-1-1024k</li></ul><h5 id="以-RS-6-3-1024k-为例解释下该-EC-策略的定义："><a href="#以-RS-6-3-1024k-为例解释下该-EC-策略的定义：" class="headerlink" title="以 RS-6-3-1024k 为例解释下该 EC 策略的定义："></a>以 RS-6-3-1024k 为例解释下该 EC 策略的定义：</h5><ul><li>RS 代表使用 RS 算法压缩</li><li>6 代表有 6 个原始数据块，分别存储在 6 个 DataNode 节点</li><li>3 代表有 3 个校验块，存储在 3 个 DataNode 节点，可以容忍最多 3 个块丢失的情况</li><li>1024k 代表每个块大小为 1024k</li></ul><p>因为实际生产中几乎不会用到 RS-LEGACY，因此这里只介绍 <strong>XOR</strong> 算法和 <strong>RS</strong> 算法。</p><h4 id="1）XOR码"><a href="#1）XOR码" class="headerlink" title="1）XOR码"></a>1）XOR码</h4><p>XOR 编解码是 EC 算法的最简单实现。XOR 代表异或运算，异或运算的一个特点就是：<strong>如果对一个值连续做两次 XOR，会返回这个值本身</strong>。以下图为例：</p><p><img src="https://img-blog.csdnimg.cn/05a1bf6c88d5489fb4e6c3d35a843d48.png#pic_center"></p><p><strong>数据 X</strong>： 0 0 1 1<br><strong>数据 Y</strong>： 0 1 0 1<br><strong>X XOR Y</strong>： 0 1 1 0<br>数据 X 与数据 Y 通过异或运算生成校验码，当丢失任意一份数据时，如丢失数据 X 的第一位 0，可以通过 Y 的第一位 0 和校验码的第一位 0 还原出 X 的数据（因为只有 0 和 0 异或结果才为 0）。</p><p>但 XOR 码的弊端也很明显，当丢失的数据超过一位时，如数据 X 和数据 Y 的第一位都丢失，则无法进行还原（因为无法确定 X 和 Y 是都为 0 还是都为 1）。</p><p>由此可见，XOR 码适合 <strong>可保证错误数较少的场景</strong>，但实际生产中并不能保证每次只有一个数据出错，要考虑多个数据丢失的情况，要尽量提高容错率，因此还要了解第二种编解码方法——RS码。</p><h4 id="2）RS码"><a href="#2）RS码" class="headerlink" title="2）RS码"></a>2）RS码</h4><p>RS 码全名为 Reed-Solomon Codes，中文名称里德所罗门码。RS 码通过线性代数计算生成多个奇偶校验块，因此可以承受数据多处出错，所以生产上我们一般都是用 RS 码。<br>RS需要配置2个参数，k和m。</p><ul><li>k 代表数据块的个数</li><li>m 代表校验块的个数</li></ul><p>RS（k，m）通过将 k 个数据块的向量与生成矩阵（GT）相乘来实现，相乘得到一个码字（codeword）向量，码字向量由 k 个数据块和 m 个校验块构成，如下图所示，即为 RS（4，2）</p><p><img src="https://img-blog.csdnimg.cn/6e2ad5fc9f6841f88311000befe7807e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70#pic_center"></p><p>如果某个数据块丢失，可以用生成矩阵（GT）的逆矩阵乘以码字向量来还原出丢失的数据，RS(k，m)]最多可容忍 m 个块（包括数据块和校验块）丢失。</p><p><strong>举个例子：</strong></p><p><strong>1、</strong> 假设现在有三个数据块组成一个向量，如下图所示：</p><img src="https://img-blog.csdnimg.cn/74b7e5a9f7f84aa9b6c152d6a54f8648.png" alt="在这里插入图片描述" style="zoom:150%;" /><p>对应的生成矩阵如下图所示：</p><img src="https://img-blog.csdnimg.cn/7b464f09c2e74feb9c8d902286b467d3.png" style="zoom:150%;" /><p>根据公式，我们可以计算出码字：</p><p><img src="https://img-blog.csdnimg.cn/623c2ad472464072b3b5a29539e7aed8.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70#pic_center"></p><p><strong>2、</strong> 现在我们假设码字中丢失了两份数据 1 和 14 ，我们该怎么还原出原始数据？</p><p>因为 1 和 14 丢失，所以我们可以去掉生成矩阵中的第一行和倒数第二行得到新的生成矩阵GT2，假设它们未参与</p><p>计算，用现有的 2 3 36 构造等式如下：</p><p><img src="https://img-blog.csdnimg.cn/1c0af0dabdfe466f86b46c2da9d293c3.png#pic_center" alt="在这里插入图片描述"></p><p>3、 在此基础上，为了还原出原始数据 1 2 3 ，只需要在等式两边同时乘新生成矩阵GT2的逆矩阵，即可还原出数据。</p><p><img src="https://img-blog.csdnimg.cn/4b3f493ec2e84f33b802e0efb93c2d41.png#pic_center"></p><p>因为 逆矩阵 × 矩阵 = 单位矩阵，因此只需要计算右边的等式：<br>-4 * 2 + -9 * 3 + 1 *36 = 1<br>1 * 2 + 0 * 3 + 0 * 36 = 2<br>0 * 2 + 1 * 3 + 0 * 36 = 3<br>即可还原出原始数据。</p><p>在 RS 算法中，通过设置 k 和 m 的值，可以灵活调整 RS 算法下的数据持久性和存储效率，m 代表可以容忍的同时发生的故障数，k / (k+m) 代表了当前的存储效率。</p><p>HDFS 中的 RS-6-3-1024k、RS-10-4-1024k 和三副本相比，既可以同时容忍 3~4 个故障，又能大大降低存储开销，下图展示了 0 副本、3 副本、XOR、RS 算法的存储效率：</p><p><img src="https://img-blog.csdnimg.cn/ae05a54179064a3eb456f27408409dee.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70"></p><h4 id="3、EC在-Hadoop-上的实现"><a href="#3、EC在-Hadoop-上的实现" class="headerlink" title="3、EC在 Hadoop 上的实现"></a>3、EC在 Hadoop 上的实现</h4><p>传统模式下，HDFS中的文件是由一个个块（block）组成，而在 EC 模式下，文件的组成单位变成了 block group，以 RS(10,4) 为例，一个 block group 包含了 10 个数据块，4个校验块。而 EC 模式下这些数据在 HDFS 上有着两种布局方式。</p><h5 id="连续布局"><a href="#连续布局" class="headerlink" title="连续布局"></a>连续布局</h5><p>文件数据被依次写入块中，一个块写满之后再写入下一个块，这种分布方式称为连续布局。如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/cb72c81076fd4b9b8fdfefcd4025bb74.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70#pic_center"></p><p>优点：</p><ul><li><p>容易实现</p></li><li><p>容易和三副本策略互相转换</p></li></ul><p>缺点：</p><p>连续布局虽然实现简单，但缺点也极其明显，就是<strong>不适合存储小文件</strong>，仅适用于文件非常大的情况。假设我们使用 RS(10,4)，在面对小文件的情况下，只需要第一个数据块就足够存储数据了，但此时仍要写入 4 个校验块，存储开销达到 400%，还不如用三副本。。此外，连续布局也仅适用于<strong>离线</strong>或<strong>后台EC</strong>，否则客户端需要缓存 GB 级的数据块以计算奇偶校验。</p><p>因此，只有在我们确定文件非常大，能连续、完整的写入多个数据块时，才能达到节省存储的效果。</p><h5 id="条形布局"><a href="#条形布局" class="headerlink" title="条形布局"></a>条形布局</h5><p>条（stripe）是由若干个相同大小的单元（cell）构成的序列。文件数据被依次写入条的各个单元中，当一个条写满之后再写入下一个条，一个条的不同单元位于不同的数据块中。这种分布方式称为条形布局。条形布局如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/845b1d4d18ee4655b8c5b642eb9d7a24.webp?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70#pic_center"></p><p>条形布局的优缺点正好与连续布局相反</p><p>优点：</p><ul><li>因为 cell 较小因此可以做在线EC，客户端缓存数据量较少，可以直接写入纠删码数据。</li><li>大小文件均适用</li></ul><p>缺点：</p><p>条形布局的缺点也很明显，首先原本一个节点上的块现在被分割到了多个不同的节点上，当 MapReduce 作业执行时将增大网络开销，失去数据本地性的优势，其次当进行编解码、DataNode 挂了计算丢失数据时都会消耗更多的 CPU，客户端在读写文件时，也需要同时连接所有涉及的 DataNode 读写数据块和校验块。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>EC 虽然存在着与三副本转化麻烦、会增大 CPU 及网络开销等问题，但当数仓中存在大量冷数据时，EC 仍然是一种</p><p>数据治理的高效手段，能大大降低集群的存储消耗同时保证数据的可靠性，在我们的生产实践中，EC 为我们带来了巨大的收益。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;摘要：相比于三副本机制，纠删码可以大大提高存储效率，本文将简介纠删码的原理。&lt;/p&gt;</summary>
    
    
    
    <category term="Hadoop" scheme="https://jiangzhiqi4551.github.io/categories/Hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>Hive中like与rlike的区别</title>
    <link href="https://jiangzhiqi4551.github.io/2021/10/11/Hive%E4%B8%ADlike%E4%B8%8Erlike%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>https://jiangzhiqi4551.github.io/2021/10/11/Hive%E4%B8%ADlike%E4%B8%8Erlike%E7%9A%84%E5%8C%BA%E5%88%AB/</id>
    <published>2021-10-11T15:04:16.000Z</published>
    <updated>2021-10-11T15:22:23.596Z</updated>
    
    <content type="html"><![CDATA[<p>Hive 中可以用 like 和 rlike 进行模糊匹配，like 采用的是 SQL 通配符，而 rlike 采用的是正则匹配。</p><span id="more"></span><h1 id="like"><a href="#like" class="headerlink" title="like"></a>like</h1><p>% 代替 0 或多个字符</p><p>_ 代替一个字符</p><p><strong>举个例子：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 返回值为 TRUE</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">&#x27;aaa&#x27;</span> <span class="keyword">like</span> <span class="string">&#x27;%a%&#x27;</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 返回值为 TRUE</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">&#x27;aaa&#x27;</span> <span class="keyword">like</span> <span class="string">&#x27;_a_&#x27;</span></span><br></pre></td></tr></table></figure><h1 id="rlike"><a href="#rlike" class="headerlink" title="rlike"></a>rlike</h1><p>rlike 采用正则表达式，以下总结几个常用的</p><p>\： 转义用，序列 \ 匹配 \</p><p>^： 匹配输入字符串开始的位置</p><p>$： 匹配输入字符串结尾的位置</p><p>*： 零次或多次匹配前面的字符或子表达式</p><p>?： 零次或一次匹配前面的字符或子表达式</p><p>x|y： 匹配 x 或 y</p><p>[xyz]： 字符集。匹配包含的任一字符</p><p>\w： 匹配任何字类字符，包括下划线</p><p>.： 匹配所有单个字符</p><p>此外，还可以用 regexp 替换 rlike，用 not regexp 替换 not rlike ，效果一样。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 返回值为 TRUE</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">&#x27;hello&#x27;</span> regexp <span class="string">&#x27;^he&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 返回值为 FALSE</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">&#x27;hello&#x27;</span> regexp <span class="string">&#x27;^e&#x27;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">-- 返回值为 TRUE</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">&#x27;hello&#x27;</span> regexp <span class="string">&#x27;llo$&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 返回值为 TRUE</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">&#x27;hello&#x27;</span> regexp <span class="string">&#x27;hello|world&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 返回值为 TRUE</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">&#x27;hello&#x27;</span> regexp <span class="string">&#x27;h[\\w]llo&#x27;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">-- 返回值为 TRUE</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">&#x27;hello&#x27;</span> rlike <span class="string">&#x27;h.llo&#x27;</span> </span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;Hive 中可以用 like 和 rlike 进行模糊匹配，like 采用的是 SQL 通配符，而 rlike 采用的是正则匹配。&lt;/p&gt;</summary>
    
    
    
    <category term="Hive" scheme="https://jiangzhiqi4551.github.io/categories/Hive/"/>
    
    
  </entry>
  
  <entry>
    <title>数据仓库工具箱笔记 - 第二章</title>
    <link href="https://jiangzhiqi4551.github.io/2021/06/28/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%B7%A5%E5%85%B7%E7%AE%B1%E7%AC%94%E8%AE%B002/"/>
    <id>https://jiangzhiqi4551.github.io/2021/06/28/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%B7%A5%E5%85%B7%E7%AE%B1%E7%AC%94%E8%AE%B002/</id>
    <published>2021-06-28T02:58:40.000Z</published>
    <updated>2021-10-11T04:20:31.007Z</updated>
    
    <content type="html"><![CDATA[<p>《数据仓库工具箱》第二章读书笔记</p><span id="more"></span><h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><h3 id="维度模型设计四步骤"><a href="#维度模型设计四步骤" class="headerlink" title="维度模型设计四步骤"></a>维度模型设计四步骤</h3><p>​        1、选择业务过程</p><p>​                每个业务过程对应数仓总线矩阵的一行。</p><p>​        2、确认粒度</p><p>​                粒度用于确定事实表中的行表示什么。</p><p>​        3、确认维度</p><p>​                维度描述业务过程事件所涉及的“谁、什么、何处、何时、为什么、如何”等背景。</p><p>​        4、确认事实</p><p>​                事实涉及来自业务过程事件的度量，基本以数值表示。</p><p>​                一个事实表中的每一行，都与按照事实表粒度描述的度量事件存在一对一的关系。</p><h1 id="事实表技术基础"><a href="#事实表技术基础" class="headerlink" title="事实表技术基础"></a>事实表技术基础</h1><h4 id="事实表结构"><a href="#事实表结构" class="headerlink" title="事实表结构"></a>事实表结构</h4><p>​        事实表行对应一个度量事件，事实表的设计完全依赖于物理活动，不受最终产出报表的影响。事实表还包含外键（用于维度关联）、可选的退化维度键、日期时间戳。</p><h4 id="可加、半可加、不可加事实"><a href="#可加、半可加、不可加事实" class="headerlink" title="可加、半可加、不可加事实"></a>可加、半可加、不可加事实</h4><p>​        可加性数字度量：可以按任意维度汇总</p><p>​        半可加性数字度量：可以对某些维度汇总（例如差额）</p><p>​        不可加性数字度量：完全不可加（例如比率）</p><h4 id="一致性事实"><a href="#一致性事实" class="headerlink" title="一致性事实"></a>一致性事实</h4><p>​        比较、计算不同事实表中的事实，必须保证事实的技术定义一致，如果一致，应该采用相同的命名。</p><h4 id="事务事实表"><a href="#事务事实表" class="headerlink" title="事务事实表"></a>事务事实表</h4><p>​        一行对应空间上或时间上某点的度量事件。</p><h4 id="周期快照事实表"><a href="#周期快照事实表" class="headerlink" title="周期快照事实表"></a>周期快照事实表</h4><p>​        一行汇总了发生在某个标准周期（一天、一周）内的多个度量事件。粒度是周期性的，不是个体的事务，表中通常包含许多事实。</p><h4 id="累积快照事实表"><a href="#累积快照事实表" class="headerlink" title="累积快照事实表"></a>累积快照事实表</h4><p>​        一行汇总了发生在过程开始到结束之间的、可预测的度量事件。</p><h4 id="聚集事实表"><a href="#聚集事实表" class="headerlink" title="聚集事实表"></a>聚集事实表</h4><p>​        对原子粒度事实数据表进行简单的上卷操作，目的是提高查询性能。</p><h4 id="合并事实表"><a href="#合并事实表" class="headerlink" title="合并事实表"></a>合并事实表</h4><p>​        将来自多个过程的，粒度相同的事实表合并为一个单一的事实表。</p><p>​        增加了ETL的处理负担，降低了BI应用的分析代价，适合经常需要共同分析的多过程度量（指标）。</p><h1 id="维度表技术基础"><a href="#维度表技术基础" class="headerlink" title="维度表技术基础"></a>维度表技术基础</h1><h4 id="退化维度"><a href="#退化维度" class="headerlink" title="退化维度"></a>退化维度</h4><p>​        事实表中存在着看起来像关联维度表的外键，但事实上没有维度表和维度关联，也就是说退化维度相关的数据都在事实表中。</p><h4 id="杂项维度"><a href="#杂项维度" class="headerlink" title="杂项维度"></a>杂项维度</h4><p>​        将多个不同维度合并到一起形成单个维度。</p><h4 id="支架维度"><a href="#支架维度" class="headerlink" title="支架维度"></a>支架维度</h4><p>​        维度包含对其他维度的引用，事实表关联维度表A，维度表A引用维度表B，B称为支架维度。</p><h4 id="一致性维度"><a href="#一致性维度" class="headerlink" title="一致性维度"></a>一致性维度</h4><p>​        不同维度表的属性具有相同的列名和领域内容时，称维度表具有一致性，利用一致性维度关联各个事实表，可以将来自不同事实表的信息合并到一张报表中。</p><h4 id="缩减维度"><a href="#缩减维度" class="headerlink" title="缩减维度"></a>缩减维度</h4><p>​        缩减维度也是一种一致性维度，由基本维度的列、行的子集构成。</p><h4 id="总线矩阵"><a href="#总线矩阵" class="headerlink" title="总线矩阵"></a>总线矩阵</h4><p>​        矩阵的行表示业务过程，列表示维度。矩阵中的点表示给定的维度和业务过程是否有联系。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;《数据仓库工具箱》第二章读书笔记&lt;/p&gt;</summary>
    
    
    
    <category term="大数据技术&amp;数据仓库" scheme="https://jiangzhiqi4551.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>数据仓库工具箱笔记 - 第一章</title>
    <link href="https://jiangzhiqi4551.github.io/2021/06/09/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%B7%A5%E5%85%B7%E7%AE%B1%E7%AC%94%E8%AE%B001/"/>
    <id>https://jiangzhiqi4551.github.io/2021/06/09/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%B7%A5%E5%85%B7%E7%AE%B1%E7%AC%94%E8%AE%B001/</id>
    <published>2021-06-09T12:06:32.000Z</published>
    <updated>2021-10-11T04:19:38.161Z</updated>
    
    <content type="html"><![CDATA[<p>《数据仓库工具箱》第一章读书笔记</p><span id="more"></span><h1 id="第一章"><a href="#第一章" class="headerlink" title="第一章"></a>第一章</h1><h2 id="为什么使用维度建模不使用ER模型"><a href="#为什么使用维度建模不使用ER模型" class="headerlink" title="为什么使用维度建模不使用ER模型"></a>为什么使用维度建模不使用ER模型</h2><p>规范化的模型主要用于操作型过程中，因为对事物的更新和插入仅触及数据库单一的地方。</p><p>对BI查询来说，规范化的模型太复杂，用户难以理解和检索，复杂查询时将耗尽数据库的优化器性能，严重影响查询的性能，不能高性能检索。</p><h2 id="度建模的核心原则"><a href="#度建模的核心原则" class="headerlink" title="度建模的核心原则"></a>度建模的核心原则</h2><p>1、建立事实表时使用统一的细节级别（粒度），可以避免出现重复计算度量的问题。（事实表中每一行对应一个度量事件）</p><p>2、文本数据应尽量放在维表中，尽量不冗余到事实表。</p><h2 id="总线矩阵存在的意义"><a href="#总线矩阵存在的意义" class="headerlink" title="总线矩阵存在的意义"></a>总线矩阵存在的意义</h2><p>多数情况下，用户获得有限的数据源，并用于解决特定的问题，输出独立的、烟囱式的数据系统，其他人不能复用，也不能与组织的其他分析信息相关联。</p><p>采用总线矩阵可以为敏捷开发提供框架和主生产计划，提供可用的、公共的维度，保障数据一致性。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;《数据仓库工具箱》第一章读书笔记&lt;/p&gt;</summary>
    
    
    
    <category term="大数据技术&amp;数据仓库" scheme="https://jiangzhiqi4551.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>基于Geohash实现根据经纬度的快速定位</title>
    <link href="https://jiangzhiqi4551.github.io/2021/06/08/%E5%9F%BA%E4%BA%8EGeohash%E5%AE%9E%E7%8E%B0%E6%A0%B9%E6%8D%AE%E7%BB%8F%E7%BA%AC%E5%BA%A6%E7%9A%84%E5%BF%AB%E9%80%9F%E5%AE%9A%E4%BD%8D/"/>
    <id>https://jiangzhiqi4551.github.io/2021/06/08/%E5%9F%BA%E4%BA%8EGeohash%E5%AE%9E%E7%8E%B0%E6%A0%B9%E6%8D%AE%E7%BB%8F%E7%BA%AC%E5%BA%A6%E7%9A%84%E5%BF%AB%E9%80%9F%E5%AE%9A%E4%BD%8D/</id>
    <published>2021-06-08T10:47:31.000Z</published>
    <updated>2021-10-11T04:22:45.909Z</updated>
    
    <content type="html"><![CDATA[<p>摘要：GeoHash原理及应用</p><span id="more"></span><h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>在项目中，SDK会上报包含用户经纬度信息的一系列数据，我们需要根据经纬度信息定位出此条数据上报时用户所在的位置（包括国家、省、市、区），并和其他信息写入宽表中。</p><h4 id="旧方案"><a href="#旧方案" class="headerlink" title="旧方案"></a>旧方案</h4><p>旧方案中，主要使用GeoSpark对数据进行定位，考虑到同一个经纬度下会有多条数据，所以我们先对数据做分组，同一个用户同一个会话下相同经纬度的数据分为一组，从每组数据中抽取第一条生成一张临时表，再在临时表上调用GeoSpark算出district_id，city_id，province_id，country_id，之后将临时表与原表关联，用临时表的四个ID填充相同经纬度的其他数据的ID。<br>在测试过程中，我们发现有很小一部分数据只有district_id或city_id等细粒度数据，却没有与之相对的province_id、country_id等数据，这是老大所不能接受的（在哪个城市都知道了，国家你给我个null？？？ =。=），所以在计算四个ID之后，会有一个反推的步骤，即：判断是否有下层ID不为空上层ID却为空的情况，如果有，通过下层ID进行反推，得到上层ID，并填充。</p><p>中间还有一些其他的过滤排序逻辑不做具体介绍，最后当整个定位逻辑完成后，需要做6~7次的shuffle，我们发现其性能远低于预期，在我们每天将近40亿的数据量下，较大的拖慢了整个流程的运行速度，影响了数据产出，因此需要对这一部分进行优化。经过调研后，本菜鸡决定采用GeoHash的方式进行优化。</p><h2 id="什么是Geohash"><a href="#什么是Geohash" class="headerlink" title="什么是Geohash"></a>什么是Geohash</h2><p>简单介绍下GeoHash，我们可以用一个经纬度的点（例如点A： 37.788422,-122.391907 ）计算出一个GeoHash字符串（9q8yyzh），这个字符串代表一个矩形面，点A以及点A附近的点B（37.787933,-122.392887）虽然经纬度不同，但通过经纬度计算出的GeoHash字符串相同，也就是说AB两个点都在（9q8yyzh）这个面内。这样就将二维的经纬度坐标转换成了一维的字符串表示。</p><p>但A附近的多少点会跟A共享相同的字符串呢？也就是这个面的大小是怎么确定的呢？这就取决于GeoHash字符串的长度了，GeoHash的字符串长度越长，意味着这个面也就越小，会有更少的点跟A共享同样的GeoHash值。</p><p>具体GeoHash的计算方式，以及字符串长度对应的面大小。请参考如下这篇文章：<br><a href="https://zhuanlan.zhihu.com/p/35940647">GeoHash算法学习讲解、解析及原理分析</a></p><p>以上就是我们实现基于GeoHash进行定位的基础。</p><h2 id="如何用Geohash实现快速定位"><a href="#如何用Geohash实现快速定位" class="headerlink" title="如何用Geohash实现快速定位"></a>如何用Geohash实现快速定位</h2><p>既然可以用经纬度代表的一个点得到一个面，那如果我们的历史数据足够多，映射出足够多的的面，这些面就会像拼图一样，慢慢把我们的世界拼出来。</p><p>例如我们可以用21个GeoHash字符串将整个北京欢乐谷拼出来：</p><p><img src="https://img-blog.csdnimg.cn/20191206171409523.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="wx4ffc"></p><p>有了这个完整的拼图之后，当欢乐谷范围内有一条新数据上报时，我们只需要根据经纬度算出对应的GeoHash值，再用这个值去和这21个字符串匹配，如果和其中任意一个相同，就说明此条数据的位置信息为中国北京的朝阳区（不具体定位到欢乐谷是因为我们最细粒度只划分到行政区)。</p><p>采用这个思路，最后我们将世界地图构建好之后，当有新数据需要定位时，我们只需要做一次GeoHash字符串计算，再到数据库中进行匹配即可。速度大大提高。</p><h2 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h2><h4 id="Geohash字符串的计算："><a href="#Geohash字符串的计算：" class="headerlink" title="Geohash字符串的计算："></a>Geohash字符串的计算：</h4><p>此处采用的方法是写一个UDF，UDF的功能是输入经纬度及想要的GeoHash字符串长度，输出对应的GeoHash字符串。再将其打成jar包，上传之后在hive中创建临时函数，再进行调用。</p><h5 id="首先导入依赖"><a href="#首先导入依赖" class="headerlink" title="首先导入依赖"></a>首先导入依赖</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;ch.hsr&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;geohash&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h5 id="继承UDF并重写evaluate方法"><a href="#继承UDF并重写evaluate方法" class="headerlink" title="继承UDF并重写evaluate方法"></a>继承UDF并重写evaluate方法</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">public class getGeoHashString extends UDF &#123;</span><br><span class="line"></span><br><span class="line">    private static int precision = 7;</span><br><span class="line"></span><br><span class="line">    public String evaluate(double latitude, double longtitude, int precisionParam) &#123;</span><br><span class="line"></span><br><span class="line">        GeoHash geoHash = GeoHash.withCharacterPrecision(latitude, longtitude, precisionParam);</span><br><span class="line">        return geoHash.toBase32();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String evaluate(double latitude, double longtitude) &#123;</span><br><span class="line"></span><br><span class="line">        GeoHash geoHash = GeoHash.withCharacterPrecision(latitude, longtitude, precision);</span><br><span class="line">        return geoHash.toBase32();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>默认采用7位长度，当然也支持传入参数自定义</p><h5 id="Maven打包并上传"><a href="#Maven打包并上传" class="headerlink" title="Maven打包并上传"></a>Maven打包并上传</h5><p>略。。</p><h5 id="使用UDF"><a href="#使用UDF" class="headerlink" title="使用UDF"></a>使用UDF</h5><p>目前是在hive命令行中运行的，具体方法如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add jar /data/home/geoHashUDF-1.0-SNAPSHOT-jar-with-dependencies.jar;</span><br></pre></td></tr></table></figure><p>先把jar包添加进来，在创建临时函数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE TEMPORARY FUNCTION get_geohash_string as &#x27;getGeoHashString&#x27;;</span><br></pre></td></tr></table></figure><p>其中get_geohash_string为函数名，getGeoHashString为你的主类。</p><h5 id="接下来写SQL就可以了"><a href="#接下来写SQL就可以了" class="headerlink" title="接下来写SQL就可以了"></a>接下来写SQL就可以了</h5><p>将每一天的新数据计算后写入分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE geohash_a_d</span><br><span class="line">PARTITION(dt)</span><br><span class="line">SELECT</span><br><span class="line">get_geohash_string(latitude,longitude),</span><br><span class="line"></span><br><span class="line">geo_district_id,</span><br><span class="line">geo_district_name_en,</span><br><span class="line">geo_district_name_zh,</span><br><span class="line"></span><br><span class="line">geo_city_id,</span><br><span class="line">geo_city_name_en,</span><br><span class="line">geo_city_name_zh,</span><br><span class="line"></span><br><span class="line">geo_province_id,</span><br><span class="line">geo_province_name_en,</span><br><span class="line">geo_province_name_zh,</span><br><span class="line"></span><br><span class="line">geo_country_id,</span><br><span class="line">geo_country_name_en,</span><br><span class="line">geo_country_name_zh,</span><br><span class="line">dt</span><br><span class="line">from </span><br><span class="line">report_i_h</span><br><span class="line">WHERE </span><br><span class="line">dt BETWEEN &#x27;2019-11-01&#x27; AND &#x27;2019-11-30&#x27;</span><br></pre></td></tr></table></figure><p>对每一天的数据进行去重合并</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE geohash_summary</span><br><span class="line">SELECT </span><br><span class="line">geohash,</span><br><span class="line">geo_district_id,</span><br><span class="line">geo_district_name_en,</span><br><span class="line">geo_district_name_zh,</span><br><span class="line">geo_city_id,</span><br><span class="line">geo_city_name_en,</span><br><span class="line">geo_city_name_zh,</span><br><span class="line">geo_province_id,</span><br><span class="line">geo_province_name_en,</span><br><span class="line">geo_province_name_zh,</span><br><span class="line">geo_country_id,</span><br><span class="line">geo_country_name_en,</span><br><span class="line">geo_country_name_zh</span><br><span class="line">from geohash_a_d</span><br><span class="line">WHERE dt=&#x27;2019-11-01&#x27;</span><br><span class="line">UNION DISTINCT</span><br><span class="line">SELECT </span><br><span class="line">geohash,</span><br><span class="line">geo_district_id,</span><br><span class="line">geo_district_name_en,</span><br><span class="line">geo_district_name_zh,</span><br><span class="line">geo_city_id,</span><br><span class="line">geo_city_name_en,</span><br><span class="line">geo_city_name_zh,</span><br><span class="line">geo_province_id,</span><br><span class="line">geo_province_name_en,</span><br><span class="line">geo_province_name_zh,</span><br><span class="line">geo_country_id,</span><br><span class="line">geo_country_name_en,</span><br><span class="line">geo_country_name_zh</span><br><span class="line">from geohash_a_d</span><br><span class="line">WHERE dt BETWEEN &#x27;2019-11-02&#x27; AND &#x27;2019-11-30&#x27;</span><br></pre></td></tr></table></figure><p>对hash值进行去重，确保一个hash值只对应一条记录（<strong>此处有大坑，之后讲</strong>）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">with tmp as (</span><br><span class="line">SELECT </span><br><span class="line">geohash,</span><br><span class="line">geo_district_id,</span><br><span class="line">geo_district_name_en,</span><br><span class="line">geo_district_name_zh,</span><br><span class="line">geo_city_id,</span><br><span class="line">geo_city_name_en,</span><br><span class="line">geo_city_name_zh,</span><br><span class="line">geo_province_id,</span><br><span class="line">geo_province_name_en,</span><br><span class="line">geo_province_name_zh,</span><br><span class="line">geo_country_id,</span><br><span class="line">geo_country_name_en,</span><br><span class="line">geo_country_name_zh,</span><br><span class="line">row_number() OVER(PARTITION BY geohash ORDER BY </span><br><span class="line">geo_country_id,geo_province_id,geo_city_id,geo_district_id</span><br><span class="line">desc) as rank</span><br><span class="line">from geohash_summary</span><br><span class="line">where geo_country_id is not null</span><br><span class="line">)</span><br><span class="line">insert overwrite table geohash_distinct</span><br><span class="line">select</span><br><span class="line">geohash,</span><br><span class="line">geo_district_id,</span><br><span class="line">geo_district_name_en,</span><br><span class="line">geo_district_name_zh,</span><br><span class="line">geo_city_id,</span><br><span class="line">geo_city_name_en,</span><br><span class="line">geo_city_name_zh,</span><br><span class="line">geo_province_id,</span><br><span class="line">geo_province_name_en,</span><br><span class="line">geo_province_name_zh,</span><br><span class="line">geo_country_id,</span><br><span class="line">geo_country_name_en,</span><br><span class="line">geo_country_name_zh</span><br><span class="line">from tmp</span><br><span class="line">where rank=1</span><br></pre></td></tr></table></figure><p>这几个SQL跑完后，我们的GeoHash维度表就初步构建完成了。</p><h2 id="效果测试"><a href="#效果测试" class="headerlink" title="效果测试"></a>效果测试</h2><p>构建完成后，便可以进行定位的效果测试了，我们采用的测试方案是：<br>取不在回溯日期内的几天的数据，通过GeoHash的方式获取其位置信息，在和用geoSpark获取的位置信息作对比，校验其准确性。</p><p>结果：99.5%的数据可以成功获取定位信息，但是其中千分之七的数据存在distinct级的误差，千分之一的数据存在city级的误差。此外，还意外的实现了千分之一的数据优化。</p><p>数据优化：有一些数据可能geoSpark定位不到，或定位的信息不全，通过geoHash可以获取到定位或将定位信息补全。随便举个例子：<br>geoSpark:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">latitude:45.12345</span><br><span class="line">longtitude:110.12345</span><br><span class="line">dim_geohash_distinct.geo_district_idnull</span><br><span class="line">dim_geohash_distinct.geo_city_idnull</span><br><span class="line">dim_geohash_distinct.geo_province_id3117</span><br><span class="line">dim_geohash_distinct.geo_country_id3142</span><br></pre></td></tr></table></figure><p>geoHash:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">latitude:45.12345</span><br><span class="line">longtitude:110.12345</span><br><span class="line">dim_geohash_distinct.geo_district_id132</span><br><span class="line">dim_geohash_distinct.geo_city_id3022</span><br><span class="line">dim_geohash_distinct.geo_province_id3117</span><br><span class="line">dim_geohash_distinct.geo_country_id3142</span><br></pre></td></tr></table></figure><p>通过GeoHash，可以将缺失的district_id及city_id补全。</p><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>采用GeoHash实现定位的前提是有足够的数据量支持，为了达到本文实现的效果，我们回溯了三个月的数据，每天的数据量在35亿左右。最后生成的维度表结构如下所示：</p><p><img src="https://img-blog.csdnimg.cn/20191209104852237.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>其中district代表行政区（如东城区、朝阳区），geohash为生成的GeoHash字符串。<br>随便抽取其中一条记录如下：</p><p><img src="https://img-blog.csdnimg.cn/20191209105347452.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>（话说以前一直以为西藏的英文是Xizang。。orz)</p><p>GeoHash这种方式虽然较快的实现了定位，但仍有一些问题丞待解决，下一篇文章将讨论这些坑以及可能的解决方案。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;摘要：GeoHash原理及应用&lt;/p&gt;</summary>
    
    
    
    <category term="大数据技术&amp;数据仓库" scheme="https://jiangzhiqi4551.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>catalog和schema的区别</title>
    <link href="https://jiangzhiqi4551.github.io/2021/06/08/catalog%E5%92%8Cschema%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>https://jiangzhiqi4551.github.io/2021/06/08/catalog%E5%92%8Cschema%E7%9A%84%E5%8C%BA%E5%88%AB/</id>
    <published>2021-06-08T10:45:56.000Z</published>
    <updated>2021-10-11T04:20:54.420Z</updated>
    
    <content type="html"><![CDATA[<p>摘要：什么是catalog，什么是schema</p><span id="more"></span><p>直接上图，直观一点：</p><p><img src="https://img-blog.csdnimg.cn/20191210125223204.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="什么是catalog"><a href="#什么是catalog" class="headerlink" title="什么是catalog"></a>什么是catalog</h3><p>从概念上说，一个catalog包含多个schema，一个schema下可以包含多个数据库对象（表，视图，字段），catalog可以理解为数据库实例的元数据集合。</p><p>常用数据库对catalog和schema的支持如下：</p><p><img src="https://img-blog.csdnimg.cn/2019121012584136.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="什么是schema"><a href="#什么是schema" class="headerlink" title="什么是schema"></a>什么是schema</h3><p>一般来说，schema是指数据库表的组织和定义，定义了表、字段以及表和字段间的关系。可以理解为表的命名空间。</p><p>推荐下stack overflow上的优秀回答：<br> <a href="https://stackoverflow.com/questions/7022755/whats-the-difference-between-a-catalog-and-a-schema-in-a-relational-database">What’s the difference between a catalog and a schema in a relational database?</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;摘要：什么是catalog，什么是schema&lt;/p&gt;</summary>
    
    
    
    <category term="大数据技术&amp;数据仓库" scheme="https://jiangzhiqi4551.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>Hive入门之基础知识（一）</title>
    <link href="https://jiangzhiqi4551.github.io/2021/06/08/Hive%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8B%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"/>
    <id>https://jiangzhiqi4551.github.io/2021/06/08/Hive%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8B%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/</id>
    <published>2021-06-08T10:41:57.000Z</published>
    <updated>2022-01-04T12:48:30.737Z</updated>
    
    <content type="html"><![CDATA[<p>随笔总结一些关于Hive的杂七杂八的知识点</p><span id="more"></span><h2 id="Hive出现的原因"><a href="#Hive出现的原因" class="headerlink" title="Hive出现的原因"></a>Hive出现的原因</h2><p>从一个基于传统关系型数据库和结构化查询语言的数据基础架构转移到Hadoop上，使用HQL查询Hadoop中的数据。</p><h2 id="Hive与传统关系型数据库的区别"><a href="#Hive与传统关系型数据库的区别" class="headerlink" title="Hive与传统关系型数据库的区别"></a>Hive与传统关系型数据库的区别</h2><p>Hive不支持记录级别的更新、插入和删除操作。执行延迟大，不支持事务。</p><h2 id="Hive组成模块"><a href="#Hive组成模块" class="headerlink" title="Hive组成模块"></a>Hive组成模块</h2><p><img src="https://img-blog.csdnimg.cn/20191213140841632.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>所有的命令和查询都会进入到驱动模块driver中，driver对输入进行解析和编译，以及对需求的计算进行优化，然后启动MR来执行job。Hive本身不会生成MR的程序，而是通过一个表示”job执行计划“的XML文件驱动执行内置的原生的MR模块。<br>Hive通过和jobtracker进行通信来初始化MR任务。</p><h2 id="Hive安装目录"><a href="#Hive安装目录" class="headerlink" title="Hive安装目录"></a>Hive安装目录</h2><p><img src="https://img-blog.csdnimg.cn/20191213142812701.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="Hive常用的命令行执行参数"><a href="#Hive常用的命令行执行参数" class="headerlink" title="Hive常用的命令行执行参数"></a>Hive常用的命令行执行参数</h2><p>Hive -e执行完一条命令后立刻退出CLI<br>Hive -S开启静默模式，去掉OK等无关紧要的输出信息<br>Hive -f文件名，指定文件中的一个或多个查询语句。</p><h2 id="Hive的基本数据类型"><a href="#Hive的基本数据类型" class="headerlink" title="Hive的基本数据类型"></a>Hive的基本数据类型</h2><p><img src="https://img-blog.csdnimg.cn/2019121314530063.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191213145333149.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>其中时间戳可以是整数、浮点数、字符串，其表示的是UTC时间。</p><h4 id="类型转换的用法："><a href="#类型转换的用法：" class="headerlink" title="类型转换的用法："></a>类型转换的用法：</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cast(user_id AS STRING) as user_id_string</span><br></pre></td></tr></table></figure><h4 id="Hive中的集合数据类型"><a href="#Hive中的集合数据类型" class="headerlink" title="Hive中的集合数据类型"></a>Hive中的集合数据类型</h4><p>Hive中的集合数据类型：map、struct、array<br>为什么Hive支持集合数据类型，而大多数传统关系型数据库不支持：关系型数据库通过外键将多个表进行关联，当数据量很大时，根据外键进行关联造成的磁盘间的寻址操作将产生很高的代价。</p><h4 id="Hive的读时模式"><a href="#Hive的读时模式" class="headerlink" title="Hive的读时模式"></a>Hive的读时模式</h4><p>传统的关系型数据库是写时模式，在数据写入时对模式进行检查。<br>对于Hive要查询的数据，有很多种方式对其进行创建，修改，损坏，因此Hive采用读时模式，在查询时进行验证，尽量修复错误或者赋空值。</p><h4 id="Hive查看表信息"><a href="#Hive查看表信息" class="headerlink" title="Hive查看表信息"></a>Hive查看表信息</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DESCRIBE  表名</span><br><span class="line">DESCRIBE EXTENDED 表名</span><br><span class="line">DESCRIBE FORMATTED 表名</span><br></pre></td></tr></table></figure><p>都可以查询表的详细信息，信息完整程度由上到下逐渐增加，DESCRIBE FORMATTED的数据最完整。</p><h2 id="外部表和内部表"><a href="#外部表和内部表" class="headerlink" title="外部表和内部表"></a>外部表和内部表</h2><p>内部表：也叫管理表，Hive会控制数据的生命周期，删除一个内部表时，Hive也会删除表中的数据。内部表也不便于和其他工作共享数据。<br>外部表：删除表并不会删除表中的数据，但是描述表的元数据信息会被删除。有些HQL的语法结构也不适用于外部表。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE 目标表 LIKE 源表   -- 生成的表是外部表</span><br><span class="line">CREATE TABLE 目标表 LIKE 源表  -- 如果源表是外部表，则目标表也是外部表；如果源表是内部表，目标表也是内部表</span><br></pre></td></tr></table></figure><h2 id="Hive表的存储格式"><a href="#Hive表的存储格式" class="headerlink" title="Hive表的存储格式"></a>Hive表的存储格式</h2><p>默认情况下，Hive采用的存储格式为文本文件格式（TEXTFILE），在这种格式下，每一行被认为是一个单独的记录。<br>记录的解析由序列化器，反序列化器（SerDe）来控制。<br>Hive使用一个inputformat对象将输入流分割成记录，使用outputformat对象将记录格式化为输出流，使用SerDe在读数据时将记录解析成列，写数据时将列编码成记录。</p><h2 id="Hive防止表被删除或查询"><a href="#Hive防止表被删除或查询" class="headerlink" title="Hive防止表被删除或查询"></a>Hive防止表被删除或查询</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE test ENABLE NO_DROP  // 防止被删除</span><br><span class="line">ALTER TABLE test ENABLE OFFLINE  // 防止被查询</span><br></pre></td></tr></table></figure><p>若要允许被删除或查询，只需把 enable 改为 disable </p><h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><p>Hive没有关系型数据库中键的概念，只有有限的索引功能。一张表的索引数据存放在另一张表中。<br>建立索引可以帮助剪裁掉一张表的一些数据块，减少MR任务的数据输入量。<br>通过explain+SQL语句可以查看是否使用了索引。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;随笔总结一些关于Hive的杂七杂八的知识点&lt;/p&gt;</summary>
    
    
    
    <category term="Hive" scheme="https://jiangzhiqi4551.github.io/categories/Hive/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark入门之基础知识（一）</title>
    <link href="https://jiangzhiqi4551.github.io/2021/06/08/Spark%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>https://jiangzhiqi4551.github.io/2021/06/08/Spark%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%80%EF%BC%89/</id>
    <published>2021-06-08T10:41:13.000Z</published>
    <updated>2021-10-10T10:16:20.935Z</updated>
    
    <content type="html"><![CDATA[<p>Spark 基础知识</p><span id="more"></span><h4 id="什么是Spark"><a href="#什么是Spark" class="headerlink" title="什么是Spark"></a>什么是Spark</h4><p>Spark 是一个用来实现快速而通用的集群计算的平台。</p><h4 id="Spark的核心"><a href="#Spark的核心" class="headerlink" title="Spark的核心"></a>Spark的核心</h4><p>Spark 的核心是一个对由很多计算任务组成的、运行在多个工作机器或者一个计算集群上的应用  进行调度、分发、监控的计算引擎。</p><h4 id="Spark软件栈设计的优点"><a href="#Spark软件栈设计的优点" class="headerlink" title="Spark软件栈设计的优点"></a>Spark软件栈设计的优点</h4><p>1）软件栈中所有程序库和高级组件都可以从下层的改进中受益。<br>2）运行整个软件栈的代价变小了。<br>3）可以构建出无缝整合处理不同模型的应用。</p><h4 id="Spark-的各个组件"><a href="#Spark-的各个组件" class="headerlink" title="Spark 的各个组件"></a>Spark 的各个组件</h4><p>Spark的各个组件如图所示：<br><img src="https://img-blog.csdnimg.cn/20191216125136843.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>Spark Core：实现了Spark的基本功能，报包含任务调度，内存管理，错误恢复，与存储系统交互等模块，还包含了对RDD的API定义。</p><p>Spark SQL：Spark用来操作结构化数据的程序包。</p><p>SparkStreaming：提供了用来操作数据流的API。</p><p>MLib与GraghX目前不在学习计划内暂时不介绍。</p><h4 id="Spark核心概念简介"><a href="#Spark核心概念简介" class="headerlink" title="Spark核心概念简介"></a>Spark核心概念简介</h4><p>每个Spark应用都由一个驱动器程序发起集群上的各种并行操作。驱动器程序通过SparkContext对象访问Spark，这个对象代表对计算集群的一个连接。驱动器程序还要管理多个执行器节点。<br><img src="https://img-blog.csdnimg.cn/20191216141248477.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>下篇文章介绍Spark的RDD编程。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Spark 基础知识&lt;/p&gt;</summary>
    
    
    
    <category term="Spark" scheme="https://jiangzhiqi4551.github.io/categories/Spark/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark入门之基础知识（二）RDD编程</title>
    <link href="https://jiangzhiqi4551.github.io/2021/06/08/Spark%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%BA%8C%EF%BC%89RDD%E7%BC%96%E7%A8%8B/"/>
    <id>https://jiangzhiqi4551.github.io/2021/06/08/Spark%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%BA%8C%EF%BC%89RDD%E7%BC%96%E7%A8%8B/</id>
    <published>2021-06-08T10:40:29.000Z</published>
    <updated>2021-10-10T10:29:17.082Z</updated>
    
    <content type="html"><![CDATA[<p>摘要：RDD基础知识总结</p><span id="more"></span><h2 id="什么是RDD"><a href="#什么是RDD" class="headerlink" title="什么是RDD"></a>什么是RDD</h2><p>弹性分布式数据集RDD是Spark的核心抽象。RDD其实就是分布式的元素集合，Spark中的操作创建、转化、或对RDD进行求值，Spark会自动将RDD的数据分发到集群上并并行执行。<br>RDD是一个不可变的分布式对象集合，每个RDD都被分为多个分区，这些分区在集群中不同的节点上运行。<br>”弹性“的解读：弹性意味着在任何时候都能进行重算，当某一部分数据丢失时，可以根据血缘关系将丢失的部分计算出来，而不是从头开始计算全部数据。</p><h2 id="RDD的属性"><a href="#RDD的属性" class="headerlink" title="RDD的属性"></a>RDD的属性</h2><p>（1）一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</p><p>（2）一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。</p><p>（3）RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</p><p>（4）一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</p><p>（5）一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。</p><h2 id="创建RDD的方法"><a href="#创建RDD的方法" class="headerlink" title="创建RDD的方法"></a>创建RDD的方法</h2><p>1）读取外部数据及</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val lines = sc.textFile(&quot;test.txt&quot;) // 从HDFS读取数据</span><br></pre></td></tr></table></figure><p>读取数据的操作同样也是惰性的，执行上面这行代码时，数据并没有真正被读取进来。<br>2）在Driver Program中分发驱动器程序中的对象集合。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val data = context.parallelize(List(1,2,3,4,5)) // 创建RDD最简单的方式</span><br></pre></td></tr></table></figure><h2 id="RDD的操作方法"><a href="#RDD的操作方法" class="headerlink" title="RDD的操作方法"></a>RDD的操作方法</h2><p>1）转化操作：transformation 会由一个RDD生成一个<em>新的</em>RDD。<br>Spark使用血缘关系图来记录不同RDD之间的依赖关系：<br><img src="https://img-blog.csdnimg.cn/201912161540010.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>2）行动操作：action会将RDD计算出一个结果，返回到Driver Program中或存到HDFS等外部存储中。</p><h5 id="转化操作和行动操作的区别"><a href="#转化操作和行动操作的区别" class="headerlink" title="转化操作和行动操作的区别"></a>转化操作和行动操作的区别</h5><p>transformation是惰性的，一个transformation操作并不会被立即计算，只有在action中用到时，才会去真正的计算。<br>转化操作返回的是一个RDD，行动操作返回的是其他的数据类型。</p><h5 id="为什么要区分转化和行动操作"><a href="#为什么要区分转化和行动操作" class="headerlink" title="为什么要区分转化和行动操作"></a>为什么要区分转化和行动操作</h5><p>主要是出于对资源和效率上的考虑，如果我们有1000行文本，先把前800行读出来记为RDD1，再将RDD1的前200行读出来记为RDD2，再读取RDD2的第一行记为RDD3，最后把RDD3存到HDFS上。<br>如果每一步都立刻计算的话，就会造成很大的开销。然而事实上我们只需要第一行的数据，在采取惰性计算后，在真正的action被执行时，Spark已经获知了整个转化操作链，只会计算我们真正需要的数据，大大加快了执行速度。</p><h2 id="关于RDD缓存"><a href="#关于RDD缓存" class="headerlink" title="关于RDD缓存"></a>关于RDD缓存</h2><h5 id="为什么要缓存"><a href="#为什么要缓存" class="headerlink" title="为什么要缓存"></a>为什么要缓存</h5><p>RDD会在每次对其进行action操作时重新计算，实际生产中很多情况下需要多次重用一个RDD（例如我们读取了一个大宽表，之后在这个宽表上产出各种维度数据），此时最好对RDD进行缓存。</p><h2 id="常见transformation与action操作讲解"><a href="#常见transformation与action操作讲解" class="headerlink" title="常见transformation与action操作讲解"></a>常见transformation与action操作讲解</h2><h4 id="transformation"><a href="#transformation" class="headerlink" title="transformation"></a>transformation</h4><p>1）map()：接受一个函数，将其应用于RDD中的每个元素，将函数返回值作为结果RDD中元素的对应值。返回值类型不需要和输入值类型一样。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val data = context.parallelize(List(1,2,3,4,5))</span><br><span class="line">    val dataAdd = data.map(x =&gt; x+1)</span><br><span class="line">    dataAdd.take(6)</span><br></pre></td></tr></table></figure><p>对每一个元素加一，结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res11: Array[Int] = Array(2, 3, 4, 5, 6)</span><br></pre></td></tr></table></figure><p>2）filter()：接受一个函数，将RDD中满足函数要求的元素放入结果RDD中返回。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val dataFilter = data.filter(x =&gt; x &gt; 3)</span><br><span class="line">   dataFilter.take(5)</span><br></pre></td></tr></table></figure><p>筛选出大于3的元素，结果为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res12: Array[Int] = Array(4, 5)</span><br></pre></td></tr></table></figure><p>3）flatMap()：对每个输入元素生成多个输出元素，可以理解为将输入的RDD拍扁。<br><img src="https://img-blog.csdnimg.cn/20191216161042645.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>经过flatmap拍扁之后，得到的是一个由各列表中元素组成的RDD，而不是一个由多列表组成的RDD。</p><h5 id="RDD集合操作"><a href="#RDD集合操作" class="headerlink" title="RDD集合操作"></a>RDD集合操作</h5><p>集合操作要求RDD之间的数据类型相同。<br>1）union操作：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = context.parallelize(List(1, 3, 5, 7, 10))</span><br><span class="line">val rdd2 = context.parallelize(List(2, 4, 6, 8, 10))</span><br><span class="line">val union = rdd1.union(rdd2)</span><br><span class="line">union.take(10)</span><br><span class="line">res14: Array[Int] = Array(1, 3, 5, 7, 10, 2, 4, 6, 8, 10)</span><br></pre></td></tr></table></figure><p>返回包含两个RDD中全部元素的新RDD，重复元素也会保留。</p><p>2）intersection操作：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = context.parallelize(List(1, 3, 5, 7, 10))</span><br><span class="line">val rdd2 = context.parallelize(List(2, 4, 6, 8, 10))</span><br><span class="line">val intersection = rdd1.intersection(rdd2)</span><br><span class="line">intersection.take(10)</span><br><span class="line">res15: Array[Int] = Array(10)   </span><br></pre></td></tr></table></figure><p>结果为两个RDD的交集，会对结果去重。</p><p>3）subtract操作：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = context.parallelize(List(1, 3, 5, 7, 10))</span><br><span class="line">val rdd2 = context.parallelize(List(2, 4, 6, 8, 10))</span><br><span class="line">val sub = rdd1.subtract(rdd2)</span><br><span class="line">sub.take(10)</span><br><span class="line">res16: Array[Int] = Array(1, 3, 5, 7)         </span><br></pre></td></tr></table></figure><p>返回存在第一个RDD中，不存在第二个RDD中的元素</p><p>4）cartesian操作：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.parallelize(List(1, 3, 5))</span><br><span class="line">val rdd2 = sc.parallelize(List(2, 4, 6))</span><br><span class="line">val cartesian = rdd1.cartesian(rdd2)</span><br><span class="line">cartesian.take(9)</span><br><span class="line">res17: Array[(Int, Int)] = Array((1,2), (1,4), (1,6), (3,2), (5,2), (3,4), (3,6), (5,4), (5,6))</span><br></pre></td></tr></table></figure><p>对两个RDD求笛卡尔积</p><h4 id="action"><a href="#action" class="headerlink" title="action"></a>action</h4><p>1）reduce：<br>操作两个RDD元素类型的数据并返回一个同样类型的新元素</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; list</span><br><span class="line">res0: List[Int] = List(1, 2, 3, 4, 5)</span><br><span class="line">scala&gt; list.reduce((x,y) =&gt; x+y)</span><br><span class="line">res2: Int = 15</span><br></pre></td></tr></table></figure><p>2）fold：<br>与reduce相同，只不过加了个初始值的设定<br>初始值为100，最后求和结果为115</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; list</span><br><span class="line">res0: List[Int] = List(1, 2, 3, 4, 5)</span><br><span class="line">scala&gt; list.fold(100)((x,y) =&gt; x+y)</span><br><span class="line">res3: Int = 115</span><br></pre></td></tr></table></figure><p>3）aggregate：<br>reduce和fold都只能返回和RDD中元素类型相同的值，例如前面的例子中。list中的是int，那么我们最后得到的返回值也是int，但是aggregate却没有这个限制，可以有不同类型的返回值。<br>举个经典的求平均值例子：<br>aggregate采用柯里化的方式接受三个参数，在本例中(0,0)代表初始值，第一个函数(x, y) =&gt; (x._1 + y, x._2 + 1)代表（目前已统计的数值之和，目前已统计的元素个数），第二个函数(x, y) =&gt; (x._1 + y._1, x._2 + y._2)代表对分布式计算的结果进行累加计算，例如从两个RDD统计的结果为（10，2），（10，3），对这两部分进行合并得到（20，5），最后用20/5即可得到平均值。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val data = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">data: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line">scala&gt; val result = data.aggregate(0, 0)((x, y) =&gt; (x._1 + y, x._2 + 1), (x, y) =&gt; (x._1 + y._1, x._2 + y._2))</span><br><span class="line">result: (Int, Int) = (15,5)</span><br><span class="line">scala&gt; val avg = result._1/result._2.toDouble</span><br><span class="line">avg: Double = 3.0                                                     </span><br></pre></td></tr></table></figure><p>4）collect：<br>会将整个RDD的内容返回，要求所有数据都必须能一同放入单台机器的内存中。所以不建议在数据量很大时使用，一般用来测试代码。<br>5）take：<br>返回RDD中的n个元素</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; list</span><br><span class="line">res0: List[Int] = List(1, 2, 3, 4, 5)</span><br><span class="line">scala&gt; list.take(3)</span><br><span class="line">res4: List[Int] = List(1, 2, 3)</span><br></pre></td></tr></table></figure><p>6）top：<br>从RDD中提取前n个元素</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val data = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">data: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line">scala&gt; data.top(2)</span><br><span class="line">res10: Array[Int] = Array(5, 4)</span><br></pre></td></tr></table></figure><p>7）takeSample：<br>从数据采样<br>该方法仅在预期结果数组很小的情况下使用，因为所有数据都被加载到driver的内存中。<br>第一个参数true or false 表示是否可以重复抽样，在选为false的情况下，抽取的元素不会发生重复。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; data.takeSample(false,2)</span><br><span class="line">res11: Array[Int] = Array(4, 1)       </span><br><span class="line">scala&gt; data.takeSample(false,2)</span><br><span class="line">res12: Array[Int] = Array(2, 4) </span><br></pre></td></tr></table></figure><h4 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h4><p>当我们对RDD进行缓存时，计算出RDD的节点会分别保存他们计算出的数据，如果某个节点的数据丢失，当我们用到RDD时，Spark会重算这部分的数据。<br>在Scala中，默认情况下persist()会把数据以序列化的形式缓存在JVM的堆空间中。</p><p>各个缓存级别的差异如图所示：<br><img src="https://img-blog.csdnimg.cn/20191217153950699.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppYW5nemhpcWk0NTUx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>persist()的调用不会触发求值，当缓存的数据太多内存放不下时，Spark会使用LRU删除最近最少使用的那部分数据，如果缓存级别为只放在内存中时，再次用到这部分的数据就需要重新计算。</p><p>下篇文章将介绍基本的键值对操作。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;摘要：RDD基础知识总结&lt;/p&gt;</summary>
    
    
    
    <category term="Spark" scheme="https://jiangzhiqi4551.github.io/categories/Spark/"/>
    
    
  </entry>
  
  <entry>
    <title>Hive入门之基础知识（二）之数据操作与查询</title>
    <link href="https://jiangzhiqi4551.github.io/2021/06/08/Hive%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%BA%8C%EF%BC%89%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C%E4%B8%8E%E6%9F%A5%E8%AF%A2/"/>
    <id>https://jiangzhiqi4551.github.io/2021/06/08/Hive%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%BA%8C%EF%BC%89%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C%E4%B8%8E%E6%9F%A5%E8%AF%A2/</id>
    <published>2021-06-08T10:38:57.000Z</published>
    <updated>2021-10-10T10:16:20.933Z</updated>
    
    <content type="html"><![CDATA[<p>随笔总结一些关于Hive的杂七杂八的知识点</p><span id="more"></span><h2 id="向Hive表中装载数据"><a href="#向Hive表中装载数据" class="headerlink" title="向Hive表中装载数据"></a>向Hive表中装载数据</h2><p>Hive不会验证向表中装载的数据和表的模式是否匹配（需要自己检查确认），但是会检查文件的格式是否和表结构定义的一致（创建表时指定的结构若为SEQUENCEFILE，则装载进去的文件也应该为sequencefile格式）。</p><p>从本地文件系统向表中装载数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA LOCAL INPATH &#x27;path&#x27; INTO TABLE &#x27;table&#x27;  </span><br></pre></td></tr></table></figure><p>从本地文件系统向表中装载数据，使用overwrite覆盖原表数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA LOCAL INPATH &#x27;path&#x27; OVERWRITE INTO TABLE &#x27;table&#x27;</span><br></pre></td></tr></table></figure><p>从本地文件系统向表中装载数据，使用overwrite覆盖原表数据并指定时间分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA LOCAL INPATH &#x27;path&#x27; OVERWRITE INTO TABLE &#x27;table&#x27; PARTITION (dt=&#x27;2019-11-11&#x27;)</span><br></pre></td></tr></table></figure><p>从HDFS向表中装载数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA INPATH &#x27;path&#x27; INTO TABLE &#x27;table&#x27;  </span><br></pre></td></tr></table></figure><p>从HDFS向表中装载数据，使用overwrite覆盖原表数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA INPATH &#x27;path&#x27; OVERWRITE INTO TABLE &#x27;table&#x27;</span><br></pre></td></tr></table></figure><p>从HDFS向表中装载数据，使用overwrite覆盖原表数据并指定时间分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA LOCAL INPATH &#x27;path&#x27; OVERWRITE INTO TABLE &#x27;table&#x27; PARTITION (dt=&#x27;2019-11-11&#x27;)</span><br></pre></td></tr></table></figure><p>另外需要注意的是，如果使用了local关键字，数据将会被<strong>拷贝</strong>到目标位置，<br>如果不使用local关键字，数据会被<strong>转移</strong>到目标位置。因为Hive默认在分布式文件系统中用户不需要一份文件的多份重复拷贝。</p><h2 id="通过查询语句向表中装载数据"><a href="#通过查询语句向表中装载数据" class="headerlink" title="通过查询语句向表中装载数据"></a>通过查询语句向表中装载数据</h2><p>PARTITION关键字可以指定要创建的分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE &#x27;table1&#x27;</span><br><span class="line">PARTITION(dt=&#x27;2019-11-11&#x27;)</span><br><span class="line">SELECT * FROM &#x27;table2&#x27;</span><br><span class="line">WHERE dt=&#x27;2019-11-11&#x27;</span><br></pre></td></tr></table></figure><h4 id="动态分区插入"><a href="#动态分区插入" class="headerlink" title="动态分区插入"></a>动态分区插入</h4><p>当分区很多时，一个一个指定很麻烦，可以使用动态分区插入<br>需要先开启动态分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SET hive.exec.dynamic.partition = true       </span><br><span class="line">SET hive.exec.dynamic.partition.mode = nostrict</span><br></pre></td></tr></table></figure><p>使用动态分区插入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">将表2中11-01号到11-11号的user_id按时间分区插入到表1中</span><br><span class="line">INSERT OVERWRITE TABLE &#x27;table1&#x27;</span><br><span class="line">PARTITION(dt)</span><br><span class="line">SELECT </span><br><span class="line">user_id,</span><br><span class="line">dt</span><br><span class="line">from &#x27;table2&#x27;</span><br><span class="line">WHERE dt BETWEEN &#x27;2019-11-01&#x27; AND &#x27;2019-11-11&#x27;</span><br></pre></td></tr></table></figure><p>还可以通过一个查询语句直接创建出表，在实际工作中长使用此功能创建临时表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE tmp AS</span><br><span class="line">SELECT</span><br><span class="line">user_id,</span><br><span class="line">dt,</span><br><span class="line">hour</span><br><span class="line">from table1</span><br></pre></td></tr></table></figure><h2 id="导出数据"><a href="#导出数据" class="headerlink" title="导出数据"></a>导出数据</h2><p>如果数据恰好是所需要的格式，直接从HDFS上拷贝文件即可。<br>如果不是需要的格式，可以参考如下示例，Hive会将所有字段序列化成字符串写入到文件中。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE LOCAL DIRECTORY &#x27;yourPath&#x27;</span><br><span class="line">SELECT</span><br><span class="line">user_id,</span><br><span class="line">name,</span><br><span class="line">dt,</span><br><span class="line">hour</span><br><span class="line">from yourTable</span><br></pre></td></tr></table></figure><h2 id="如何引用集合类型中的元素"><a href="#如何引用集合类型中的元素" class="headerlink" title="如何引用集合类型中的元素"></a>如何引用集合类型中的元素</h2><h4 id="array"><a href="#array" class="headerlink" title="array"></a>array</h4><p>数组的索引从0开始，使用array[索引]的语法，引用一个不存在的元素将返回null</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT user_info[0] from user_detail</span><br></pre></td></tr></table></figure><h4 id="map"><a href="#map" class="headerlink" title="map"></a>map</h4><p>与array相同，使用array[…]的语法，不过使用对应的key值而不是索引</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT user_info[&quot;location&quot;] from user_detail</span><br></pre></td></tr></table></figure><h4 id="struct"><a href="#struct" class="headerlink" title="struct"></a>struct</h4><p>使用 点 <strong><font color='yellow'>.</font></strong> 符号</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT address.city from user_detail</span><br></pre></td></tr></table></figure><h2 id="如何解决算术计算中可能的上溢或下溢问题"><a href="#如何解决算术计算中可能的上溢或下溢问题" class="headerlink" title="如何解决算术计算中可能的上溢或下溢问题"></a>如何解决算术计算中可能的上溢或下溢问题</h2><p>1）使用范围更广的数据类型，但会占用更多空间。<br>2）进行缩放，除以10、100、1000等，还可以取log值进行计算。</p><h2 id="可以进行数据类型转换的函数"><a href="#可以进行数据类型转换的函数" class="headerlink" title="可以进行数据类型转换的函数"></a>可以进行数据类型转换的函数</h2><p>floor、round、ceil，输入的是double类型，返回值为bigint类型。在进行数据类型转换时，这些函数是首选的处理方式。</p><h2 id="什么情况下Hive可以避免产生一个MR任务"><a href="#什么情况下Hive可以避免产生一个MR任务" class="headerlink" title="什么情况下Hive可以避免产生一个MR任务"></a>什么情况下Hive可以避免产生一个MR任务</h2><p>1）本地模式，如 select * from table ，不会产生MR，Hive会直接读取存储目录下的文件，输出格式化后的数据。<br>2）在where子句中只有分区字段时，也不会产生MR。 </p><h2 id="Hive的join优化"><a href="#Hive的join优化" class="headerlink" title="Hive的join优化"></a>Hive的join优化</h2><p>大多数情况下，Hive会对每对join对象启动一个MR任务，但如果对3个或3个以上的表进行join时，on条件使用了相同的连接键，只会产生一个MR任务。</p><h2 id="order-by与sort-by"><a href="#order-by与sort-by" class="headerlink" title="order by与sort by"></a>order by与sort by</h2><p>order by：对结果执行全局排序，所有数据全部放在一个reducer中执行，当数据量很大时，会执行很长时间。<br>sort by：只会在每个reducer中进行排序，即局部排序。可以保证每个reducer输出的结果是有序的，但是不同reducer输出的结果可能会有重复的。</p><h2 id="distribute-by语句的使用"><a href="#distribute-by语句的使用" class="headerlink" title="distribute by语句的使用"></a>distribute by语句的使用</h2><p>distribute by控制map的输出在reduce中是如何划分的，可以指定distribute by的值，将相同值得数据分发到一个reducer中去，类似于group by。在分发后的数据中可以调用sort by 进行reducer内部的排序。<br>按用户ID做distribute，再按客户端时间做排序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SELECT</span><br><span class="line">* </span><br><span class="line">from your_table</span><br><span class="line">DISTRIBUTE BY user_id</span><br><span class="line">SORT BY client_event_time</span><br></pre></td></tr></table></figure><p>当distribute by和sort by中的字段相同时，可以使用<strong>cluster by</strong>做替代，达成相同的效果，但是使用cluster by会剥夺sort by的并行性，而且cluster by也不能指定ASC或者desc，只能按降序排列，但是可以实现数据的全局有序。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;随笔总结一些关于Hive的杂七杂八的知识点&lt;/p&gt;</summary>
    
    
    
    <category term="Hive" scheme="https://jiangzhiqi4551.github.io/categories/Hive/"/>
    
    
  </entry>
  
</feed>
